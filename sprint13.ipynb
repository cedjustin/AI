{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMw3HYn3UdRHnyqFx0qHXqm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedjustin/AI/blob/master/sprint13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0AkknTD6kvN"
      },
      "source": [
        "# import dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "tf = tf.compat.v1"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tFoADgN6r0K"
      },
      "source": [
        "[Problem 1] Looking back on the scratch\n",
        "* had an epoch loop\n",
        "* had to initialize weight and bias\n",
        "* using mini batches\n",
        "* have to calculate forward propagation and backward propagation\n",
        "* using activation functions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoqajRNY60xq"
      },
      "source": [
        "[Problem 2] Consider the correspondence between scratch and TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6UCr19t8eeN"
      },
      "source": [
        "Using the numpy scratch vs tensorflow, tensorflow reduces the time of implementation and it gives more complexity to the things that you are implementing and which also results in giving you more control on what you are doing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekrh92gdNfT5"
      },
      "source": [
        "【Problem 3】Create a model of Iris using all three object variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDOF1L43-9JZ"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPvvO1mMAvjX"
      },
      "source": [
        "def estimation_calc(X, Y, X_train, y_train, X_val, y_val, X_test, y_test, num_epochs, get_mini_batch_train):\n",
        "  def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "  #Read network structure                              \n",
        "  logits = example_net(X)\n",
        "  # Objective function\n",
        "  loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "  # Optimization method\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "  train_op = optimizer.minimize(loss_op)\n",
        "  # Estimated result\n",
        "  correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
        "  #Indicator value calculation\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "  #Initialization of variable\n",
        "  init = tf.global_variables_initializer()\n",
        "\n",
        "  #Run calculation graph\n",
        "  with tf.Session() as sess:\n",
        "      sess.run(init)\n",
        "      for epoch in range(num_epochs):\n",
        "          #Loop for each epoch\n",
        "          total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "          total_loss = 0\n",
        "          total_acc = 0\n",
        "          for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "              # Loop for each mini-batch\n",
        "              sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "              loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "              total_loss += loss\n",
        "          total_loss /= n_samples\n",
        "          val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "          # print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "      test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "      print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa213QjK_whN"
      },
      "source": [
        "Calculation using 3 species"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QcpXEi-_279",
        "outputId": "c970f78c-b2c4-4b79-a769-b88f11ec30b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Load dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "#Condition extraction from data frame\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# Convert label to number\n",
        "y[y == \"Iris-setosa\"] = 0\n",
        "y[y == \"Iris-versicolor\"] = 1\n",
        "y[y == \"Iris-virginica\"] = 2\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "#Split into train and test\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train2, X_val2, y_train2, y_val2 = train_test_split(X_train2, y_train2, test_size=0.2, random_state=0)\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train2.shape[1]\n",
        "n_samples = X_train2.shape[0]\n",
        "n_classes = 1\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train2 = GetMiniBatch(X_train2, y_train2, batch_size=batch_size)\n",
        "estimation_calc(X, Y, X_train2, y_train2, X_val2, y_val2, X_test2, y_test2, num_epochs, get_mini_batch_train2)"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_acc : 0.633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBW0DzGaL2Hr"
      },
      "source": [
        "Calculation using house prices"
      ]
    }
  ]
}