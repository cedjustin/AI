{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform machine learning on something close to real data\n",
    "\n",
    "I'm going to perform using a home credit dataset from kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "Confirmation of competition contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What to learn**: The transaction information of the clients\n",
    "- **What to predict**: The repayment abilities\n",
    "- **Submission file**:\n",
    "For each SK_ID_CURR in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:\n",
    "```SK_ID_CURR,TARGET\n",
    "100001,0.1\n",
    "100005,0.9\n",
    "100013,0.2\n",
    "etc.\n",
    "```\n",
    "- **What kind of index value will the submitted items be evaluated?**: Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Learning and verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from category_encoders import CountEncoder\n",
    "\n",
    "# loading the csv of the dataset\n",
    "df = pd.read_csv('application_train.csv')\n",
    "\n",
    "# cleaning the dataset by removing the empy data(null)\n",
    "cleaned_df = df.dropna()\n",
    "\n",
    "categorical_feats = df.select_dtypes('object').columns.tolist()\n",
    "\n",
    "# # separating them into variables\n",
    "X = df.drop(columns=['TARGET'])\n",
    "y = df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding values\n",
    "X = CountEncoder(cols=categorical_feats).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.9196909388901896\n",
      "ROC 0.5088892347318036\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_trans = scaler.transform(X_train)\n",
    "X_test_trans = scaler.transform(X_test)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "reg = LGBMClassifier(random_state=5).fit(X_train_trans, y_train)\n",
    "\n",
    "# predicting\n",
    "reg_pred = reg.predict(X_test_trans)\n",
    "\n",
    "print(\"Acc:\", accuracy_score(y_true=y_test, y_pred=reg_pred))\n",
    "print(\"ROC\", roc_auc_score(y_test,reg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** is very high which is a good indication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "Estimation for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv of the test dataset\n",
    "test_df = pd.read_csv('application_test.csv')\n",
    "\n",
    "# cleaning the dataset by removing the empy data(null)\n",
    "test_cleaned_df = test_df.dropna(axis=0)\n",
    "\n",
    "# separating them into variables\n",
    "test_X = X = CountEncoder(cols=categorical_feats).fit_transform(test_df)\n",
    "\n",
    "# standardizing the data\n",
    "test_scaler = StandardScaler()\n",
    "test_X_test_trans = scaler.fit_transform(test_X)\n",
    "\n",
    "# predicting\n",
    "test_reg_pred = reg.predict(test_X_test_trans)\n",
    "\n",
    "kgl_submission = pd.concat([test_df['SK_ID_CURR'], pd.Series(test_reg_pred, name='TARGET')], axis=1)\n",
    "kgl_submission.to_csv('kggl_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48739</th>\n",
       "      <td>456221</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48740</th>\n",
       "      <td>456222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48741</th>\n",
       "      <td>456223</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48742</th>\n",
       "      <td>456224</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48743</th>\n",
       "      <td>456250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48744 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SK_ID_CURR  TARGET\n",
       "0          100001       0\n",
       "1          100005       0\n",
       "2          100013       0\n",
       "3          100028       0\n",
       "4          100038       0\n",
       "...           ...     ...\n",
       "48739      456221       0\n",
       "48740      456222       0\n",
       "48741      456223       0\n",
       "48742      456224       0\n",
       "48743      456250       0\n",
       "\n",
       "[48744 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kgl_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "Based on the baseline model, we will make various improvements to the input feature quantities to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the dataset by removing the empy data(null)\n",
    "cleaned_df = df.dropna()\n",
    "# # separating them into variables\n",
    "X = cleaned_df.drop(columns=['TARGET'])\n",
    "y = cleaned_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'Cash loans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-af05186cc0a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# drop the missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mimp_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp_mean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# One hot encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \"\"\"\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_indicator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    247\u001b[0m                 new_ve = ValueError(\"Cannot use {} strategy with non-numeric \"\n\u001b[0;32m    248\u001b[0m                                     \"data:\\n{}\".format(self.strategy, ve))\n\u001b[1;32m--> 249\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mnew_ve\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'Cash loans'"
     ]
    }
   ],
   "source": [
    "# imputation\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# pattern 1\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "\n",
    "# drop the missing values\n",
    "imp_X = imp_mean.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X = enc.fit_transform(imp_X).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(enc_imp_X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_1)\n",
    "X_train_trans_1 = scaler.transform(X_train_1)\n",
    "X_test_trans_1 = scaler.transform(X_test_1)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier(random_state=5)\n",
    "lgb = lgbm.fit(X_train_trans_1, y_train_1)\n",
    "\n",
    "# predicting\n",
    "reg_pred_1 = lgb.predict(X_test_trans_1)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_1,reg_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9446768944676894\n"
     ]
    }
   ],
   "source": [
    "imp_median = SimpleImputer(strategy='median')\n",
    "\n",
    "# drop the missing values\n",
    "imp_X_1 = imp_median.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_1 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_1 = enc.fit_transform(imp_X_1).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(enc_imp_X_1, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_2)\n",
    "X_train_trans_2 = scaler.transform(X_train_2)\n",
    "X_test_trans_2 = scaler.transform(X_test_2)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_1 = LGBMClassifier(random_state=5)\n",
    "lgb_1 = lgbm_1.fit(X_train_trans_2, y_train_2)\n",
    "\n",
    "# predicting\n",
    "reg_pred_2 = lgb_1.predict(X_test_trans_2)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_2,reg_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.05 GiB for an array with shape (8602, 94474) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-61eb4b4952b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0menc_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0menc_imp_X_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimp_X_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# splitting the data into training and testing data using train_test_split from sklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.05 GiB for an array with shape (8602, 94474) and data type float64"
     ]
    }
   ],
   "source": [
    "imp_mf = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# drop the missing values\n",
    "imp_X_2 = imp_mf.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_2 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_2 = enc_2.fit_transform(imp_X_2).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(enc_imp_X_2, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_3)\n",
    "X_train_trans_3 = scaler.transform(X_train_3)\n",
    "X_test_trans_3 = scaler.transform(X_test_3)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_2 = LGBMClassifier(random_state=5)\n",
    "lgb_2 = lgbm_2.fit(X_train_trans_3, y_train_3)\n",
    "\n",
    "# predicting\n",
    "reg_pred_3 = lgb_2.predict(X_test_trans_3)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_3,reg_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cnst = SimpleImputer(strategy='constant')\n",
    "\n",
    "# drop the missing values\n",
    "imp_X_3 = imp_cnst.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_3 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_3 = enc_3.fit_transform(imp_X_3).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(enc_imp_X_3, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_4)\n",
    "X_train_trans_4 = scaler.transform(X_train_4)\n",
    "X_test_trans_4 = scaler.transform(X_test_4)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_3 = LGBMClassifier(random_state=5)\n",
    "lgb_3 = lgbm_3.fit(X_train_trans_4, y_train_4)\n",
    "\n",
    "# predicting\n",
    "reg_pred_4 = lgb_3.predict(X_test_trans_4)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_4,reg_pred_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature engineering,\n",
    "\n",
    "I used imputation and hot encoding technique because they are more useful,\n",
    "and from my observation, for all paterns of you can use in simple imputer the accuracy is still high and the stays constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
