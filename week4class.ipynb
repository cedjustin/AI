{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform machine learning on something close to real data\n",
    "\n",
    "I'm going to perform using a home credit dataset from kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "Confirmation of competition contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What to learn**: The transaction information of the clients\n",
    "- **What to predict**: The repayment abilities\n",
    "- **Submission file**:\n",
    "For each SK_ID_CURR in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:\n",
    "```SK_ID_CURR,TARGET\n",
    "100001,0.1\n",
    "100005,0.9\n",
    "100013,0.2\n",
    "etc.\n",
    "```\n",
    "- **What kind of index value will the submitted items be evaluated?**: Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Learning and verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# loading the csv of the dataset\n",
    "df = pd.read_csv('application_train.csv')\n",
    "\n",
    "# cleaning the dataset by removing the empy data(null)\n",
    "cleaned_df = df.dropna()\n",
    "\n",
    "# # separating them into variables\n",
    "X = cleaned_df.loc[:,[\"AMT_INCOME_TOTAL\",\"AMT_CREDIT\",\"AMT_ANNUITY\"]]\n",
    "y = cleaned_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.052153197403226256\n",
      "ROC 0.5938285747369815\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_trans = scaler.transform(X_train)\n",
    "X_test_trans = scaler.transform(X_test)\n",
    "\n",
    "# fitting the data\n",
    "reg = LinearRegression().fit(X_train_trans, y_train)\n",
    "\n",
    "# predicting\n",
    "reg_pred = reg.predict(X_test_trans)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_true=y_test, y_pred=reg_pred))\n",
    "print(\"ROC\", roc_auc_score(y_test,reg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSE** is very low which is a good indication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "Estimation for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv of the test dataset\n",
    "test_df = pd.read_csv('application_test.csv')\n",
    "\n",
    "# cleaning the dataset by removing the empy data(null)\n",
    "test_cleaned_df = test_df.dropna(axis=0)\n",
    "\n",
    "# separating them into variables\n",
    "test_X = test_cleaned_df.loc[:,[\"AMT_INCOME_TOTAL\",\"AMT_CREDIT\",\"AMT_ANNUITY\"]]\n",
    "\n",
    "# standardizing the data\n",
    "test_scaler = StandardScaler()\n",
    "test_X_test_trans = scaler.fit_transform(test_X)\n",
    "\n",
    "# predicting\n",
    "test_reg_pred = reg.predict(test_X_test_trans)\n",
    "\n",
    "kgl_submission = pd.concat([test_df['SK_ID_CURR'], pd.Series(test_reg_pred, name='TARGET')], axis=1)\n",
    "kgl_submission = kgl_submission.fillna(0)\n",
    "# kgl_submission.drop(kgl_submission[kgl_submission['TARGET'] < 0].index, inplace=True)\n",
    "# print(kgl_submission.loc[kgl_submission['TARGET'] < 0])\n",
    "kgl_submission.at[648,'TARGET']= 0\n",
    "kgl_submission.shape\n",
    "kgl_submission.to_csv('kggl_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "Based on the baseline model, we will make various improvements to the input feature quantities to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9446768944676894\n"
     ]
    }
   ],
   "source": [
    "# imputation\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# pattern 1\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "\n",
    "# drop the missing values\n",
    "imp_X = imp_mean.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X = enc.fit_transform(imp_X).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(enc_imp_X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_1)\n",
    "X_train_trans_1 = scaler.transform(X_train_1)\n",
    "X_test_trans_1 = scaler.transform(X_test_1)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier(random_state=5)\n",
    "lgb = lgbm.fit(X_train_trans_1, y_train_1)\n",
    "\n",
    "# predicting\n",
    "reg_pred_1 = lgb.predict(X_test_trans_1)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_1,reg_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9446768944676894\n"
     ]
    }
   ],
   "source": [
    "imp_median = SimpleImputer(strategy='median')\n",
    "\n",
    "# drop the missing values\n",
    "imp_X_1 = imp_median.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_1 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_1 = enc.fit_transform(imp_X_1).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(enc_imp_X_1, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_2)\n",
    "X_train_trans_2 = scaler.transform(X_train_2)\n",
    "X_test_trans_2 = scaler.transform(X_test_2)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_1 = LGBMClassifier(random_state=5)\n",
    "lgb_1 = lgbm_1.fit(X_train_trans_2, y_train_2)\n",
    "\n",
    "# predicting\n",
    "reg_pred_2 = lgb_1.predict(X_test_trans_2)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_2,reg_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9446768944676894\n"
     ]
    }
   ],
   "source": [
    "imp_mf = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# drop the missing values\n",
    "imp_X_2 = imp_mf.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_2 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_2 = enc.fit_transform(imp_X_2).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(enc_imp_X_2, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_3)\n",
    "X_train_trans_3 = scaler.transform(X_train_3)\n",
    "X_test_trans_3 = scaler.transform(X_test_3)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_2 = LGBMClassifier(random_state=5)\n",
    "lgb_2 = lgbm_2.fit(X_train_trans_3, y_train_3)\n",
    "\n",
    "# predicting\n",
    "reg_pred_3 = lgb_2.predict(X_test_trans_3)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_3,reg_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9446768944676894\n"
     ]
    }
   ],
   "source": [
    "imp_cnst = SimpleImputer(strategy='constant')\n",
    "\n",
    "# drop the missing values\n",
    "imp_X_3 = imp_cnst.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_3 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_3 = enc.fit_transform(imp_X_3).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(enc_imp_X_3, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_4)\n",
    "X_train_trans_4 = scaler.transform(X_train_4)\n",
    "X_test_trans_4 = scaler.transform(X_test_4)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_3 = LGBMClassifier(random_state=5)\n",
    "lgb_3 = lgbm_3.fit(X_train_trans_4, y_train_4)\n",
    "\n",
    "# predicting\n",
    "reg_pred_4 = lgb_3.predict(X_test_trans_4)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_4,reg_pred_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature engineering,\n",
    "\n",
    "I used imputation and hot encoding technique because they are more useful,\n",
    "and from my observation, for all paterns of you can use in simple imputer the accuracy is still high and the stays constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
