{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMr5lmgDBxPugSE9EFNuoXL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedjustin/AI/blob/master/sprint20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eIibpiWBPOI"
      },
      "source": [
        "[Problem 1] Code review\n",
        "\n",
        "- This implementation uses res-net instead of u-net\n",
        "- It uses layers that has been learned by using image-net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roL5VSm38PhB"
      },
      "source": [
        "import gc\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.callbacks import *\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import *\n",
        "from keras.models import Model, load_model, save_model\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import warnings\n",
        "from keras import backend as K\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 9)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvrWeV7NC6UR"
      },
      "source": [
        "def compute_coverage(df, masks):\n",
        "    \n",
        "    df = df.copy()\n",
        "    \n",
        "    def cov_to_class(val):\n",
        "        for i in range(0, 11):\n",
        "            if val * 10 <= i:\n",
        "                return i\n",
        "\n",
        "    # Output percentage of area covered by class\n",
        "    df['coverage'] = np.mean(masks, axis=(1, 2))\n",
        "    # Coverage must be split into bins, otherwise stratified split will not be possible,\n",
        "    # because each coverage will occur only once.\n",
        "    df['coverage_class'] = df.coverage.map(\n",
        "        cov_to_class)\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_depth_abs_channels(image_tensor):\n",
        "    image_tensor = image_tensor.astype(np.float32)\n",
        "    h, w, c = image_tensor.shape\n",
        "    for row, const in enumerate(np.linspace(0, 1, h)):\n",
        "        image_tensor[row, :, 1] = const\n",
        "    image_tensor[:, :, 2] = (\n",
        "        image_tensor[:, :, 0] * image_tensor[:, :, 1])\n",
        "\n",
        "    x_dx = np.diff(image_tensor[:, :, 0], axis=0)\n",
        "    x_dy = np.diff(image_tensor[:, :, 0], axis=1)\n",
        "    x_dx = cv2.copyMakeBorder(x_dx, 1, 0, 0, 0, cv2.BORDER_CONSTANT, 0)\n",
        "    x_dy = cv2.copyMakeBorder(x_dy, 0, 0, 1, 0, cv2.BORDER_CONSTANT, 0)\n",
        "    image_tensor[:, :, 1] = np.abs(x_dx + x_dy)\n",
        "\n",
        "    return image_tensor"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ-f-qTIEWwU"
      },
      "source": [
        "Data loading & depth merge:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE6xEGyiHLr5",
        "outputId": "7a9f36ea-5296-408c-95cd-e20d96d39def"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX4agkDagEBH",
        "outputId": "5a23fd6c-905e-4f63-aeba-2a89805ceceb"
      },
      "source": [
        "cd drive/MyDrive/tgs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/tgs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkWmwiGvDG_h",
        "outputId": "8c2dce22-5382-45e5-d321-9d8d9a7966e5"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('sample_submission.csv')\n",
        "depth = pd.read_csv('depths.csv')\n",
        "\n",
        "train_src = '../train_data/'\n",
        "\n",
        "print('train:\\n{}'.format(train.head()))\n",
        "print('\\ntest:\\n{}'.format(test.head()))\n",
        "\n",
        "\n",
        "train = train.merge(depth, how='left', on='id')\n",
        "test = test.merge(depth, how='left', on='id')\n",
        "\n",
        "print('\\n{}'.format(train.head()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:\n",
            "           id                                           rle_mask\n",
            "0  575d24d81d                                                NaN\n",
            "1  a266a2a9df                                          5051 5151\n",
            "2  75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...\n",
            "3  34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...\n",
            "4  4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...\n",
            "\n",
            "test:\n",
            "           id rle_mask\n",
            "0  155410d6fa      1 1\n",
            "1  78b32781d1      1 1\n",
            "2  63db2a476a      1 1\n",
            "3  17bfcdb967      1 1\n",
            "4  7ea0fd3c88      1 1\n",
            "\n",
            "           id                                           rle_mask    z\n",
            "0  575d24d81d                                                NaN  843\n",
            "1  a266a2a9df                                          5051 5151  794\n",
            "2  75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...  468\n",
            "3  34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...  727\n",
            "4  4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...  797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNtNkQNAhjVV"
      },
      "source": [
        "Load images and masks, examine random sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BREwNdrhcd3",
        "outputId": "9bab7f90-8c8e-46a0-bb09-87c4a1eeb675"
      },
      "source": [
        "X_train = np.asarray(\n",
        "    [cv2.imread('train/images/{}.png'.format(x), 0) for x in train.id.tolist()], \n",
        "    dtype=np.uint8) / 255.\n",
        "y_train = np.asarray(\n",
        "    [cv2.imread('train/masks/{}.png'.format(x), 0) for x in train.id.tolist()],\n",
        "    dtype=np.uint8) / 255.\n",
        "\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 101, 101) (4000, 101, 101)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "gqaRMdwEhluH",
        "outputId": "e67d9a8b-2d4c-4ee2-c5a9-9a505b62a4a7"
      },
      "source": [
        "random_index = np.random.randint(0, X_train.shape[0])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "ax[0].imshow(X_train[random_index], cmap='gray')\n",
        "ax[1].imshow(y_train[random_index], cmap='gray')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdba5ad60d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAFSCAYAAAAJl+KKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29a6xl6Vnn97zV3W7cdTl16Xa7x23FRlgzQiNNQBYDIooQnlGAjMb5gBBkNHGQI39hMsxFGkzygeRDpEFCwzDSCKU1MJAIwUwYFFsIzYR4QFGQ4tAExM1cHBPjNnZfq+vadru7Vj7U2cv/2uzfWc+uc8q165zfT7L8nFXvftd7WXt5eT3//X/GNE0lIiIiIiLMqfs9ABERERGRXceHZhERERGRBXxoFhERERFZwIdmEREREZEFfGgWEREREVnAh2YRERERkQXuyUPzGOPbxhh/OMb45Bjjw/fiHCIicnR43xYROZhx1D7NY4yHquqPquqvV9VzVfXrVfU90zT9/pGeSEREjgTv2yIiyzx8D/r8hqr65DRNn6qqGmP8XFW9v6rw5vvwww9PjzzySFVVdR7ixxgb41OnTm08nty6dWuO81wUJ9l/xg8//OVlfOihhxZj6qdDjn99rJ05JLRGdJz6zOOdPjPedvyH2fvsM9fxzTffXGyz7Xi2jQlakxzbtuOk6y+v0YP+LePO3DrXTed7SdDcqJ/c7068tNavvfZavf7668ubudtsdd8eY1gVS0QeZF6apumJbT90Lx6a31FVn4m/n6uqv3rQBx555JH66q/+6qqqeuONNza2yf9hzP/Rfstb3jLHX/VVXzXH+SCbvP766xvjL3zhC3Oc/8OY533rW986x4899tgcX7hwYY7Pnz+/8fjZs2fn+MyZM3P86KOPzjE9+OWa5Jirqr74xS/O8Ze+9KWNn0loHTsP8vSgSeuVe7D6P0Xr581+ci45/nzgyT5zvzPOcyW5djdv3pzj69evz/Frr7221XhyLnne3Ne8Run/YNEDdO5pjifHmdfutut2+vTpOT537twd586/M87P5HeC5kYPnTmfjHOfcv607rm+uQd0rtzvq1evzvGrr746x1euXJnja9euzXGu9era/bVf+7U6Bmx93xYReYD59N186F48NLcYY3yoqj5UxQ85IiKyG+Q9W0TkJHIvHpo/W1XvjL+f3j92B9M0PVNVz1RV7e3tTe985+2P5FumTqqZ3jTnm75sT2+a8+3TjRs3NrbJ8eSbq3wTRW9j8w1VxvS2NN/a0Zu6g/6tk+Je72sTlNbON4C5RtlnvrGmt4G5N/nZfGOa503obSPtPb3Vpr3JeeV46M16jif7pzfu20pKcgwZ52ep/1wfypjkuq2PO6F9OozsppPpoOzGQRKTTcdzjSim62mTpGRbidWOsnjfznu28gwROYnci7v9r1fVe8YY7x5jvKWqvruqPnoPziMiIkeD920RkQWO/E3zNE1vjDH+TlX9u6p6qKp+cpqm3zvq84iIyNHgfVtEZJl7ommepumXquqXuu0fffTReve7311V/GM2khJkCjnTqJmCztQsSSxSnpE/AsofB2W6nlLF9GMzSlGTzIEkDOtQyr7jqLAtOdaMU9KQx5Pcp0x9Z0zpfZKFdH6QR3uffXaus4Ta0B5n/zn+jpMGrQm5t3TWOSUZKQnK78x6XzQfktd0JAu0juSGkXR+6Epj66wjSTWS1TXUcUF5ENj2vi0ictI4FmI8EREREZF7iQ/NIiIiIiIL3DfLueSRRx6pd7zjHVXFv8yntDw5BGTamXxyM0Wfjhkpz3j55ZfnOD1dU3pB7g6U0iZP6OyHUu7rdAoyEOTeQDF575Lv8raFI8jnlwpWkJSHZBudeZETQ6braS4JpeypPUFFRUhKkBIlisnTev06I3kD+VcfpgAKnZf6JNeY/G51rks6F61psurnmLhniIjIAt7tRUREREQW8KFZRERERGSBnZBnPPzww3Xx4sWq4mIUnRLRmV7NFDS5BWRaNftMGcZqXFV3SjXSVSOlGumA0XEByNRyzvGggiZErlenMAxJLKjQRKcoRGefOkUkSJJCZaXpXMm2JaxpD8htoyPb6BT6IHkJOcVQye5O+W5yFKliyQvJo2isJLGgOCHJRKcQDkm6yEmDJC9UzGbV/3FxzxARkYPxTbOIiIiIyAI+NIuIiIiILLAT8oyqL6dhM9XcKXCRxzNVTG4BZ86c2dgmP3v+/PmN8dmzZ+c4pRqXL1+e43ThILkFOQhQCvwgeQY5YCTbFjehfjJ9nefN9e2MO1PutE8pLei4Z6RUg6QRJAfI/kluQRIAcnghqUZHktEZc64PFXMhGUZKiA6Si5AMhfa1I9+hcdN3keK8FqnAz2GcSkjGtUlmpTxDRORk4JtmEREREZEFfGgWEREREVlgJ+QZ0zTNRQno1/LkREGpb5JAUDGUTA+n20bGHSlBFkbpSDU6TgQHOT3QPCk1Tan1TpEVkjSQlKJTKCTX9K1vfesc0/pSgYtc35Qf0HzJzYTkFtkntaG4636ygq7XjuMHyZhIOkLShvW/qSBNjiOvFSo0RHH2T9IOojNnWlMac8f9Y1PfIiJyfPFNs4iIiIjIAj40i4iIiIgssBPyjDfffHOWNWTqN6URVOwiIecGStlm+jnTwHmulAyQrILS1ZkGXslP1sdAkhKSQqynh8lhZFMRhvU2lHJP8nwpUchUdu4TpeW3Td2fPn16Y//ZJseWcyEpBUlW6LPpyJH7l21yXlQcoyMb6hTCyTY5rxxPxjnm1157bePxgwrEkHtGx+mDZDe0rznubE/SH/p+53zyOBXpoT0j+VH2v5ovFWYREZHjhW+aRUREREQW8KFZRERERGSBnZFnXLt2bY5XZHqYUv0dSQY5VCT5WUoJJyRPIGlA9kPuDh33jPXx5OcpZU/yjEzTH+SisGkc20os8niuF+0Npc2zn07RjE5REpIhZHvag23dLUiCQ/KMzvWde33z5s05vn79+sY4XV1SbpD9VPXkO3mc9iz3O4sLZbEgkpikzIO+9x2XkGxP657953WW5PHV2JRniIicDHzTLCIiIiKygA/NIiIiIiIL7IQ849atW/Ov+8ndIdOumYpf72cTmYKmOKUKlBLuyD86KV4q0EHyhK48I8eUUJGHXF9aF9qPHBPNmWIqHkOp+729vTlOaUG2oUI4JF/pFDGhuCMHSMglg+hIMtINI9fk6tWrc7ySPFWxVINcXap4Pjk+cgbJayLHSnIQWlMqckPQmBPaD7qOl2QhHWcfERF58PFNs4iIiIjIAj40i4iIiIgssHN5xUwXU+EOSsWTlCChX/VnnMUVsn1CafNM+1PqlyB5Rsbr8oxcIxorFe+gX/2TbCP3JudPDgrZf46NCl/kHqTzxrlz5+Y4pRp5nAqsUEEMctKg47TH5DRCEoCDpDYryBki9yIlFinJIBkGFTchWURVr7gJFTrJuZELB8lQ8ry5r3ndUIESciHpyJ2oPTm2rMavPENE5GTgm2YRERERkQV8aBYRERERWWAn8opjjDnFSSnxTOVmmjrJz2YbcpXItGs6MWRKONt00ukkW6CY+uyk9w/6N5K2dCQHlLKnPch0f6egRJKpbSqYklKN3KeMSaqR6XQqGpLQ8Y6sgFL9FFP/JKfJdU65RRY0IekF7QXNa51OAZjOZ/O6IVlD59olZ5a8hqhAEDmYkLwr228qokN7KiIixwvv9iIiIiIiC/jQLCIiIiKywM7JM8jtgH6xT4UpyDmA0sOZ0s+0f0oDyLWjA7WnX+xnepgkDFXsBJCfSYcKcoqglHunOAg5mOR+dArDJDn+juNJyjNyz6hIRcdBodPmqCQZncIree1Se5JhkIznoOI4VNyFrs2kI83JPkmOdJBkZFP/HScLun/kfaLzvdI1Q0TkZOGbZhERERGRBXxoFhERERFZYOfyiyk/6BRRSOiX/JnWvnLlysb2mWpNecaZM2fmOGUOlHLOdHdCRReoAEiuQ8ZZ4GH9b3KKyDjlCp0CHx1nENqbnHOnOEim62nvSbaRkoyMc886a5XHKaYUfadwCclgSNZCe0TOEJ1rgMazLs9IJ44cU0fiQ98D+t5kn1TgKOkUjCEXDvou3rhxY45TqpHxJjlYx0FEREQefHzTLCIiIiKygA/NIiIiIiIL7IQ849atW7OEopMSz/RqpoFTepBk6vfatWtznA4b5LxBaf+UTJB7BEkYSJ7RkWSszzHlB/lv+RkqekLnzs9SUYuE+iH3hew/pTOZBqfPUuGVXPfsM9eH5tjZAyq8QnKIhGQLtLZUjIdkGHR9ZBty0jjovNkXFU0hiQnNkyQT20qCEpK85J6R0wV9Nvcyr6FXX311js+fP19VXGxJRESOF75pFhERERFZwIdmEREREZEFdkaesZJNZBo1nSsyBZ1p10xBZxo4C19ke3JuuHnz5hynHCDT0hlT0YxOwYqEnBhyXiS7qLozdZxzJtkGFbZIOjIAap/noiIdnfXNmGQb2X9HGtFxUyBpQELuC+TcQOelsZEbRF7HVOQl970jHSF3jvXPZ0x70ImpsE3nsyTTSXlEXjckwUly/uTwkt/LvL739vb+3PlFROT44ptmEREREZEFfGgWEREREVlgJ+QZb7zxxvyrdHKWyFRrpqNTwpGfXaVOq+50vch+SFaRv5DP1GvHtaOTos84U8IdZ4X1FDql1ymtnfOnsZKbRLah4hIkP+gU8uhINcjFgcZPsh5yViD3iTxOchmSeXSKyCTZT46N5Bk5no5jBI0hz3vQmPK6IekF9bXtNU4SmY5sir7r5HbTKbSTn11JYfL8IiJyfPFNs4iIiIjIAj40i4iIiIgssBPyjFu3bs0FRUiekanyTEGn9CKdA0jOkXGmbPNczz333BynVCOhtDnJH0iekFAamFwM1sdBaedMm+eYyF0h55PrQvvRKXZBso1cr9y/TI+nRIaKniQ0FypQQsepaAj12XHw6FwH2U/HXYWu44RkGCR/WB8H7T3JMEiOlBIIknwk5NSRfZI8gxxuOoVOOk4dqzWhYjQiInK88E2ziIiIiMgCPjSLiIiIiCywE/KMaZrmtOeqyEnVnSlbSgNnqjXbkzyD3A4y/Zx9vvDCC3Ocqdk8Vzp4pFyEUvdUQCOPJ1TsYn2sVGgjIXcFGitJCzIlnelxcsYgZ49M9ZMjCZ2XpAV0HZAUhArBkFSD1pD2IqH1JGlO5ztATh20F13HDLoe6TMk5+h8z6j4T5JrR4VIKO7INkgWQ243q/sBrZOIiBwvfNMsIiIiIrKAD80iIiIiIgvshDyj6svp0Ex/rhw11ukUjsg2KZ/I1Oz58+c3tk8ydZ/SkST7zHNRAYpOAZTkIHkGSSlIAkL9pkyCUuvZDzla5PGUbVCbzvw7bUgCQA4Yua+5TyTbSEkGrRU5WtAeUUzz7Ugvcm2p+Et+T8ht46BxdPaApA4kTyGZC0k46DtE9wP6DpArT84l1zTbv/baa39uLCIicny567v9GOOdY4xfGWP8/hjj98YY379//OIY45fHGH+8/98Xjm64IiJyN3jPFhE5HId5RfJGVf3DaZq+tqq+saq+b4zxtVX14ar62DRN76mqj+3/LSIi9xfv2SIih+Cu5RnTNH2uqj63H18bY3yiqt5RVe+vqm/Zb/bTVfWrVfUDB/V16tSpOXWeqdBMr964cWPjZ+lX9JmyThnG3t7eHGdqOl0vnnjiiTnOdGwWOsk0MPWzratG0kl1r7ejFDzF2Z6cDygNnuuea52SDIpzjw8qrrEi50jODeTKkPtHDgodeQYVmKHzkjyDpAQkW8j9IkkG7VfnvLmP69cWOVp03CLoeiKpRsdFhvonNxBaixxDXgf5HSVXl7yOb968WVVVn/rUpxbHuwsc5T1bROQkciRivDHGu6rq66rq41X15P7Nuarq81X15FGcQ0REjgbv2SIi23Poh+Yxxpmq+jdV9femabqa/zbdfmW08bXRGONDY4xnxxjPUjlkERE5Wo7inv0VGKaIyM5xKPeMMcYjdfvm+zPTNP3C/uHnxxhPTdP0uTHGU1X1wqbPTtP0TFU9U1V14cKFaZUKz/Qqpa8zRUrp/XwQX6VRq+6UeWQ6NtO0mZo9e/bsHGeKlx70U3pBcoCM6Rf75Aax7nbQKaKxqSDD+vFsT+PIc5HsgYrK0Hk7BT4SkiJ01ivT8uSqQcc78oxOcZmO9GBbh5AcD32249qRe7H+b+RckZ/pFGKha6tT0CX3L79/ne8iFbbJazTlVNkmx5PrsLoP5Xl2naO6Z48xrOgiIieOw7hnjKr6iar6xDRN/yT+6aNV9YH9+ANV9ZG7H56IiBwF3rNFRA7HYd40f3NV/e2q+p0xxm/tH/tvquofV9W/HmN8sKo+XVXfdbghiojIEeA9W0TkEBzGPeP/rKrNOdiq923T16lTp+Y0KUkg6Jf8mR5OGQY5BKwKElRxIZJMd6+PcwW5ACTkfECFIjpOGOvnOqjwyRLkNkLOIOQIQbIHkjp0XBO2TeN3IPkErXtHekCylq4EYtNxat9xmCDZBkHzXT9fxnmt0Npl3ClckjHNgZxy6PtB1yU5pGSc0q38bM53dd7OOu8CR3nPFhE5iVjKSkRERERkAR+aRUREREQWOJR7xlFx6tSpOTVKUodMj3dS4pnKTceMdN7I4+RukSleSj9nnKllmksez9QvFaOgNHlVr/AHFcXItUjZSq5d9tkpFJLzIScRSuknHakGtSd5A0kR6HinmEhCbhMdt5COYwZJU3I8naI4JJXpyjPyfOTAQtD5sp+8bmjtSLaSkMwjpVgZ57p0itysWC8KIyIixxPfNIuIiIiILOBDs4iIiIjIAjuRVzx16tT8a3VyFzhM6jdT35lmT0kCFe7oFK+gYhqZ7s3zZqEPSv2Se8Z6IQdKj1O8qTjDQXGSafNMa2cBmIzTnYQKuiS0xx0ZA6XuyVmhcz2RowONLfcm94VcH2gM27pN5Hp2xtwpWLPeL80zP9NxQklonzqSDIIkKbkWOS+SbVBhlE1rva2Li4iIPJh4txcRERERWcCHZhERERGRBXZCnvHQQw/VhQsXqqqXfk95Q+cX9eRKQQUSKJ1M8oxMA5PDREpBcvyZik6pBrlw5Ger7pRSZHEXkgHQOtL4yDUhiz/kZ6lICkkjyBWFCsNs63rRGUNCRUzoGiIHiFwHKtJD60DSC5IJ5Hnz+qNiNJ3iMlUsach16RT5IQcQWsdti99QAR6STW37Pc54kzyD7hciInK88E2ziIiIiMgCPjSLiIiIiCzgQ7OIiIiIyAI7oWl+5JFH6sknn6wq1htStb/U36ZeNCE9Y/aZutODqqStoCqA22qas//URqb9VWotU9e6PlayiiPtNp27ozPOc5G+lLSe2U9nP0grTPPa1lqO5t6xEqP12VbTTNdQ7jfZsmX/pOntaLLXvz907W+7RtR+Wy0w9UM6brLc2xayoFzFappFRE4GvmkWEREREVnAh2YRERERkQV2Rp7x1FNPVRVX6yKrqpQJkGyDbKU6KetuKnsTVGWP5Akk7aB4/e9cO5KDdCQNJDMgW76cQ65Rzpkqx2W6O/snOzxaU6q6d69tyjoWhlThLjnI+m1TG5JkdOQPnQqL65/fthpmpzJfpx/ap05Ma0HyK/reJ2SfJyIixx/fNIuIiIiILOBDs4iIiIjIAjshz3j44Yfr0qVLVXWnawRJDzIFm6nvjK9fv77xeKZsSQrSSSHncZIzUCU/StHnvLI6YMa5Puv/dvbs2Tmm6oAkySByPjkHcrfItHauEe1BprhJXpN7eePGjTm+du3aHJNUg+QZVHWPnBg618e2Fe4IWhOaS0LSi05lvfU+qSIgVSnsVNQjKUWnfcclo+PeQnvTqSTZkXCIiMjxxDfNIiIiIiIL+NAsIiIiIrLATsgzTp06NcsOzp07Nx9P6cHp06fnOGUb9Kv4lCdk6j6PZ2o5+8zzZio6z5XHM4Webci1ImUFlK6nNPZ6cROaQ6ayk0wvk0wi42yfkox0tCB3C1oXSnFTwQ46b0o1Ms425HJCcoBcT5IkkFSICn105BBUaCbpOG+QCwpJNcgho+rOtcjPdJwxSPJCxzvyjI4Mg4rlJHTdk0MIrXteryIicvzxTbOIiIiIyAI+NIuIiIiILLAT8oxpmuYU/JkzZ+bjmR4meQbJEKgoR0omMu64LOR5Mz2cY6DUb0oGMq179erVjeNPDiqqsre3N8e5diQl6RR66Ug7qOAKSSbItYMcC2jOJOEgCU4ezz2m/SMpQc6RJBwdh41OTGtFBX5InkEyGJJa5JjX25FTBLlb5Bql3InWqON6QfPsFINJOnIZckLZFOuiISJyMvBNs4iIiIjIAj40i4iIiIgssBPyjDfeeKNefPHFqrozDZ7FOi5evDjHlAbOFG8nZZtFM6hASab3s38qjkG/9k+yzzxvSjVoDOm8UXXnupC0pSMryeP5WZKeUIqb1ihlFbTWneOdohNUzIWcNEiyknPPPdhWtrFtkRSKE5IS0F6Q5CGPr8tyyFGGpDmdOM9BY6I968h3qE3ScczoOJiszqs8Q0TkZOCbZhERERGRBXxoFhERERFZYCfkGV/84hfr05/+dFXdmcrOtOeFCxfmOGUbmRInJwAqFPL888/P8ZUrVzaOLWUF6bbRKepAqd9sTw4TKcPIMayP8+WXX55jSqF3CnOQ3ITS7FT0hZxK0j0kpRdUAIZcTnItyB2BIEeEbQus5DWX7TOmdaPx5DWR8yIJBzm8kCMFxdln7mkVS3xWhYiq7rzmqCgQFSuhQiydwjb0vcz2tMcJfS8PKvqyPn6S/YiIyPHCN80iIiIiIgv40CwiIiIissBOyDNef/31WZ6RZAo2U63nzp2b40wJP/nkk3NMKetMM6fbxOc///k5TslEpn7Xx7yC0s8k1aBUeRblyFR0xjm2qjsdQCjVTu4QtEZ5PNc31ytT9CSRyT0juUWuI0kdyEkk0+KZfqf5UkGMbZ0Ytm1PhUXyWuy4XnRcPqg4CznOUPv1v/MzFNP3oLPWJMPI70QnpuuDrpWk46qxKc5rVUREji++aRYRERERWcCHZhERERGRBXZGnvHcc89VFf8qPlO2b3/72+f4iSeemONMJ7/tbW+b40x3p/PG+fPn5/gzn/nMHL/wwgtz/Morr8xxpnjJuSGPU7q6k07P9plyzjR2FbtPJJkSz34TcmZICcHp06c3xuSaQK4aHRlGp1gJtSdniSTbZEzFU8jpoZPSJ7cKug46LjB03ZCkggqVkNRi/Xwkw6ACPrReCbmopNwi5UgpRcqYZE0kA+oUvEloX1frQxIuERE5XvimWURERERkAR+aRUREREQW2Al5xhtvvDEX6chUZ8oSstjH1atX5zjTt48//vgcZzGUS5cuzXHKM/b29uY45Rx/9md/NsfpqpGFRXI85I5AMgFKy5PDAaWfq+5cI3IOSDouAtSepDNU9CRT2bku2Q/1T/IMkkzQXEgakccpXd8pjpFk+5QD0N5nn3St0GfzeMfpoVPsJ/tf/5skGSTBoX2lAjZ57eY1nt+z/B505Bn03aDiKZ3rLOe+WpN1yZSIiBxPfNMsIiIiIrKAD80iIiIiIgvshDxjmqY5VUvOCvTL+ZRqPP3003OcKdN02EjHjCzQkdKOLJLy4osvznG6aqzkJFV3ppBzbJTuJQeFTanf9TilEFWcdqaUOEkaMs1OxynFn5D7BMktSPJxmMIUVDwm171TcIPOSxKcjBOSedA6d5w3aL86bhA0zuz/oM/QHpATCskw6HjGnSImdC6SYXTGQ9+fZLU3nTUXEZEHH980i4iIiIgs4EOziIiIiMgCOyHPGGPMqc5MZafsgdKoKYegwgbZZ6a7s0DHuXPn5jhdNZ566qk5TqnGSy+9tPF4yjZSOpKpZZItpOSBJAbrBShSYpJ9keSAyHN3JAcJtSHHDEqVd6CiMpuKThwUk7NHQil6ctig8SQkz+jENM4cQ44594W+PyRnqOLCJ7SOJLshZwwaB8Wd4kKdYjD0/et8Zza17ziriIjIg49vmkVEREREFvChWURERERkgZ2QZ5w6dWqWSlAaP1O8ly9fnmNKL9Mv4TO9mtKLdM/IAijptpESjiyGkvIMil955ZU5pgIMOX6SPKy7VnRSwyS3oGIZ6dBxkKPCJjL1Ten6nH/GWaiG5AQdR5KOPIPkKElHUtJxKSE5A513W0kGxbQ+uc60F1VVb33rW+c45RlZwKZT6KQjB+kUGUnyvFRQh4oFdcbWcdVYjS3XU0REji++aRYRERERWcCHZhERERGRBXZOnpHp5UwXp/tEtkn3jEwP0y/hM9WafebxS5cuzXE6aZw5c2aO03kj09jZnhw5UqqRDhs0lyTTw1Wcgif3jUxlZ8o655BzIwkHSQ6oaAi5KWRqO+dPEgKS2iTkoLDt+PO6oYIbVJSDCprk+m+7htQnOXtknP2T48W6e0Z+J3LcNIe8Vmh8nflQoZqEjpNciYr65BhILpL7ukmOk99nERE5vhz6TfMY46Exxm+OMX5x/+93jzE+Psb45BjjX40x3rLUh4iIfGXwni0icncchTzj+6vqE/H3D1fVj07T9DVVdbmqPngE5xARkaPBe7aIyF1wKHnGGOPpqvpPq+p/qKp/MG7nVb+1qv7z/SY/XVX/XVX9+EH9nDp1apYHZOqUJAaUBs90cjpsJPQL+Uy5p6tGumSkq0bKGbLACMkZsk26U2Scn+26VqzLNTZBcoWOPCPHnTEVviCnDpLL5LqTPINkNJ3CIjR3cn3I6y9T9DmGHBvF5PhBMglygKAiLCQx2FaekONc/ywV3iF5BslNOuPIPSAHDPp+kJyDrgOCnF9ItrGK/+iP/mix713hqO7ZIiInkcO+af6nVfWPqmr1vzaXqurVaZpW/4vzXFW9Y9MHxxgfGmM8O8Z4Nh9eRUTknnEk9+x7P0wRkd3jrh+axxh/o6pemKbpN+7m89M0PTNN03unaXpvvm0VEZGj5yjv2Uc8NBGRB4LDyDO+uar+5hjjO6rqq6rqXFX9WFWdH2M8vP/m4umq+uxSR2OMOfVKaWH8ilEAACAASURBVFqKNxUbqLozvXrt2rU5puIPJBO4cuXKHJOrBkksMo2d0o4cG7lkJDnOdUkCyR6oDaXHqaAGFUMheQq5LCTZZ0cuQ4UmOu4ZFJN8gAqykMtHXlskLyGpBtEpaJKkLCL3sSPxyTbr/VPBkTzecWyh7y6NtdMnyUJIzkGyKRpP7kHOd1PBG7rOd5Aju2eLiJxE7vpN8zRNPzhN09PTNL2rqr67qv79NE1/q6p+paq+c7/ZB6rqI4cepYiIHArv2SIih+NeFDf5gbr9A5NP1m293E/cg3OIiMjR4D1bRKTBkRQ3mabpV6vqV/fjT1XVN2zz+THGnHol9wUiU7aZQibpQabKU3pBEogsPpKOHFm4JOMsgJJpYPqFP8kcMiWcY14vQEEpdJoPFRzJc1AaPFPc2YaO05ypwEquHUkaSI6TaXZyyaCxZT+b0u9V7J6R1wdJNbI4CxXK2FQ0Yz3uSBjIkaLjqrEuC8nP5PjIuYO+r3StkCMHuXPQdUOfpbjjUkPXxKY4x/igcNh7tojIScQy2iIiIiIiC/jQLCIiIiKywE7kFdM9g35RTylhcnogMlWc/WTaPGUb5O7wyiuvzHE6Y2RhkIypAAil38mFY/2X+tkuU8cdSUYnNU2OJLkuGafcJNeLnDc6ThdUkIXGT+l9KnDRkWfkeak4Te53SjWoAArJNmht6fqmQh8d9wza3/W/6RrKNSInF7quc+3ymkhyL0nmsa08g5w9SNJFkrFuASIRETkeeNcXEREREVnAh2YRERERkQV2Rp6xSn+T9CKhVDE5BJCzAsXZT6aZM7We6fRMs2caON0gMhVNEoNMxZPsZF1iQKnmjsQioWIOOaZ0kEj5AaXZM+44H1CRmI5rR+4fzZ3kDZ3rJs/VaU/XHNGRNtD4STJA1wa5aqxfW3Q9kWyDXF7I5SSvDyrcQtcNSSlIxpXjofl3HDM2yVFo7CIicrzwTbOIiIiIyAI+NIuIiIiILLAT8oxTp07NqVpK/XZSxfRL/k76muQA9Kv7JM9LxUZIhpB9kkTiIAkKSTeo0AsVQMnj2WdKTzpFTzKFTvKXjoMCHc/+SebR2T+SOuSaUOqe0vFUiIOug5T4ULEVckEhSUJCkgwqurMuIyFZSZ6PrlO6tmh9k07hHLofkDsMnZfWl5xiNhWeIcmTiIgcL3zTLCIiIiKygA/NIiIiIiIL7Iw847HHHqsqllhkmraTdqW4kx7OtHTKBPI4FTag1H3Oi86bdApIrI+DioN0JBlJJ/VPqXI6TlIBckdYXQ9Vd8oz6Di5k1BxE9q/zpiJjqyC1p/cGug6IGcZkhDR3Ol7tT5uktoQOT4q1kLuFunSQuuen6XCQdQ+13ST3OKg9pv2UnmGiMjJwDfNIiIiIiIL+NAsIiIiIrLATsgzxhh3pIlXZNqTfvHfcTugFCw5B6RkICGXCEqJk9tBp1AEuXCsyzPIPaNTrIXkBAedbwlaC5o/pf1TbpGSjNOnT288Tg4buWfkotKRuFBMEo5ct3TJILcJkmFQnOtJkoQcW7Ynecb6dZ/9khSGXGdyL9OBJdt0iqSQ00WuaY6bCpfQ+tK9IVlyTlGeISJyMvBNs4iIiIjIAj40i4iIiIgssBPyjGma5hQouQIkmeLNdD1JAG7cuDHHS7+EX4fcF1IaQEU2EipcQql7kpGsp+Ip1Z7Q52m9iE5xDSqq0lnrnEuuY7opZJx7nzG5Z5A8gyQuJHeh4ySHyGuus98dqQzJLUiuRBKUXOeUYFTx9ZTjzu9ByjYyzu9f7h+tBRVPybnlZztFTzouGZ3vQLI6V8cNR0REHnx80ywiIiIisoAPzSIiIiIiC+yMPGP1i3lKWXeKTmSqnH7JT24EnbQ5jWdvb2+OM8VNhSOyf3IZINZTwbQulI4nmUGOtePwQMU7SJLRKTBD6XHaY5oLuXMkVNyEJBwkASB5Bjm8HFSoZtNnqUhNZ8w0TnLMWJdnkMyFnC5SkpF9Xb9+fWNMshv6jlLBkY5jC8mGyIWE9mDTNdEpfCMiIg8+vmkWEREREVnAh2YRERERkQV2Qp5x69atOT1L0oCE5AaZvqXUeqZSydWgk07OtHSOOTlz5swcp8sAFdzopNnXZQUdNxAqzpDpdJKh5NxyzjR/khDkeSmd3nGxOGgtNkHpd1ofckLYNnVPY6P+yQWF2tN65mdzf1NqQfG680t+Pv8tx5QyjGxD58g+87uVn02ZUn4vqUAJQbIeatMpcrOpmAtdzyIicrzwTbOIiIiIyAI+NIuIiIiILLAz8oxVSjZTs1TYgIpgUPqdUtydAhRXr16d42vXrs0xyTZy/JcuXZrj8+fPzzG5EmSaN9PYJAeoulMmQRKLhNaC5ArkMJLz7KTTyd2C0uMdhweSD9CaJh3ZQ+cayvad4ikk4chrguQAJJFISCKR8qBcw2yzXsyEZFBUIIeu047sJs+dYyJXG7o3dAqNdMazJMnImKQfIiJyvPBNs4iIiIjIAj40i4iIiIgssBN5RZJnkPSA0quUQu4UkUjyXCm9uHHjxhynVCPjK1euzPHly5fn+PHHH5/jc+fOzfHZs2c3jp/i9TGTG0h+puMIQSlukgdQUY+Mc/9oD8ixgGQGJM+gQjIESUQ67ZOO3IXGQ9cuzZckNAlJCchJI9f5IBeIzjWUcyBJDfVDnyXHDCp0QoVLko5LBknAlGeIiJxcfNMsIiIiIrKAD80iIiIiIgvsRF5xmqb5V/KUjk3JBKXWKd2dqdZM2VJ7SrfmL/nJMePVV1+d45dffnmOX3rppTlOV42LFy/OcRZDOX369ByTLGJ9DnScUtadwh8k88gUP6XBcwwkz+g4RWwrLcjxkIyh42jRGX/SkTBQ/+QcQp/t9J/9kKPIQZKMXLuU2tD5aK1JdpOQe0auOzmb5L2B2nfcTGhdcu0sbiIicnLxTbOIiIiIyAI+NIuIiIiILLAz8oyVFIPS4J2UO0kP1os2bDpOv/An146UZ2ShE4pfeeWVOU6pxoULF+Y4pRp5PGUb6bZR1XOxSHLtcq23lXCQrCKlFDm2XMccG7kmZIqeCnCQjIZcJjrOCh03DOqn04ZkAiQPIjcM2uuDCuFsIse2/t0jV4okz0dxQlINug461yUV9TlMoROSs2y6/pRniIicDHzTLCIiIiKygA/NIiIiIiIL7Iw8Y5XaplQ2peWpfcelIKGUeB7PIiYk1cg409tXr16d4yyAklKN8+fPzzHJNjKuqtrb25vjlHFQejmh1HyOm6QO5GBCkolOujulHZmipzQ+SQA6Y062lRhQn5013Fb+sa18gMa/rfNEFRcT6YyvUzSE1rcjf0loDtQ+oXtGp9DQKu4UxxERkQcf3zSLiIiIiCzgQ7OIiIiIyAI7Ic+o+nIadttiJQnJEDpOA0nKBDqSgSxokmQxFCrUkpKPlHCkbCMlGY8//vgd53jb29628d+yOEo6g1DKPuUE5EZABWY6ThHJNoUj1mOSInQkAx0HiI6Uh5wbqJgGre22UgKaL82d5Dc5BiomVHVnQZNt5Rkd9xNaa5obFS7pFIBJqP02kowcv/IMEZGTgW+aRUREREQW8KFZRERERGSBnZFnbEqrblv8gVKz26ZPM1WeMoyOe0Eez+Im6aqRafBMgWfaPKUa2U8eX+/3tddem+NLly7NcRZEyfnkemW6O8eX6fuMKVVO5BqRFIHS9STJIKlNp7BLx2mFzkv95LrldUDSBlpDkkzk8dzH/A505BnU/7o8gyRFCckb6Jro7FNHhkJr15GRdCQcHSeQVZ/KM0RETga+aRYRERERWcCHZhERERGRBXZCnjHG2FgUg9Lm+Wt8kk90UrDkAJGp6Ey95nkpbZzzSOeNlE7cuHFjY5xp85RtpMNGHl8fa7p1ZJxFU7IASq4duWqQlIQcIToFRHKNcgzZfzp+PPbYY3Oce0COKp0CJR3nCuqH9j7pFOggFwsaT8owyMllW/cMkn+styO5BRWqSTpSG4qpeAy5utC+dsaQ0L6KiMjJxTfNIiIiIiIL+NAsIiIiIrLAzsgzKLW7IlPxVFCBCidQ0RNyFOgU7khSMpDnyrT53t7eHKd0IuUZJOGgIhPrfV2+fHnj+PLz2T4lEJvkMVV3pr6zn4zJZYFS+pkqzzVKOUsWZ8k4pRrZftv0e0LX07bFN0gyQBIIck4huUuOM691cnIhWQhd9+v7ta3UIaE2JKmhmOaQMX13ae3oeMeZZVOslENE5GRwqDfNY4zzY4yfH2P8wRjjE2OMbxpjXBxj/PIY44/3//vCUQ1WRETuHu/ZIiJ3z2HlGT9WVf92mqa/VFV/pao+UVUfrqqPTdP0nqr62P7fIiJy//GeLSJyl9y1PGOMsVdV/3FV/ZdVVdM0vV5Vr48x3l9V37Lf7Ker6ler6gcO6uvUqVNz2p2kEZRCpzaUviZ3gYQkBp0UeqZ106mCxpz9k2wjC5jk8YPIlHUWR8n5p2yg46RBUo2UlZBsg1Li5KRB8oxc05SXUJGbbd0RSCbQceToyCE60oiOxCXbdK7vjsxh3bWDXDxo7WgOVEyFpB0kFepIpTpFd0h20jme8Wr8D4o84yjv2SIiJ5HDvGl+d1W9WFX/cozxm2OMfzHGOF1VT07T9Ln9Np+vqicPO0gRETk03rNFRA7BYR6aH66qr6+qH5+m6euq6katpfWm269gNr6GGWN8aIzx7Bjj2XxTKSIi94Qju2ff85GKiOwgh3HPeK6qnpum6eP7f/983b4BPz/GeGqaps+NMZ6qqhc2fXiapmeq6pmqqre//e3TJnkGxZ10KLlYZOqXpB2bUrDrZBvqh+Qi6fpAzgfZP7lqVN0p6VgvfLI0JirikuTcMs1OMTl1kOSA3Cpynh1XjY48oyNRIDpODxRTP4cprEHSg2372fZ7ddDnScaQkozO/Enu1HG1oeIxHScXumdkvKm4Tuf62RGO7J49xngwNCkiIkfIXb9pnqbp81X1mTHGX9w/9L6q+v2q+mhVfWD/2Aeq6iOHGqGIiBwa79kiIofjsD7N/3VV/cwY4y1V9amq+t66/SD+r8cYH6yqT1fVdx3yHCIicjR4zxYRuUsO9dA8TdNvVdV7N/zT+7bp56GHHppdEShtvq1zRcaZuiepRh5PUnpAjgBUXIHcBDItTXKDlCTQeKrulG50XCwSSit3imtQe1ojkrNkG5KakJPI1atX5zj3j6QRVBAjx0xpf5K45DWUqXtK6VOb7CfjpaI/B425E5P8oWp7+Qh9X8mxhch1oUI1NB+ShdA12hlD7ll+L1fHO1KZXeGo7tkiIicRy2iLiIiIiCzgQ7OIiIiIyAKH1TQfCadOnaqzZ89WFf8Sngok5HFyYsh0d6ZXM/WdKdhMWVMqOlPFKR+ggh5UGCSP55iziEfG6zISOgfJNnJ8tI40vpRP5GezyEjKJ3Kt87MkIyG3jWyTMUkayKGB5DKUxqe0e+4TuaKQ7GZ1na+Pk2QbJDshGVOnuAe5UKzLWjoOI/n5jsyFvt/UJ7midPrP+dM1RFINkt3ktb6KHyD3DBEROQS+aRYRERERWcCHZhERERGRBXZGnpEp7BXkskASg05alwoYUBtKxWfKNo+nVCPlBpnCJWeI7DMlGZkqvnDhQiWZLs6xdoqPkDSCXCwyTolFSjLys9euXZvjnOf169c3xtme5BzkzpFr1CmaQal7kqzktUXX0LqzyaaxrYr4rEMyDyqEQ7IccizpuM+sywxIxkFOKEm2OUyRomyT86c2dK7Od4DcUvJ7uUnK8yC5Z4iIyN3jm2YRERERkQV8aBYRERERWWBn5BmrNDQVMMg0NUk16Nf7CaWHM+VOKXH6RX3GV65cmeOUHpDEII9nsY4cQ6b0032hil02cj4pPaHUfMdJI9ukfCLlGSTDyOOvvvrqHKck4+WXX954PNeFZCR03ZBUI49TSp/ibYt+ZPvcF3LMIHlGSjhIekB0CtmsO0mQE0dHYkHr22lPxWmSXMdt+yGpSWeOm64t3TNERE4GvmkWEREREVnAh2YRERERkQV2Qp6RUCqb0tpUBCOlBJl2zeNJtsk0eEoy0qkij1Mhlewn45Qz5HlTbkBSjfXiJnnuXLuUdOS4yfWD0uYk56C1zrml9CSlGjm3lGqkPOOVV17ZGOdnU/JB4+y4Pmx7DSUk60k3mM5eUEGTjPNcNMe8/shJoyO/WW9HEoUk5RAkjchrlKQUnZj6oe9f7g25qHQK22xax44LiIiIPPj4pllEREREZAEfmkVEREREFtgJecY0TXNqlH4VT2nXTF9TgQuSDGRMzhiZTqc0cLZJOgVTUpKRad5MlackYb3AQ6aacz7pspGuGjnWlBCQJCDT2jluSklnuj/T3bkHGafbyOXLl+c4pRok20iHjZx7jqHjjkDynZQnkPwjyXUjxxPaCyq0Q04dBMlRSJJB67b+GXIkybGS0wyNiVxdtp0/nTevXZJ9ZXty4iFW49x2j0RE5MHEN80iIiIiIgv40CwiIiIissDOyDNWMgWSYWRKlVwpMq1LTg+Ums72lGangib52RwDtUlyXpSizuMpSVj/t3SoIMeGlGRQYZSUE5BUJdco25Ccg5wJcsw5t3TVSNlGyjOyTUpYUv6R8heSW5CDQqc9OVfQulFM1zpJG0heQnFKDzrfhyqeM31HO0VJko6UJCHHEPrukgtOHs896Oz3pr1XniEicjLwTbOIiIiIyAI+NIuIiIiILLAT8oxbt27NKfVMa2cKln6lTylYKgpB0oAslJHkeFLCkPIHSlFTgQQqKtJxaFgvQJHnIFcKWpeUUpw7d26O9/b25phkGzn/bL/temWfly5dmuOUauQ+pVSDYvosyRJy3SktT4VC6BqlYjzk9ECSDLomMs6xkfsHzYXig86dkNtNQvIFkljQuHN9s01exwk5wuR+JCTTIVeN1fFtZSkiIvJg4t1eRERERGQBH5pFRERERBbYGXlGuhysoAIaVJyAHAiyTceVIseS7fOX9ik3yJiKVCSUlqaiEeQysP43pe8ptZ7nyDmQq0ZKODK+cOHCHKdUI49TP7lP2Sb3Mh0/cg+yfbbpuGqkUwSl3yldT7IbKqBBEoZsT9cEjYFcPlJ2QtILcqE4SjoFSg66rld0rl2S2uQ1QXKZjHOceV5yHlldQ8ozREROBt7tRUREREQW8KFZRERERGSBnZFnZOo8j6/IVC6leynVmulTctvI/lOekanZlHDkuTINnL/kJ1lIJ3VNbdbbU4qfUvYkRaCiLCSHSInF+fPn5/jixYtz/MQTT2w8/vjjj88xFWFJco4pf6GiLVSMIiFZAl03JJdJOnvccYzIfek4fnTcMMjxgyQl6+RYj+q6pv47hVtoPDTnJMeW13reG7INzXc1BoubiIicDHzTLCIiIiKygA/NIiIiIiIL7IQ8480335ylD1TYgdLReZyKSGSbTEFT8QMqokCp8pQ8dPokuQXFVDRj/W8ad8pN0jUij5NUI1PWVLgk45RqZLGSlGq87W1v29g+JR+5drSXue4kOyE5RMZ0bZEcICFJA0k4Os4Ymxwaqu7cL3LJIDkDuWckB8kzyA2Err/OdU2Qq0bGtPfbSlI6jjtLc1eeISJyMvBNs4iIiIjIAj40i4iIiIgssDPyjMuXL1fVnenVdKKgdH26bqR8INtT0YLsJ50YMqY0cKZ78zil/ZNMY1MKuVP0pIrdQKg4Q6fAB507+6cCEeSwkY4ZGaeEIwujpBsGuaLQHpAc5caNGxtjcksh94WE0vgkSSB5Rsotcjy5XzTmjmMEOdEcVNyEnDE6hYM6cUISB5LUZHuScREdGUbee5J7VQxGRER2H980i4iIiIgs4EOziIiIiMgCOyHPeOONN+qll16qqjvT0ZkiJWcMcnegOFOwme4liUGmY3Ns+dlOGrvjCrKtG8T6OWgc5MxAbgzkrkCFMPJcuWcZP//883OcMowLFy7M8dmzZ+c4pR3ZDxWgSMg1guZOa539kDMErQNJNUhuQPISkpR0pDWdgkA0zvV/68gzSKrRkXCQ9CIhaQ59t8hBJtcxrye6tui7tLo+lGyIiJwMfNMsIiIiIrKAD80iIiIiIgvshDzjzTffrFdffbWq7kydposFpYEzTseMlFtkP3mcUsX0y/nsn1L3lNLOFG6mjVPykZCEI9PM6/9GULGIlCtQoQ2C5Aq5f9lm5Y5Sded+pAwjHTMyzvbkonKQzGAFzZEkGUlHgkN7T5DTSu4LuZ3kcZKXdMZPa1jFDiAdeQbtE0k16BrNc+X3huaTa0f9d+RKtC5UbEVERI4/vmkWEREREVnAh2YRERERkQV2Qp5x69atOfWcKeiMOy4RHXlGxikByF/O57lIGpDpXnLzWE93r8i0bs7x+vXrc7ySq1Txr/fX/86+EnJUoKIY28ozkuwzybFduXJljnOeHUlNrm/uB7kgdGQbHfeDjryhU8SDzktSjVzPPN5pTxKijEmutA7JM2g/SJ6R7beVZ3RkUHmu7JPa5zrm2qXkJfvc9D3RPUNE5GTgm2YRERERkQV8aBYRERERWcCHZhERERGRBXZC01z1ZZ3hYaqbkeVV2kplnH2mvjkt57If0oV2qhJmmzxvjift2s6fPz/Hr7zyysZ+qqquXr1amyBta0JV2EhrS1XYEmqT48k+0+YrdadUAXJbu0HS3JIWuVM5jyzXqIpcQv0nVJEx50JtyB4toTVcv7ZoTTtxR9NMlRKpAiZdc/S7hmzf0aHnetF9SERETi6+aRYRERERWcCHZhERERGRBXZCnvHQQw/NcoS0XaOqbQkdp9RspoTJIivlGWRlRp8le7uMyYoupRopu0h5xt7e3h2fefHFFze2S6lHSiNyXaiSXI4jj2d7SpuTjRhZipGlWsYkjci4s5cpH6A9pr1JSFZAc+9IMjqWdtlnjn9b60CSl6zLMzrSC1rfbEPnS0iSkZ/NtaZ+aK1JOtOp2tmRMYmIyPHHN80iIiIiIgv40CwiIiIissCh5BljjL9fVf9VVU1V9TtV9b1V9VRV/VxVXaqq36iqvz1N0+vYSd1OtV66dKmq7kzxphygk8bfNiV+0HhWUEq/k65OR44zZ85sbJOfJVeJrJr3xBNP3DHWF154YY5ffvnlOc6qe9kXVY9LSQZVZSTJR/ZJ0oVcU3J46EgdyMkgx0PV6yil33FWoPFThTuqQNe5LkmCQo4c27qd0PqsS1NoLcidpFMZk64PmgPJM0hi0akASVB1QJIKrc77IMk0juqeLSJyErnrN81jjHdU1d+tqvdO0/SXq+qhqvruqvrhqvrRaZq+pqouV9UHj2KgIiJy93jPFhE5HIeVZzxcVW8dYzxcVY9V1eeq6lur6uf3//2nq+o/O+Q5RETkaPCeLSJyl9y1PGOaps+OMX6kqv60ql6rqv+tbqf2Xp2maZXPfK6q3rE4iIcfrgsXLlTVnXKFjitASjjI3SHTtFQ0g1wySG7RKaBB8WOPPbYxXncvWHHx4sU5Xpdn5L+ljCPjmzdvznFKNXJNU56R7dPFI/tMqUb2mSluKlhB+5pxR1pAjggkGaD9TmeT3Fdy5+gUWyFpA0kskpzXtpKHTv90roPkDB33CWqzbXEQ6oecRGjvO3IWuuZI9rVJOvKgyDOO8p4tInISOYw840JVvb+q3l1Vf6GqTlfVt23x+Q+NMZ4dYzybD10iInL0HOU9+x4NUURkpzmMPOOvVdWfTNP04jRNX6qqX6iqb66q8/upv6qqp6vqs5s+PE3TM9M0vXeapvfm21wREbknHNk9+yszXBGR3eIw7hl/WlXfOMZ4rG6n+t5XVc9W1a9U1XfW7V9jf6CqPrLU0RhjlkFk2plcGag4Rr6xpuIE5HxAaXByz6BiJZTuJqeATCF3CnScPXu2kpSMZGGYbeUZKW3J9pcvX94YZ/95XnLqyPmTJIOKcWyb3ienh9yzXMdcw5TLkENKR/JBbQgqoNEp4EKyjW2lGusyg45EgfaV9p6cbzouKvTdIoeUTqEduq+QhIO+xw8IR3bPFhE5idz1m+Zpmj5et3888v/UbeuiU1X1TFX9QFX9gzHGJ+u2hdFPHME4RUTkEHjPFhE5HIfyaZ6m6Yeq6ofWDn+qqr7hMP2KiMjR4z1bROTuOdRD872A5AqZgibZA8kQ0hmCoPQz/ZK/416Q6eccQ8dNgdLPKR9Yb0euHLQWlE7P9itXk6o7JRlZSIWkGnkuSulvm/ZPKD1O7hOdwjN5nFw1tpVkrBcN2QTNkZwhcjzk5NIp9HFQ4RiSQVFRHJLm0PcgY9pLkl7QPGnOnQIw9F1cKhjzAMo0RETkLrCMtoiIiIjIAj40i4iIiIgssBPyjGma5pRvJ11K8oaUc1ABFHIEeO211+Y4090p+aCUex6nX+Nn6jrPlTFJBlJWcJA9X84/3SFSZkBSFSo+kmPKPnN8586dm2Ny7SD5QdJJoZOEg/oh94lcx5Sy5Hw7xWzoOui4WCyl/atYakJzIblIp9DH+nrmdUDynfw8OdaQMwtJhcj9hL73JIsheQa5dnQKvWySnWzr7iIiIg8mvmkWEREREVnAh2YRERERkQV2Qp5R9eU0aaZLyXWAfiFPRQhIqkFt6NfwJGGgFDI5CCSZTu+4O6wXN6HiHbkueQ5K01PBEXIsIHlAxjdu3JjjXK9OGpz2gGQMHdkGpfGpaA25Z3ScGzpsm9bvuEds6+BBRTyqeG86c+7IM1KalN+PvEbzukk6kgwqbpJ0nC8615yIiBx/fNMsIiIiIrKAD80iIiIiIgvshDxjjDGnWKlwCRUHIYlF/sI/08PZz7btM7VM7hSZKiY5R0eqkX3muc6fP3/HZ9K5Ym9vb+PnU36wrRwi0+/ZJzk8JDmfXFNKoWdMEg46V6d4CkGyh20LaNB4ks54qE/qvxMftrjJYQrSkDwqv08kw8jjnWuOigLR3HcKCwAAEJBJREFUfYVkSTT3TfOyuImIyMnAN80iIiIiIgv40CwiIiIissBOyDNOnTo1u0Vk+jOlESSfINlDfjbbdNwzsj0VJbl69eocp2whU8KZtqXxkFtISgMOkmdcunRpY5zt8vPkqNBJ8edYqQgNOWwkJM/YtmgIFa+g1DrNhTjIWWLT+JPDyEVIskKSBzpXx2HioDWh81FxkE6xFoIKo3TORXFef3nd0FzoXEttlGeIiJwMfNMsIiIiIrKAD80iIiIiIgvsjDxjJSHIVGdKIzJNnUUzsk3HoYJcEMiRI/u8du3axjGkO0XSkWd0xvnYY4/NcbplVFW9+uqrc5ySkZRqXLhwYeNYUw7RKeCQx8kRISUsOc9OkYrcA9onak/7R3tJMckVSFLSoVN4JaFzUcGavLZIHkPrdtDYyE2C5E4kGSFHCxoTSbQ612iOjVxt8rM5/k68aU2UZ4iInAx80ywiIiIisoAPzSIiIiIiC+yMPGMlQcg0akoJKKVMsgdK3Wf6Oo+TU0Ln1/Up1aAUdcpISNpAqftHH310ji9fvnzHv6UkI+OUbVy8eHGOVy4l6/12XCA6a0Spe5Jn5HmpIEueK8dMbhsJpfFpzCTVoBR8p/BKsq1UIyGpBhVnSXkGrRU5kBw0VpIu5Hcx2+e4c0y5xyRTIqeP/D5lG7ouSQpC30W6r2zrhCIiIscH3zSLiIiIiCzgQ7OIiIiIyAI7Ic8YY8wp40y/0y/eO8VKyIGAUtlU6CPPRcVWOoVXqB9yHyD5R465qur69etznPKMlHHs7e3NcRY6IXcBkhmQ1IGKTpC0Jcnz5t6nY0jGJOEg2Q3JCnIPOvtBUgq6zigmiUvHsYRikmqQPIOKfqxLdEi60ZG2UDGYHEfuX54rx5d70ylukvuXdNxD6Dua89207so0REROBr5pFhERERFZwIdmEREREZEFdkKeMU3TnBrN9C2lbDNFT+lbSo9nPykHICcJcupIyURKJDINfPPmzY1jy5jkJZkqzvRvFg9Z/zuLr6Q8I+UNZ86c2XicCmF0HCEoPd2RFpCzSY4tHT8yzjb52Rx/J/1ObhDkmkCSDCrcQc4eJDEgmQ4d31aqQd+BdekPuWx0JDgkaaDvIsl0qBAQ9U8xyYNI9tU5l/IMEZGThW+aRUREREQW8KFZRERERGSBnZBn3Lp1a5YyZOo308WZAs20MRVIyHRvQr/e7zgxZPqWZALZT7pZ5JgzptRupxDH+t8dyQjNOVPilDandH3neEJzzvY5l5S5pByFnDS2dV3pHCcJBEkyKCa5S0fyQGPruHyQewZdD+vtqPgNuX6QAwhJW+h7mWwrC6H228o2aI60JiIicjzxri8iIiIisoAPzSIiIiIiC+yMPGOVds8UNBVhyBQpSQ/WnQA29UPFTagASkoY6FzkUpBtyCGECk1QwYaqniMErRc5FtCcaT702U5BCYLcRjKdnvPN/SD5Syeln+vWcajY9noiKUuyrZNGrgPJNvJcOc6UuGRcxYWGSHaTkASHCg11rkWSQ9D+dWJyBck55rp33GREROR44ptmEREREZEFfGgWEREREVlgJ+QZWdzkC1/4wnw8086U+k7oV/cEySHWx7apfUoVqE9K0ZPMIR0vsk2uyfp5M9Wc58u16xTyyJgkL5m+T8cQSq1T+j0hx4WEJChJptBJnkHyho4coiPPIGcFWofO9dcpnkJSFpKmUNGWdSeJXGtyVMnxkXSBHDOSbd1uOgVs8nuTrisZ03jI/SPXtyMzEhGR44NvmkVEREREFvChWURERERkgZ2QZyT06/9Mi5K8oVOUI6FUNqVj8zilzTOF3Cne0JE/ZJo546o7U+hZECTb5XGSZCRUzIEkCrnu5MaQbcixoFOkI8m9yetmvQDMpvHTHDvQZzvXU5LXB11PtM553ryGSJKQe00FPdaP0z6RowVJNahNksdznvSdozF0HD9IRkJ7RgWOVm0sciIicjLwbi8iIiIisoAPzSIiIiIiC+yEPGOMMadPM3VKMomMqZhGyhuomAil96nABf1yngpckGzhzJkzG+Nz587NcTppZJp93T0i/87U/M2bN+c45Rl5PPslaURHVpJrnfM5e/bsxvbkOEEODx1Hi23pFJXJuJO6J0kCnavjqNIphEPXce5v7nteJyS/OYjOHlAxm873m6Q2eR3ketGekZyF+iSZDklnVuO34ImIyMnAN80iIiIiIgv40CwiIiIissBOyDOqvpwCzZRqpk47KVAqPkK/qM90LBUJyVR2jofS0lTog9Lsed5Mp1+7dm2OU6qx7nhBspLsK+UZFKfMg9wnck1ThrG3tzfHKTHJ4+SkkeT60lxyP0hGQ2n2jqPDYZwVEkrp03WZ8pU8TtdQR3JEzirksrK+77kf5LhBkJSk42LRkU2RIw7JP5JOwRhqn+u+us50zxARORl4txcRERERWcCHZhERERGRBXZGnrEiU6TbuiNkWjfTyZTKzuOdtHGmu8k9gyQMGVPxBipIklKNlCpUcdGKHGvHSSPj/Gyuac4z5RbpkpFxSjXSYYOcD3JNSVqQUhWSl5AjQp4rrzMqgtGRbRCU9s9rriPPyHUm2QZJFVLaQNdArme2qeLiKB0pzLbrS8dJQkUyjE3uFutj7sg2qHhKjkd5hojIycK7vYiIiIjIAj40i4iIiIgssDPyjFWqk351TylVKlqQUgci08aUKs40LaV7sz2l1tNJ4vTp0xv7p3T61atX5zhT61Xs7pGp7JQuZMq9U/yi46SR80wZRsY5Z5IWdNxMkk5hDiqssW1hEZJqkGyjIwXpOGnk2ubx/Gz2mePJuVNhGprver8d15KkI4Mi2QZJpfJcOYbce5J3bVsgh66VxOImIiInC980i4iIiIgs4EOziIiIiMgCOyHPmKZpTiV3ihYkVMyBZBvZhlLcyaZfyx8EpXUz/Z5OGilbIPeIbJNuB1V3yjUyZd1x1cg40+/kREFFPXKeJAMgaQE5D+T4sw2l3GlsVCCn42hBRXE6chFaHzpOMomMt5Uz5LlyLjlOchqpYukFrW9n/jSfbYvN0BxozHm84/6R1xOt6eq48gwRkZPB4pvmMcZPjjFeGGP8bhy7OMb45THGH+//94X942OM8c/GGJ8cY/z2GOPr7+XgRUTkz+N9W0Tk6OnIM36qqr5t7diHq+pj0zS9p6o+tv93VdW3V9V79v/zoar68aMZpoiIbMFPlfdtEZEjZVGeMU3T/zHGeNfa4fdX1bfsxz9dVb9aVT+wf/x/mm7nNv+vMcb5McZT0zR97qBzjDHmFDO5Z2ybds30frpEUFq7U+hkWycGkjmQ8wa5KVB6e/3f8nzkcJBSFXLboDXtpKy3lRmQ9CLHSS4qVGAmJSsk4eisO7lM5PhJvkNykY7zRkL95/rQdbmte8T6eMj1gyQNdJy+c+SkQeuVY6XroOOi0omJbEOSrl3gK3HfFhE5adztDwGfjBvq56vqyf34HVX1mWj33P6xP8cY40NjjGfHGM+uVyITEZEj51D37bxn39thiojsJod2z9h/O7Fdvevbn3tmmqb3TtP03vTzFRGRe8vd3Lfznn2PhiUistPcbX7x+VX6bozxVFW9sH/8s1X1zmj39P6xgzt7/vmXfuRHfuTTVfV4Vb10l2N6EHG+x5uTNt+qkzfnx6vq9GKr3eAo79svVZX37OPPSZtv1cmb80md739wNx++24fmj1bVB6rqH+//90fi+N8ZY/xcVf3VqrrS0cVN0/REVdUY49mT9BbD+R5vTtp8q07enPfn+677PY4mR3bf9p59Mjhp8606eXN2vtux+NA8xvjZuv3jkcfHGM9V1Q/V7Zvuvx5jfLBuv234rv3mv1RV31FVn6yqm1X1vXc7MBERuTu8b4uIHD0d94zvgX9634a2U1V932EHJSIid4/3bRGRo2fXymg/c78H8BXG+R5vTtp8q07enE/afNc5afN3vsefkzZn57sFg3xNRURERETkNrv2pllEREREZOfYiYfmMca3jTH+cIzxyTHGh5c/8WAxxnjnGONXxhi/P8b4vTHG9+8fvzjG+OUxxh/v//eF+z3Wo2SM8dAY4zfHGL+4//e7xxgf39/nfzXGeMtSHw8S+5XUfn6M8QdjjE+MMb7pOO/xGOPv71/PvzvG+Nkxxlcdtz0eY/zkGOOFMcbvxrGNezpu88/25/7bY4yvv38jv7cc93t2lfftk3Df9p7tPXvbe/Z9f2geYzxUVf+8qr69qr62qr5njPG193dUR84bVfUPp2n62qr6xqr6vv05friqPjZN03uq6mP7fx8nvr+qPhF//3BV/eg0TV9TVZer6oP3ZVT3jh+rqn87TdNfqqq/Urfnfiz3eIzxjqr6u1X13mma/nJVPVRV313Hb49/qqq+be0Y7em3V9V79v/zoar68a/QGL+inJB7dpX37RXH7TudeM8+fvv7U3Uv79nTNN3X/1TVN1XVv4u/f7CqfvB+j+sez/kjVfXXq+oPq+qp/WNPVdUf3u+xHeEcn96/OL+1qn6xqkbdNhR/eNO+P+j/qaq9qvqT2v+dQBw/lntcXy69fLFuu/D8YlX9J8dxj6vqXVX1u0t7WlX/Y1V9z6Z2x+k/J/GevT9P79vH5Du9Pxfv2d6zt75n3/c3zfXljVzx3P6xY8kY411V9XVV9fGqenL6chGBz1fVk/dpWPeCf1pV/6iqbu3/famqXp2m6Y39v4/bPr+7ql6sqn+5n9r8F2OM03VM93iaps9W1Y9U1Z9W1eeq6kpV/UYd7z1eQXt6Uu5lJ2WeM963j+V32nu29+yt72W78NB8YhhjnKmqf1NVf2+apqv5b9Pt/5tzLKxMxhh/o6pemKbpN+73WL6CPFxVX19VPz5N09dV1Y1aS+sdsz2+UFXvr9v/w/MX6nYp6fWU2LHnOO2pbMb79rHFe7b37K3ZhYfmz1bVO+Pvp/ePHSvGGI/U7Rvvz0zT9Av7h58fYzy1/+9PVdUL92t8R8w3V9XfHGP8f1X1c3U71fdjVXV+jLEqqHPc9vm5qnpumqaP7//983X7hnxc9/ivVdWfTNP04jRNX6qqX6jb+36c93gF7emJuJfVyZmn9+3jfd/2nu09e+t72S48NP96Vb1n/xecb6nbwvSP3ucxHSljjFFVP1FVn5im6Z/EP320qj6wH3+gbmvmHnimafrBaZqenqbpXXV7P//9NE1/q6p+paq+c7/ZsZlvVdU0TZ+vqs+MMf7i/qH3VdXv1zHd47qd4vvGMcZj+9f3ar7Hdo8D2tOPVtV/sf+L7G+sqiuREjxOHPt7dpX37Trm923v2d6z627u2fdbsL0vvv6Oqvqjqvp/q+q/vd/juQfz+4/qdjrgt6vqt/b/8x11Wy/2sar646r636vq4v0e6z2Y+7dU1S/ux19dVf93VX2yqv6Xqnr0fo/viOf6H1bVs/v7/L9W1YXjvMdV9d9X1R9U1e9W1f9cVY8etz2uqp+t2/q/L9XtN1MfpD2t2z+a+uf797Hfqdu/Ur/vc7hH63Ks79n7c/S+PR3v+7b3bO/Z296zrQgoIiIiIrLALsgzRERERER2Gh+aRUREREQW8KFZRERERGQBH5pFRERERBbwoVlEREREZAEfmkVEREREFvChWURERERkAR+aRUREREQW+P8B7DVvrHxxQvcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_mU9em5-0th"
      },
      "source": [
        "\n",
        "Compute salt coverage (this will serve as a basis for stratified split):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDFVEDBN-mKo"
      },
      "source": [
        "train = compute_coverage(train, y_train)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXPVHJn1-58v"
      },
      "source": [
        "\n",
        "Prepare data for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIWQbXTJ-3R_",
        "outputId": "a58c42d4-8d21-48be-f8d6-dec32c3e5dfc"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=5, random_state=1337)\n",
        "\n",
        "# Add channel features\n",
        "X_train_ch = np.repeat(np.expand_dims(X_train, axis=-1), 3, -1)\n",
        "X_train_ch = np.asarray(list(map(lambda x: create_depth_abs_channels(x), X_train_ch)))\n",
        "\n",
        "# Resize to 224x224, default ResNet50 image size\n",
        "X_resized = np.asarray(list(map(lambda x: cv2.resize(x, (224, 224)), X_train_ch)))\n",
        "y_resized = np.asarray(list(map(lambda x: cv2.resize(x, (224, 224)), y_train)))\n",
        "\n",
        "for train_index, valid_index in kfold.split(train.id.values, train.coverage_class.values):\n",
        "    \n",
        "    X_tr, X_val = X_resized[train_index], X_resized[valid_index]\n",
        "    y_tr, y_val = y_resized[train_index], y_resized[valid_index]\n",
        "    \n",
        "    break\n",
        "    \n",
        "y_tr = np.expand_dims(y_tr, axis=-1)\n",
        "y_val = np.expand_dims(y_val, axis=-1)\n",
        "\n",
        "print(X_tr.shape, y_tr.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "\n",
        "del X_train_ch, y_resized\n",
        "del X_resized\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3200, 224, 224, 3) (3200, 224, 224, 1)\n",
            "(800, 224, 224, 3) (800, 224, 224, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoJU8vQf_ANd"
      },
      "source": [
        "from keras.losses import binary_crossentropy\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# Dice & combined\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred = K.cast(y_pred, 'float32')\n",
        "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
        "    return score\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return 1. - score\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "\n",
        "def bce_logdice_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n",
        "\n",
        "# Lovash loss: https://github.com/bermanmaxim/LovaszSoftmax\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    gts = tf.reduce_sum(gt_sorted)\n",
        "    intersection = gts - tf.cumsum(gt_sorted)\n",
        "    union = gts + tf.cumsum(1. - gt_sorted)\n",
        "    jaccard = 1. - intersection / union\n",
        "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
        "    return jaccard\n",
        "\n",
        "# --------------------------- BINARY LOSSES ---------------------------\n",
        "\n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        def treat_image(log_lab):\n",
        "            log, lab = log_lab\n",
        "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
        "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
        "            return lovasz_hinge_flat(log, lab)\n",
        "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
        "        loss = tf.reduce_mean(losses)\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss():\n",
        "        labelsf = tf.cast(labels, logits.dtype)\n",
        "        signs = 2. * labelsf - 1.\n",
        "        errors = 1. - logits * tf.stop_gradient(signs)\n",
        "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
        "        gt_sorted = tf.gather(labelsf, perm)\n",
        "        grad = lovasz_grad(gt_sorted)\n",
        "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        return loss\n",
        "\n",
        "    # deal with the void prediction case (only void pixels)\n",
        "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
        "                   lambda: tf.reduce_sum(logits) * 0.,\n",
        "                   compute_loss,\n",
        "                   strict=True,\n",
        "                   name=\"loss\"\n",
        "                   )\n",
        "    return loss\n",
        "\n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = tf.reshape(scores, (-1,))\n",
        "    labels = tf.reshape(labels, (-1,))\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = tf.not_equal(labels, ignore)\n",
        "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
        "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
        "    return vscores, vlabels\n",
        "\n",
        "def lovasz_loss(y_true, y_pred):\n",
        "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
        "    #logits = K.log(y_pred / (1. - y_pred))\n",
        "    logits = y_pred #Jiaxin\n",
        "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
        "    return loss\n",
        "\n",
        "# IoU metric for observation during training\n",
        "# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\n",
        "def get_iou_vector(A, B):\n",
        "    # Numpy version    \n",
        "    batch_size = A.shape[0]\n",
        "    metric = 0.0\n",
        "    for batch in range(batch_size):\n",
        "        t, p = A[batch], B[batch]\n",
        "        true = np.sum(t)\n",
        "        pred = np.sum(p)\n",
        "        \n",
        "        # deal with empty mask first\n",
        "        if true == 0:\n",
        "            metric += (pred == 0)\n",
        "            continue\n",
        "        \n",
        "        # non empty mask case.  Union is never empty \n",
        "        # hence it is safe to divide by its number of pixels\n",
        "        intersection = np.sum(t * p)\n",
        "        union = true + pred - intersection\n",
        "        iou = intersection / union\n",
        "        \n",
        "        # iou metrric is a stepwise approximation of the real iou over 0.5\n",
        "        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n",
        "        \n",
        "        metric += iou\n",
        "        \n",
        "    # teake the average over all images in batch\n",
        "    metric /= batch_size\n",
        "    return metric\n",
        "\n",
        "def my_iou_metric(label, pred):\n",
        "    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n",
        "\n",
        "# For Lovash loss\n",
        "def my_iou_metric_2(label, pred):\n",
        "    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP95_rWv_WHz",
        "outputId": "8a23b470-98c4-4eaa-88be-cdea88d3bc48"
      },
      "source": [
        "input_size = (224, 224, 3)\n",
        "\n",
        "base_model = ResNet50(input_shape=input_size, include_top=False)\n",
        "base_model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI5mzPGf_wKA"
      },
      "source": [
        "Decoder blocks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srktrq_F_ZRM"
      },
      "source": [
        "# Basic decoder block with Conv, BN and PReLU activation.\n",
        "def decoder_block_simple(\n",
        "        layer_name, block_name,\n",
        "        num_filters=32,\n",
        "        conv_dim=(3, 3)):\n",
        "\n",
        "    x_dec = Conv2D(\n",
        "        num_filters, conv_dim,\n",
        "        padding='same',\n",
        "        name='{}_conv'.format(block_name))(layer_name)\n",
        "    x_dec = BatchNormalization(\n",
        "        name='{}_bn'.format(block_name))(x_dec)\n",
        "    x_dec = PReLU(\n",
        "        name='{}_activation'.format(block_name))(x_dec)\n",
        "\n",
        "    return x_dec\n",
        "\n",
        "# Decoder block with bottleneck architecture, where middle conv layer\n",
        "# is half the size of first and last, in order to compress representation.\n",
        "# This type of architecture is supposed to retain most useful information.\n",
        "def decoder_block_bottleneck(\n",
        "        layer_name, block_name,\n",
        "        num_filters=32,\n",
        "        conv_dim=(3, 3),\n",
        "        dropout_frac=0.2):\n",
        "\n",
        "    x_dec = Conv2D(\n",
        "        num_filters, conv_dim,\n",
        "        padding='same',\n",
        "        name='{}_conv1'.format(block_name))(layer_name)\n",
        "    x_dec = BatchNormalization(\n",
        "        name='{}_bn1'.format(block_name))(x_dec)\n",
        "    x_dec = PReLU(\n",
        "        name='{}_activation1'.format(block_name))(x_dec)\n",
        "    x_dec = Dropout(dropout_frac)(x_dec)\n",
        "\n",
        "    x_dec2 = Conv2D(\n",
        "        num_filters // 2, conv_dim,\n",
        "        padding='same',\n",
        "        name='{}_conv2'.format(block_name))(x_dec)\n",
        "    x_dec2 = BatchNormalization(\n",
        "        name='{}_bn2'.format(block_name))(x_dec2)\n",
        "    x_dec2 = PReLU(\n",
        "        name='{}_activation2'.format(block_name))(x_dec2)\n",
        "    x_dec2 = Dropout(dropout_frac)(x_dec2)\n",
        "\n",
        "    x_dec2 = Conv2D(\n",
        "        num_filters, conv_dim,\n",
        "        padding='same',\n",
        "        name='{}_conv3'.format(block_name))(x_dec2)\n",
        "    x_dec2 = BatchNormalization(\n",
        "        name='{}_bn3'.format(block_name))(x_dec2)\n",
        "    x_dec2 = PReLU(\n",
        "        name='{}_activation3'.format(block_name))(x_dec2)\n",
        "    x_dec2 = Dropout(dropout_frac)(x_dec2)\n",
        "\n",
        "    x_dec2 = Add()([x_dec, x_dec2])\n",
        "\n",
        "    return x_dec2"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDCIhwib_7Dp"
      },
      "source": [
        "Model definition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egHgo_al_2ZD"
      },
      "source": [
        "# Model is parametrized in a way to enable easy change of decoder_block type,\n",
        "# as this is an argument that can be given a function, like decoder_block_simple.\n",
        "def unet_resnet(input_size, decoder_block,\n",
        "                weights='imagenet',\n",
        "                loss_func='binary_crossentropy',\n",
        "                metrics_list=[my_iou_metric],\n",
        "                use_lovash=False):\n",
        "\n",
        "    # Base model - encoder\n",
        "    base_model = ResNet50(\n",
        "        input_shape=input_size, \n",
        "        include_top=False,\n",
        "        weights=weights)\n",
        "    \n",
        "    # Layers for feature extraction in the encoder part\n",
        "    encoder1 = base_model.get_layer('conv1_conv').output # activation_1\n",
        "    encoder2 = base_model.get_layer('conv2_block3_3_conv').output # activation_10\n",
        "    encoder3 = base_model.get_layer('conv3_block4_3_conv').output # activation_22\n",
        "    encoder4 = base_model.get_layer('conv4_block5_3_conv').output # activation_40\n",
        "    encoder5 = base_model.get_layer('conv5_block2_3_conv').output\n",
        "\n",
        "    # Center block\n",
        "    center = decoder_block(\n",
        "        encoder5, 'center', num_filters=512)\n",
        "    concat5 = concatenate([center, encoder5], axis=-1)\n",
        "\n",
        "    # Decoder part.\n",
        "    # Every decoder block processed concatenated output from encoder and decoder part.\n",
        "    # This creates skip connections.\n",
        "    # Afterwards, decoder output is upsampled to dimensions equal to encoder output part.\n",
        "    decoder4 = decoder_block(\n",
        "        concat5, 'decoder4', num_filters=256)\n",
        "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1)\n",
        "\n",
        "    decoder3 = decoder_block(\n",
        "        concat4, 'decoder3', num_filters=128)\n",
        "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1)\n",
        "\n",
        "    decoder2 = decoder_block(\n",
        "        concat3, 'decoder2', num_filters=64)\n",
        "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
        "\n",
        "    decoder1 = decoder_block(\n",
        "        concat2, 'decoder1', num_filters=64)\n",
        "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1)\n",
        "\n",
        "    # Final upsampling and decoder block for segmentation.\n",
        "    output = UpSampling2D()(concat1)\n",
        "    output = decoder_block(\n",
        "        output, 'decoder_output', num_filters=32)\n",
        "    output = Conv2D(\n",
        "        1, (1, 1), activation=None, name='prediction')(output)\n",
        "    if not use_lovash:\n",
        "        output = Activation('sigmoid')(output)\n",
        "        \n",
        "    model = Model(base_model.input, output)\n",
        "    model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_yLpetbAFJH"
      },
      "source": [
        "Inspect created model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLyirRHBAAxO",
        "outputId": "8ee7fd31-f742-4d12-93d5-34471b45a8ae"
      },
      "source": [
        "input_size = (224, 224, 3)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = unet_resnet(\n",
        "    input_size, decoder_block_simple, weights='imagenet')\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/kernel:0' shape=(3, 3, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'center_bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/kernel:0' shape=(3, 3, 2560, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'decoder4_bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/kernel:0' shape=(3, 3, 1280, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'decoder3_bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/kernel:0' shape=(3, 3, 640, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder2_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/kernel:0' shape=(3, 3, 320, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder1_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/kernel:0' shape=(3, 3, 128, 32) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/bias:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_bn/gamma:0' shape=(32,) dtype=float32>\n",
            "  <tf.Variable 'decoder_output_bn/beta:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/kernel:0' shape=(1, 1, 32, 1) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/bias:0' shape=(1,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Total params: 19,107,712\n",
            "Trainable params: 19,064,832\n",
            "Non-trainable params: 42,880\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QqKvIJIAI1m",
        "outputId": "dc54c305-1a73-4d21-9534-150fadcf668a"
      },
      "source": [
        "model_depth = unet_resnet(input_size, \n",
        "                          decoder_block_simple,\n",
        "                          weights='imagenet',\n",
        "                          loss_func=bce_dice_loss, \n",
        "                          metrics_list=[my_iou_metric],\n",
        "                          use_lovash=False)\n",
        "\n",
        "print(model_depth.summary())\n",
        "\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('unet_resnet.h5',\n",
        "                                   monitor='val_my_iou_metric', \n",
        "                                   mode='max',\n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=True,\n",
        "                                   verbose=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric',\n",
        "                              mode='max',\n",
        "                              factor=0.5, \n",
        "                              patience=5, \n",
        "                              min_lr=0.0001, \n",
        "                              verbose=1)\n",
        "\n",
        "epochs = 2  # 25\n",
        "batch_size = 16\n",
        "\n",
        "history = model_depth.fit(X_tr,\n",
        "                          y_tr,\n",
        "                          validation_data=(X_val, y_val), \n",
        "                          epochs=epochs,\n",
        "                          batch_size=batch_size,\n",
        "                          verbose=1\n",
        "                         )\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/kernel:0' shape=(3, 3, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'center_bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/kernel:0' shape=(3, 3, 2560, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'decoder4_bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/kernel:0' shape=(3, 3, 1280, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'decoder3_bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/kernel:0' shape=(3, 3, 640, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder2_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/kernel:0' shape=(3, 3, 320, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder1_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/kernel:0' shape=(3, 3, 128, 32) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/bias:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_bn/gamma:0' shape=(32,) dtype=float32>\n",
            "  <tf.Variable 'decoder_output_bn/beta:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/kernel:0' shape=(1, 1, 32, 1) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/bias:0' shape=(1,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Total params: 19,107,712\n",
            "Trainable params: 19,064,832\n",
            "Non-trainable params: 42,880\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "200/200 [==============================] - 2443s 12s/step - loss: 2.1689 - my_iou_metric: 0.0865 - val_loss: 1.1472 - val_my_iou_metric: 0.1316\n",
            "Epoch 2/2\n",
            "200/200 [==============================] - 2436s 12s/step - loss: 1.1424 - my_iou_metric: 0.1183 - val_loss: 1.1060 - val_my_iou_metric: 0.1423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTob6K8eUoVz"
      },
      "source": [
        "Validation set prediction and resizing to original size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEPfMYIDUnXT"
      },
      "source": [
        "val_preds = model_depth.predict(X_val, batch_size=16)\n",
        "\n",
        "y_val_pred = np.asarray(list(map(lambda x: cv2.resize(x, (101, 101)), val_preds)))\n",
        "y_val_true = np.asarray(list(map(lambda x: cv2.resize(x, (101, 101)), y_val)))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCAoWQXdVN9t"
      },
      "source": [
        "Threshold optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxe7Icw3VKRi"
      },
      "source": [
        "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
        "    labels = y_true_in\n",
        "    y_pred = y_pred_in\n",
        "    \n",
        "    true_objects = 2\n",
        "    pred_objects = 2\n",
        "\n",
        "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
        "\n",
        "    # Compute areas (needed for finding the union between all objects)\n",
        "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
        "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
        "    area_true = np.expand_dims(area_true, -1)\n",
        "    area_pred = np.expand_dims(area_pred, 0)\n",
        "\n",
        "    # Compute union\n",
        "    union = area_true + area_pred - intersection\n",
        "\n",
        "    # Exclude background from the analysis\n",
        "    intersection = intersection[1:,1:]\n",
        "    union = union[1:,1:]\n",
        "    union[union == 0] = 1e-9\n",
        "\n",
        "    # Compute the intersection over union\n",
        "    iou = intersection / union\n",
        "\n",
        "    # Precision helper function\n",
        "    def precision_at(threshold, iou):\n",
        "        matches = iou > threshold\n",
        "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
        "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
        "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
        "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
        "        return tp, fp, fn\n",
        "\n",
        "    # Loop over IoU thresholds\n",
        "    prec = []\n",
        "    if print_table:\n",
        "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
        "    for t in np.arange(0.5, 1.0, 0.05):\n",
        "        tp, fp, fn = precision_at(t, iou)\n",
        "        if (tp + fp + fn) > 0:\n",
        "            p = tp / (tp + fp + fn)\n",
        "        else:\n",
        "            p = 0\n",
        "        if print_table:\n",
        "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
        "        prec.append(p)\n",
        "    \n",
        "    if print_table:\n",
        "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
        "    return np.mean(prec)\n",
        "\n",
        "def iou_metric_batch(y_true_in, y_pred_in):\n",
        "    batch_size = y_true_in.shape[0]\n",
        "    metric = []\n",
        "    for batch in range(batch_size):\n",
        "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
        "        metric.append(value)\n",
        "    return np.mean(metric)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEPUkwkBWBmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6c7808-2af1-4c77-ff7b-06b20644b7fc"
      },
      "source": [
        "# Threshold range, over which optimization is performed\n",
        "thresholds = np.arange(0.2, 0.9, 0.02)\n",
        "\n",
        "# For every threshold, set predictions to binary arrays, \n",
        "# where values above threshold are treated as 1 and the rest as 0.\n",
        "# Loop over thresholds and compute IoU for them based on IoU function above.\n",
        "ious = np.array(\n",
        "    [iou_metric_batch(y_val_true,\n",
        "                      np.int32(y_val_pred > threshold)) for threshold in tqdm(thresholds)])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35/35 [00:37<00:00,  1.08s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uk6Q58JWF9E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "bb7a637f-48cf-415f-e33b-06c1b5b12509"
      },
      "source": [
        "df_iou = pd.DataFrame(thresholds, columns=['threshold'])\n",
        "df_iou['iou'] = ious\n",
        "\n",
        "# Get index of best IoU\n",
        "best_index = df_iou['iou'].idxmax()\n",
        "print('Best IoU: {:.4f} at threshold: {:.3f}'.format(\n",
        "    df_iou.iou[best_index], df_iou.threshold[best_index]))\n",
        "\n",
        "# Describe IoU DF\n",
        "df_iou.describe()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best IoU: 0.5236 at threshold: 0.860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>threshold</th>\n",
              "      <th>iou</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.276754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.204939</td>\n",
              "      <td>0.138534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.140375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.167375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.202250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.710000</td>\n",
              "      <td>0.379187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.523625</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       threshold        iou\n",
              "count  35.000000  35.000000\n",
              "mean    0.540000   0.276754\n",
              "std     0.204939   0.138534\n",
              "min     0.200000   0.140375\n",
              "25%     0.370000   0.167375\n",
              "50%     0.540000   0.202250\n",
              "75%     0.710000   0.379187\n",
              "max     0.880000   0.523625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oljqWU-WlIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "outputId": "f584027d-c2a6-42cc-f434-71f6c6c1145d"
      },
      "source": [
        "# Plot IoU values over threshold range.\n",
        "df_iou.plot(x='threshold', y='iou')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdba079f350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnD_sQyVXWjf"
      },
      "source": [
        "[Problem 2] Code rewriting </br>\n",
        "Change the code where ResNet was used for the encoder to VGG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0h1_w_9W-PK"
      },
      "source": [
        "from keras.applications.vgg19 import VGG19, preprocess_input"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxA-Y90QX26b"
      },
      "source": [
        "def unet_VGG(input_size, decoder_block,\n",
        "                weights='imagenet',\n",
        "                loss_func='binary_crossentropy',\n",
        "                metrics_list=[my_iou_metric],\n",
        "                use_lovash=False):\n",
        "    \n",
        "    # Encoder part\n",
        "    base_model = VGG19(\n",
        "        input_shape=input_size, \n",
        "        include_top=False,\n",
        "        weights=weights)\n",
        "\n",
        "    encoder1 = base_model.get_layer('block1_conv2').output # (224,224,64)\n",
        "    encoder2 = base_model.get_layer('block2_conv2').output # (112,112,228)\n",
        "    encoder3 = base_model.get_layer('block3_conv4').output # (56,56,256)\n",
        "    encoder4 = base_model.get_layer('block4_conv4').output # (28,28,512)\n",
        "    encoder5 = base_model.get_layer('block5_conv4').output # (14,14,512)\n",
        "\n",
        "    # Center block\n",
        "    center = decoder_block(\n",
        "        encoder5, 'center', num_filters=512)\n",
        "    concat5 = concatenate([center, encoder5], axis=-1) # (14,14,1024)\n",
        "\n",
        "    # Decoder part.\n",
        "    decoder4 = decoder_block(\n",
        "        concat5, 'decoder4', num_filters=256) \n",
        "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1) \n",
        "    \n",
        "    decoder3 = decoder_block(\n",
        "        concat4, 'decoder3', num_filters=128) \n",
        "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1) \n",
        "\n",
        "    decoder2 = decoder_block(\n",
        "        concat3, 'decoder2', num_filters=64)\n",
        "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
        "\n",
        "    decoder1 = decoder_block(\n",
        "        concat2, 'decoder1', num_filters=64)\n",
        "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1)\n",
        "\n",
        "    # Final upsampling and decoder block for segmentation.\n",
        "    #output = UpSampling2D()(concat1)\n",
        "    #output = Conv2D(2, 32, activation = None, padding = 'same')(concat1)\n",
        "    output = decoder_block(\n",
        "        concat1, 'decoder_output', num_filters=32)\n",
        "    output = Conv2D(\n",
        "        1, (1, 1), activation=None, name='prediction')(output)\n",
        "    if not use_lovash:\n",
        "        output = Activation('sigmoid')(output)\n",
        "        \n",
        "    model = Model(base_model.input, output)\n",
        "    model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkk9Xr9kYA14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6eb8a9a-9f65-4820-9689-926106bc7dfc"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Build model:\n",
        "# Here, you can experiment with various losses.\n",
        "# For dice and BCE (binary_crossentropy), my_iou_metric should be used,\n",
        "# whereas for lovash_loss my_iou_metric2 should be used, because range of values\n",
        "# for lovash loss is between -inf and +inf, not between 0 and 1, as for BCE and dice.\n",
        "# What is more, when lovash loss is used, last layer (sigmoid) should be deleted.\n",
        "# This is controlled by use_lovash parameter.\n",
        "\n",
        "model_depth = unet_resnet(input_size, \n",
        "                          decoder_block_simple, # bottle\n",
        "                          weights='imagenet',\n",
        "                          #loss_func=bce_dice_loss, \n",
        "                          #metrics_list=[my_iou_metric],\n",
        "                          use_lovash=False)\n",
        "\n",
        "#print(model_depth.summary())\n",
        "\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('unet_resnet.h5',\n",
        "                                   monitor='val_my_iou_metric', \n",
        "                                   mode='max',\n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=True,\n",
        "                                   verbose=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric',\n",
        "                              mode='max',\n",
        "                              factor=0.5, \n",
        "                              patience=5, \n",
        "                              min_lr=0.0001, \n",
        "                              verbose=1)\n",
        "\n",
        "epochs = 1  # 25\n",
        "batch_size = 16\n",
        "\n",
        "history = model_depth.fit(X_tr,\n",
        "                          y_tr,\n",
        "                          validation_data=(X_val, y_val), \n",
        "                          epochs=epochs,\n",
        "                          batch_size=batch_size,\n",
        "                          verbose=1\n",
        "                         )"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/kernel:0' shape=(3, 3, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'center_bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/kernel:0' shape=(3, 3, 2560, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'decoder4_bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/kernel:0' shape=(3, 3, 1280, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'decoder3_bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/kernel:0' shape=(3, 3, 640, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder2_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/kernel:0' shape=(3, 3, 320, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder1_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/kernel:0' shape=(3, 3, 128, 32) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/bias:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_bn/gamma:0' shape=(32,) dtype=float32>\n",
            "  <tf.Variable 'decoder_output_bn/beta:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/kernel:0' shape=(1, 1, 32, 1) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/bias:0' shape=(1,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "200/200 [==============================] - 2445s 12s/step - loss: 0.5647 - my_iou_metric: 0.1667 - val_loss: 0.5051 - val_my_iou_metric: 0.2138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKa0H3Vil6T6"
      },
      "source": [
        "[Problem 3] Learning / estimation</br>\n",
        "Learn and estimate with both ResNet and VGG code and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVro4F-GYOqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c8c560b-773a-476e-ab91-605a2a16d2f5"
      },
      "source": [
        "# VGG19\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model_depth = unet_VGG(\n",
        "    input_size, decoder_block_bottleneck, weights='imagenet',\n",
        "    loss_func=bce_dice_loss, metrics_list=[my_iou_metric],\n",
        "    use_lovash=False)\n",
        "print(model_depth.summary())\n",
        "\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'unet_VGG.h5' ,monitor='val_my_iou_metric', mode='max',\n",
        "    save_best_only=True, save_weights_only=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_my_iou_metric',\n",
        "    mode='max',\n",
        "    factor=0.5, \n",
        "    patience=5, \n",
        "    min_lr=0.0001, \n",
        "    verbose=1)\n",
        "\n",
        "epochs = 1 \n",
        "batch_size = 16\n",
        "\n",
        "history = model_depth.fit(X_tr, y_tr,\n",
        "                    validation_data=(X_val, y_val), \n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=[model_checkpoint,reduce_lr], \n",
        "                    verbose=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "center_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "center_bn1 (BatchNormalization) (None, 14, 14, 512)  2048        center_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "center_activation1 (PReLU)      (None, 14, 14, 512)  100352      center_bn1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 14, 14, 512)  0           center_activation1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "center_conv2 (Conv2D)           (None, 14, 14, 256)  1179904     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "center_bn2 (BatchNormalization) (None, 14, 14, 256)  1024        center_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "center_activation2 (PReLU)      (None, 14, 14, 256)  50176       center_bn2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 14, 14, 256)  0           center_activation2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "center_conv3 (Conv2D)           (None, 14, 14, 512)  1180160     dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "center_bn3 (BatchNormalization) (None, 14, 14, 512)  2048        center_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "center_activation3 (PReLU)      (None, 14, 14, 512)  100352      center_bn3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 14, 14, 512)  0           center_activation3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 14, 14, 512)  0           dropout[0][0]                    \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 14, 14, 1024) 0           add[0][0]                        \n",
            "                                                                 block5_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder4_conv1 (Conv2D)         (None, 14, 14, 256)  2359552     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "decoder4_bn1 (BatchNormalizatio (None, 14, 14, 256)  1024        decoder4_conv1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder4_activation1 (PReLU)    (None, 14, 14, 256)  50176       decoder4_bn1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 14, 14, 256)  0           decoder4_activation1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder4_conv2 (Conv2D)         (None, 14, 14, 128)  295040      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder4_bn2 (BatchNormalizatio (None, 14, 14, 128)  512         decoder4_conv2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder4_activation2 (PReLU)    (None, 14, 14, 128)  25088       decoder4_bn2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 14, 14, 128)  0           decoder4_activation2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder4_conv3 (Conv2D)         (None, 14, 14, 256)  295168      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder4_bn3 (BatchNormalizatio (None, 14, 14, 256)  1024        decoder4_conv3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder4_activation3 (PReLU)    (None, 14, 14, 256)  50176       decoder4_bn3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 14, 14, 256)  0           decoder4_activation3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 14, 14, 256)  0           dropout_3[0][0]                  \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 28, 28, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 28, 28, 768)  0           up_sampling2d[0][0]              \n",
            "                                                                 block4_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder3_conv1 (Conv2D)         (None, 28, 28, 128)  884864      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder3_bn1 (BatchNormalizatio (None, 28, 28, 128)  512         decoder3_conv1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder3_activation1 (PReLU)    (None, 28, 28, 128)  100352      decoder3_bn1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 28, 28, 128)  0           decoder3_activation1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder3_conv2 (Conv2D)         (None, 28, 28, 64)   73792       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder3_bn2 (BatchNormalizatio (None, 28, 28, 64)   256         decoder3_conv2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder3_activation2 (PReLU)    (None, 28, 28, 64)   50176       decoder3_bn2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 28, 28, 64)   0           decoder3_activation2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder3_conv3 (Conv2D)         (None, 28, 28, 128)  73856       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder3_bn3 (BatchNormalizatio (None, 28, 28, 128)  512         decoder3_conv3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder3_activation3 (PReLU)    (None, 28, 28, 128)  100352      decoder3_bn3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 28, 28, 128)  0           decoder3_activation3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 28, 28, 128)  0           dropout_6[0][0]                  \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 56, 56, 128)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 56, 56, 384)  0           up_sampling2d_1[0][0]            \n",
            "                                                                 block3_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_conv1 (Conv2D)         (None, 56, 56, 64)   221248      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_bn1 (BatchNormalizatio (None, 56, 56, 64)   256         decoder2_conv1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_activation1 (PReLU)    (None, 56, 56, 64)   200704      decoder2_bn1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 56, 56, 64)   0           decoder2_activation1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_conv2 (Conv2D)         (None, 56, 56, 32)   18464       dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_bn2 (BatchNormalizatio (None, 56, 56, 32)   128         decoder2_conv2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_activation2 (PReLU)    (None, 56, 56, 32)   100352      decoder2_bn2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 56, 56, 32)   0           decoder2_activation2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_conv3 (Conv2D)         (None, 56, 56, 64)   18496       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_bn3 (BatchNormalizatio (None, 56, 56, 64)   256         decoder2_conv3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder2_activation3 (PReLU)    (None, 56, 56, 64)   200704      decoder2_bn3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 56, 56, 64)   0           decoder2_activation3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 56, 56, 64)   0           dropout_9[0][0]                  \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 112, 112, 64) 0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 112, 112, 192 0           up_sampling2d_2[0][0]            \n",
            "                                                                 block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_conv1 (Conv2D)         (None, 112, 112, 64) 110656      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_bn1 (BatchNormalizatio (None, 112, 112, 64) 256         decoder1_conv1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_activation1 (PReLU)    (None, 112, 112, 64) 802816      decoder1_bn1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 112, 112, 64) 0           decoder1_activation1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_conv2 (Conv2D)         (None, 112, 112, 32) 18464       dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_bn2 (BatchNormalizatio (None, 112, 112, 32) 128         decoder1_conv2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_activation2 (PReLU)    (None, 112, 112, 32) 401408      decoder1_bn2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 112, 112, 32) 0           decoder1_activation2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_conv3 (Conv2D)         (None, 112, 112, 64) 18496       dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_bn3 (BatchNormalizatio (None, 112, 112, 64) 256         decoder1_conv3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder1_activation3 (PReLU)    (None, 112, 112, 64) 802816      decoder1_bn3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 112, 112, 64) 0           decoder1_activation3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 112, 112, 64) 0           dropout_12[0][0]                 \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 224, 224, 64) 0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 224, 224, 128 0           up_sampling2d_3[0][0]            \n",
            "                                                                 block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_conv1 (Conv2D)   (None, 224, 224, 32) 36896       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_bn1 (BatchNormal (None, 224, 224, 32) 128         decoder_output_conv1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_activation1 (PRe (None, 224, 224, 32) 1605632     decoder_output_bn1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 224, 224, 32) 0           decoder_output_activation1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_conv2 (Conv2D)   (None, 224, 224, 16) 4624        dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_bn2 (BatchNormal (None, 224, 224, 16) 64          decoder_output_conv2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_activation2 (PRe (None, 224, 224, 16) 802816      decoder_output_bn2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 224, 224, 16) 0           decoder_output_activation2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_conv3 (Conv2D)   (None, 224, 224, 32) 4640        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_bn3 (BatchNormal (None, 224, 224, 32) 128         decoder_output_conv3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_activation3 (PRe (None, 224, 224, 32) 1605632     decoder_output_bn3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 224, 224, 32) 0           decoder_output_activation3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 224, 224, 32) 0           dropout_15[0][0]                 \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "prediction (Conv2D)             (None, 224, 224, 1)  33          add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 224, 224, 1)  0           prediction[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 36,339,185\n",
            "Trainable params: 36,333,905\n",
            "Non-trainable params: 5,280\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "200/200 [==============================] - 8792s 44s/step - loss: 1.0017 - my_iou_metric: 0.1213 - val_loss: 3.0416 - val_my_iou_metric: 0.1365\n",
            "\n",
            "Epoch 00001: val_my_iou_metric improved from -inf to 0.13650, saving model to unet_VGG.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVGc_UYPmQqj"
      },
      "source": [
        "val_preds = model_depth.predict(X_val, batch_size=16)\n",
        "\n",
        "y_val_pred = np.asarray(list(map(lambda x: cv2.resize(x, (101, 101)), val_preds)))\n",
        "y_val_true = np.asarray(list(map(lambda x: cv2.resize(x, (101, 101)), y_val)))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP6lDj4Epqd3",
        "outputId": "2c136f17-ccf4-415f-f008-7905e243b672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Threshold range, over which optimization is performed\n",
        "thresholds = np.arange(0.2, 0.9, 0.02)\n",
        "\n",
        "# For every threshold, set predictions to binary arrays, \n",
        "# where values above threshold are treated as 1 and the rest as 0.\n",
        "# Loop over thresholds and compute IoU for them based on IoU function above.\n",
        "ious = np.array(\n",
        "    [iou_metric_batch(y_val_true,\n",
        "                      np.int32(y_val_pred > threshold)) for threshold in tqdm(thresholds)])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35/35 [00:39<00:00,  1.12s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjYlNhVLpsgl",
        "outputId": "f04ead32-41c6-4298-9ef9-f62891af22f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "df_iou = pd.DataFrame(thresholds, columns=['threshold'])\n",
        "df_iou['iou'] = ious\n",
        "\n",
        "# Get index of best IoU\n",
        "best_index = df_iou['iou'].idxmax()\n",
        "print('Best IoU: {:.4f} at threshold: {:.3f}'.format(\n",
        "    df_iou.iou[best_index], df_iou.threshold[best_index]))\n",
        "\n",
        "# Describe IoU DF\n",
        "df_iou.describe()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best IoU: 0.5045 at threshold: 0.200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>threshold</th>\n",
              "      <th>iou</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.438154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.204939</td>\n",
              "      <td>0.042254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.339750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.410750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.443500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.710000</td>\n",
              "      <td>0.466500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.504500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       threshold        iou\n",
              "count  35.000000  35.000000\n",
              "mean    0.540000   0.438154\n",
              "std     0.204939   0.042254\n",
              "min     0.200000   0.339750\n",
              "25%     0.370000   0.410750\n",
              "50%     0.540000   0.443500\n",
              "75%     0.710000   0.466500\n",
              "max     0.880000   0.504500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jov5zUXcrMQQ",
        "outputId": "4b951eeb-a671-40e4-ec40-c981acf81e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        }
      },
      "source": [
        "# Plot IoU values over threshold range.\n",
        "df_iou.plot(x='threshold', y='iou')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdb9cc9ab10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAIWCAYAAACoQ2BQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3RVVeL28Wff9EAghDTSCB2pCYRe1VFRkaKAgCKWUUGwT3H8TXccHR17wzKKg6IoiqCAWOk1gVBDb0kIEAg1IX2/fxB8MzMIAZKce5PvZ6274J57zrnP/cf1uNlnb2OtFQAAAIDzczkdAAAAAPAUlGcAAACggijPAAAAQAVRngEAAIAKojwDAAAAFUR5BgAAACrI2+kAFyI0NNTGx8c7HQMAAAA1WEpKyiFrbdjZPvOo8hwfH6/k5GSnYwAAAKAGM8bs+bnPmLYBAAAAVBDlGQAAAKggyjMAAABQQR415xkAAADOKyoqUkZGhvLz852Ockn8/f0VExMjHx+fCl9DeQYAAMAFycjIUFBQkOLj42WMcTrORbHW6vDhw8rIyFCTJk0qfB3TNgAAAHBB8vPz1bBhQ48tzpJkjFHDhg0vePSc8gwAAIAL5snF+YyL+Q2UZwAAAHicnj17OvK9lGcAAAB4nKVLlzryvZRnAAAAeJy6detKOv3g369//Wu1a9dO7du317Rp0yRJ8+fP18CBA386f+LEiZo8efIlfy+rbQAAAOCi/eXLjdq073il3rNNVD396Ya2FTr3888/V2pqqtauXatDhw6pS5cu6tu3b6XmKY+RZwAAAHisxYsXa9SoUfLy8lJERIT69eunVatWVdn3MfIMAACAi1bREeLq5u3trdLS0p/eV9aGLow8AwAAwGP16dNH06ZNU0lJibKzs7Vw4UJ17dpVjRs31qZNm1RQUKCjR4/q+++/r5TvY+QZAAAAHmvo0KFatmyZOnbsKGOMnnnmGUVGRkqSRowYoXbt2qlJkyZKTEyslO8z1tpKuVF1SEpKssnJyU7HAAAAqNXS0tJ02WWXOR2jUpzttxhjUqy1SWc7n2kbAAAAQAVRngEAAIAKojyfR35RifKLSpyOAQAAADdAeT6Pp+du1g2vLNa6jKNORwEAAHAbnvTc3M+5mN9AeT6Py1uH60R+sYa+vlTPf7tVhcWl578IAACgBvP399fhw4c9ukBba3X48GH5+/tf0HWstlEBx04V6S9fbtTnqzPVNqqenhvRUa0j61V7DgAAAHdQVFSkjIyMStt4xCn+/v6KiYmRj4/Pfxw/12oblOcL8M3G/Xp8xnodO1Wkh69qqXv6NJW3F4P3AAAANQlL1VWSq9tG6puH++mqNhF65ustGv7mMu3MPul0LAAAAFQTyvMFCqnjq9dGd9LLoxK1MztX1728SO8u3qXSUs8ZwQcAAMDFoTxfBGOMBnWM0rcP91XPZqH661ebNPqd5UrPyXM6GgAAAKoQ5fkShNfz17/GJumZYR20IfO4Bry4UFNX7PXoJ08BAADw8yjPl8gYoxFJsZr3cF8lxAXr8RnrNfa9Vdp/zLOfPgUAAMD/ojxXkujgAE25s5ueGNxWq3bl6OoXFmjGmgxGoQEAAGqQCpVnY8wAY8wWY8x2Y8xjZ/n8dmNMtjEmtez1y3KfjTXGbCt7jS13vLMxZn3ZPV82xpjK+UnOcbmMxvSI19wH+6hlRJAenrZW905JUfaJAqejAQAAoBKctzwbY7wkvSbpWkltJI0yxrQ5y6nTrLUJZa93yq4NkfQnSd0kdZX0J2NMg7Lz35B0t6QWZa8Bl/pj3EV8aB1Nu7eHHr+uteZvzdY1Ly7UnPVZTscCAADAJarIyHNXSduttTuttYWSPpY0uIL3v0bSt9baHGvtEUnfShpgjGkkqZ61drk9Pa/h35KGXER+t+XlMrqnbzPNvr+3YhoE6L4PV+vOyav07aYDKiphi28AAABP5F2Bc6IlpZd7n6HTI8n/7SZjTF9JWyU9bK1N/5lro8teGWc5/j+MMfdIukeS4uLiKhDXvbSICNJn43vqrYU79d6S3fphc7JC6/pqSEK0hiXFsM03AACAB6msBwa/lBRvre2g06PL71fSfWWtfctam2StTQoLC6us21YrHy+XJlzeXMt+d4X+NTZJSY1D9P6y3Rrw4iLd8Mpivb90t47mFTodEwAAAOdRkZHnTEmx5d7HlB37ibX2cLm370h6pty1/f/r2vllx2POdc+ayMfLpSsvi9CVl0UoJ7dQM1MzNT0lQ3+atVFPzk7TL9qEa3jnWPVpESpvLxZCAQAAcDcVKc+rJLUwxjTR6YI7UtLo8icYYxpZa888ETdIUlrZ3+dJ+nu5hwSvlvQ7a22OMea4Maa7pBWSbpP0yqX9FM8SUsdXd/Rqojt6NdGmfcc1PSVDX6Rmas76/QoP8tPQTtEa3jlGzcODnI4KAACAMqYi6xAbY66T9KIkL0nvWmufNMb8VVKytXaWMeYpnS7NxZJyJI231m4uu/ZOSY+X3epJa+17ZceTJE2WFCBprqT77XnCJCUl2eTk5Av/lR6isLhUP245qE+TM/TjloMqKbVKiA3WsM4xuqFjlOoH+DgdEQAAoMYzxqRYa5PO+pknbeJR08tzedknCjQzNVOfJmdoy4ET8vV26Zq2kRrWOUa9m4fKy+Xxy2IDAAC4JcqzB7PWakPmcU1PSdfMtft0NK9IEfX8NKhjlIYkRqtNo3qqAfvLAAAAuA3Kcw1RUFyi7zYd1Iw1mVqw9aCKSqxaRtTV4IRoDU6IUkyDQKcjAgAAeDzKcw10JLdQs9dn6Ys1mUrec0SS1LVJiIYmRuu6do1UP5D50QAAABeD8lzDpefkaWZqpmasydSO7Fz5erl0eeswDU2MVv9W4fL38XI6IgAAgMegPNcSZ+ZHf5GaqZmp+3ToZIGC/L11fftGGpIYra7xIXLxoCEAAMA5UZ5roeKSUi3dcVhfrMnU1xv3K6+wRFH1/TUoIVpDE6PVKpL1owEAAM6G8lzL5RUW69tNBzQzdZ8WbM1WSalV68gg3dW7iW7qFMNoNAAAQDmUZ/zk0MkCzV6XpU+S07Vx33F1btxAfx3cVm2j6jsdDQAAwC1QnvE/Skutpq/O0NNzN+toXqFu6xGvR65uqXr+rNIBAABqt3OVZ1d1h4F7cLmMRiTF6sdH+2t0tzi9v2y3rvjnAn2+OkOe9D9UAAAA1YnyXMvVD/TR34a016wJvRXdIECPfLJWN7+5XJv3H3c6GgAAgNuhPEOS1D6mvmaM76mnbmyvrQdP6PqXF+uJrzbpRH6R09EAAADcBuUZP3G5jEZ1jdOPj/bXiKRYvbtkl658boFmpmYylQMAAECUZ5xFgzq+eurG9ppxXy9F1PPXgx+navTbK7TtwAmnowEAADiK8oyflRAbrC8m9NLfhrTTpqzjuvalRXpqTppyC4qdjgYAAOAIyjPOyctldGv3xvrh0X66sVO03ly4U1c+t0Cz12UxlQMAANQ6lGdUSMO6fnpmWEd9Nr6HGtTx1YSpq3Xbuyu1I/uk09EAAACqDeUZF6Rz4xB9ObGX/nxDG6XuPaoBLy7Uaz9uV0kpo9AAAKDmozzjgnl7uXR7ryb64Vf9dXXbSD07b4tue3eFDp7IdzoaAABAlaI846KFBfnp1VGJ+sdN7ZWy54iue2mRFmzNdjoWAABAlaE845IYY3Rzlzh9ObG3Gtbx09h3V+qpuWkqKil1OhoAAEClozyjUrSICNLMib00uluc3lywU8MnLVN6Tp7TsQAAACoV5RmVxt/HS38f2l6vje6kHdkndd1LizR7XZbTsQAAACoN5RmV7voOjTTngT5qFl5XE6au1uMz1iu/qMTpWAAAAJeM8owqERsSqE/H9dC4fs00dcVeDX51ibayvTcAAPBwlGdUGR8vlx67trXev7OrDucWaNCri/Xxyr3sTAgAADwW5RlVrl/LMM15sI+SGofosc/X6/6P1uh4fpHTsQAAAC4Y5RnVIjzIX/++s6t+fU0rzd2wXwNfXqy16UedjgUAAHBBKM+oNi6X0YTLm+uTe7urpNTqpjeW6u2FO1XK1t4AAMBDUJ5R7To3DtGcB/roysvC9eScNN35/iodPlngdCwAAIDzojzDEfUDfTTp1s56YnBbLd1xWNe+tEipTOMAAABujvIMxxhjNKZHvL64r5f8fFwa884KCjQAAHBrlGc4rk1UPU27p4ca1PGlQAMAALdGeYZbiAoO0Mf3dP+pQK/Ze8TpSAAAAP+D8gy3Ub5A3/avlRRoAADgdijPcCtnCnRIXQo0AABwP5RnuJ2o4AB9dPf/L9CrKdAAAMBNUJ7hlsoX6LEUaAAA4CYoz3Bb5adwUKABAIA7oDzDrTWq/59zoCnQAADASZRnuL0zBbohBRoAADiM8gyPQIEGAADugPIMj3GmQIeWFeiUPRRoAABQvSjP8CiN6gfoo7ICPfZdCjQAAKhelGd4HAo0AABwCuUZHun0FI4e5Qp0jtORAABALUB5hseKrO9frkCvokADAIAqR3mGR6NAAwCA6kR5hscrX6Bv+9dKrU0/6nQkAABQQ1GeUSOcKdAN6vjqrvdXKT0nz+lIAACgBqI8o8aIrO+vyXd0UWFxqe6YvErH8oqcjgQAAGoYyjNqlObhQXrrtiTtOZyre6Ykq6C4xOlIAACgBqE8o8bp3rShnh3WUSt25ei309fJWut0JAAAUEN4Ox0AqApDEqOVcSRP//xmq2JDAvXo1a2cjgQAAGoAyjNqrAmXN1d6zim98sN2xTYI1IgusU5HAgAAHo7yjBrLGKO/DW2nfcdO6fEZ69Uo2F99WoQ5HQsAAHgw5jyjRvPxcun1WzqpeXhdjf9gtdKyjjsdCQAAeDDKM2q8IH8fvXdHF9Xx89Kdk1dp/7F8pyMBAAAPVaHybIwZYIzZYozZbox57Bzn3WSMscaYpLL3txhjUsu9So0xCWWfzS+755nPwivnJwH/q1H9AL17excdP1WkOyav0smCYqcjAQAAD3Te8myM8ZL0mqRrJbWRNMoY0+Ys5wVJelDSijPHrLUfWmsTrLUJksZI2mWtTS132S1nPrfWHrzE3wKcU9uo+nr91s7aeuCEJny4WkUlpU5HAgAAHqYiI89dJW231u601hZK+ljS4LOc94Skf0j6uX8TH1V2LeCYfi3D9Lch7bRga7b+OHMDa0ADAIALUpHyHC0pvdz7jLJjPzHGdJIUa62dfY773Czpo/869l7ZlI0/GGPM2S4yxtxjjEk2xiRnZ2dXIC5wbqO6xum+/s300cp0vbFgh9NxAACAB7nkBwaNMS5Jz0t69BzndJOUZ63dUO7wLdba9pL6lL3GnO1aa+1b1toka21SWBjLjKFy/OrqVhrUMUrPfL1Fs9buczoOAADwEBUpz5mSyu8uEVN27IwgSe0kzTfG7JbUXdKsMw8Nlhmp/xp1ttZmlv15QtJUnZ4eAlQLl8vo2eEd1DU+RL/6ZK1W7spxOhIAAPAAFSnPqyS1MMY0Mcb46nQRnnXmQ2vtMWttqLU23lobL2m5pEHW2mTpp5HpESo339kY422MCS37u4+kgZLKj0oDVc7P20tv3dZZMSEBuvvfydqRfdLpSAAAwM2dtzxba4slTZQ0T1KapE+stRuNMX81xgyqwHf0lZRurd1Z7pifpHnGmHWSUnV6JPvtC04PXKLgQF9Nvr2rvF1Gd7y3SodOFjgdCQAAuDHjSasNJCUl2eTkZKdjoAZKTT+qkW8tU+vIevro7u4K8PVyOhIAAHCIMSbFWpt0ts/YYRCQlBAbrJdGJmptxlE9NG2NSko9538qAQBA9aE8A2WuaRupP1zfRvM2HtCTs9NYAxoAAPwPb6cDAO7kzt5NlH4kT+8u2aW9OXl6YkhbNaof4HQsAADgJhh5Bv7LH65vo8eva63F27N11fMLNWX5HpUyjQMAAIjyDPwPl8vonr7N9M1D/ZQQG6w/fLFBI95cpu0HWcoOAIDajvIM/Iy4hoGacldXPTusg7YdPKnrXlqkl7/fpsLiUqejAQAAh1CegXMwxmh4Uqy+e6Sfrm4boee/3aqBryzS6r1HnI4GAAAcQHkGKiAsyE+vju6kd25L0on8Yt30xlL95cuNyi0odjoaAACoRpRn4AL8ok2Evnm4r8Z0b6zJS3fr6hcW6sctB52OBQAAqgnlGbhAQf4++uvgdvr03h4K8PXSHe+t0kMfr9FhtvYGAKDGozwDFykpPkSzH+itB69sodnrs/SL5xdoxpoMNlcBAKAGozwDl8DP20sPX9VSsx/oo/jQOnp42lqNfW+V0nPynI4GAACqAOUZqAQtI4I0fVxP/WVQW6XsztHVLyzUvxbvUgmbqwAAUKNQnoFK4uUyGtszXt880k/dm4boia82achrS/RZSobyClmVAwCAmsB40vzMpKQkm5yc7HQM4LystZq1dp+e/3ar9hzOUx1fL13foZGGJ8UqqXEDGWOcjggAAH6GMSbFWpt01s8oz0DVsdZq1e4jmp6SrtnrspRbWKL4hoEa1jlGN3aKUVRwgNMRAQDAf6E8A24gt6BYX2/Yr09T0rV8Z46MkXo3D9WwzjG6pm2k/H28nI4IAABEeQbcTnpOnqanZGh6SoYyj55SkJ+3BnaM0rDOMeoUF8y0DgAAHER5BtxUaanV8l2HNT05Q3M2ZCm/qFRNw+qcntaRGKPI+v5ORwQAoNahPAMe4ER+keasz9L0lAyt2n1ELiP1aRGm4Ukx+sVlEUzrAACgmlCeAQ+z61CuPkvJ0GerM5R1LF9B/t66pm2kBnZopF7NQ+XjxSqTAABUFcoz4KFKSq2W7jikGWsy9e3GAzpRUKwGgT4a0C5S17ePUvemIfKmSAMAUKnOVZ69qzsMgIrzchn1aRGmPi3ClF9UooVbs/XVuizNTN2nj1amK7Surwa0i9TADlHqEh8iLxcPGgIAUJUYeQY80KnCEs3fclBfrcvS95sPKL+oVOFBfrqufSPd0LGREmMbyEWRBgDgojBtA6jBcguK9f3mg/pq7T7N35qtwuJSRdX313XtG2lgxyh1jKnP0ncAAFwAyjNQS5zIL9J3aQf01dosLdyWraISq9iQAF3fPkoDOzRS26h6FGkAAM6D8gzUQsfyijRv037NXpelJdsPqbjUqmloHQ1OiNaQxCg1bljH6YgAALglyjNQyx3JLdTXG/drVuo+Ld91WNZKneKCNSQxWte3b6SGdf2cjggAgNugPAP4yb6jpzRr7T59sSZTm/efkLfLqF/LMA1OjNZVl0UowJfNWAAAtRvlGcBZpWUd1xepmZq5Zp/2H89XHV8vDWjXSEMTo9WjWUOWvgMA1EqUZwDnVFpqtWJXjr5Yk6k567N0oqBY4UF+GtQxSkMSo3nQEABQq1CeAVRYflGJftx8UDPWZOrHLQdVVGLVPLyuhiZGa1DHKMWGBDodEQCAKkV5BnBRjuYVavb6LM1cs08rd+dIkrrEN9DwzrEanBglP2/mRwMAah7KM4BLlp6Tp1lr92nGmkxtP3hSkfX89cs+TTSqa5zq+Hk7HQ8AgEpDeQZQaay1WrTtkN6Yv0PLdh5W/QAfje0Zrzt6xqtBHV+n4wEAcMkozwCqxOq9R/TG/B36dtMBBfh4aVTXOP2yTxNFBQc4HQ0AgItGeQZQpbYdOKE3FuzQzNR9chlpSEK07u3XTM3D6zodDQCAC0Z5BlAtMo7k6Z1Fu/Txqr0qKC7VNW0idd/lzdQhJtjpaAAAVBjlGUC1OnSyQJOX7Nb7y3brRH6xejVvqPv6N1fPZg1ZLxoA4PYozwAccSK/SFNX7NU7i3cp+0SBOsbU1/j+zXV1mwi52L0QAOCmKM8AHJVfVKLPV2fqzYU7tOdwnpqF1dG4fs00OCFavt4up+MBAPAfKM8A3EJxSanmbNivN+bvUFrWcYUH+WlU1ziN6hqnyPr+TscDAEAS5RmAm7HWasHWbL2/dLfmb82Wyxhd3SZCY7o3Vg/mRQMAHHau8sy2YACqnTFG/VuFq3+rcO09nKcPV+7RJ6vSNXfDfjULq6NbuzfWjZ1iVD/Ax+moAAD8B0aeAbiF/KISzV6XpSnL9yg1/agCfLw0JDFat3aPU9uo+k7HAwDUIkzbAOBR1mcc0wfL92jm2kzlF5Wqc+MGGtO9sa5tHyk/by+n4wEAajjKMwCPdCyvSNNXZ+iD5Xu061CuGtbx1YgusRrdNU6xIYFOxwMA1FCUZwAerbTUaumOw5qyfLe+3XRAVtIVrcI1pkdj9W0RxprRAIBKxQODADyay2XUu0WoercI1b6jp/TRyr36aGW6vn9vleJCAjW2Z7xGdY1VoC//SQMAVC1GngF4pMLiUs3buF//XrZbq3YfUXCgj27vGa/be8YrONDX6XgAAA/GtA0ANVrKniN6Y/52fZd2UIG+XhrdNU6/7NOUjVcAABeF8gygVtiy/4QmLdihWWv3yWWkGxNjdG+/pmoaVtfpaAAAD0J5BlCrpOfk6e1FOzVtVboKS0p1bbtIje/XXO1jWC8aAHB+lGcAtVL2iQJNXrpL/162Ryfyi9WnRajG92+mHk3ZAhwA8PMozwBqteP5Rfpw+V79a/EuHTpZoITYYI3v30xXXRbBMncAgP9BeQYAnd4CfHpKht5cuEPpOafUIryuxvVrpkEJUfLxcjkdDwDgJijPAFBOcUmpZq/P0hvzd2jz/hOKDg7Q3X2a6OYucQrwZftvAKjtKM8AcBbWWs3fkq3X52/Xqt1HFB7kpweubKGbu8QyEg0AtRjlGQDOY8XOw3p23hYl7zmixg0D9chVLXVDhyjmRANALXSu8lyhoRVjzABjzBZjzHZjzGPnOO8mY4w1xiSVvY83xpwyxqSWvSaVO7ezMWZ92T1fNjz6DsBB3Zo21Kfjeujd25MU4OOlBz9O1XUvL9L3aQfkSYMMAICqdd7ybIzxkvSapGsltZE0yhjT5iznBUl6UNKK//poh7U2oew1rtzxNyTdLalF2WvAxf0EAKgcxhhd0TpCcx7oo5dGJuhUUYnuej9Zwyct04qdh52OBwBwAxUZee4qabu1dqe1tlDSx5IGn+W8JyT9Q1L++W5ojGkkqZ61drk9PaTzb0lDKh4bAKqOy2U0OCFa3z3ST08Obaf0I3m6+a3lGvvuSm3IPOZ0PACAgypSnqMlpZd7n1F27CfGmE6SYq21s89yfRNjzBpjzAJjTJ9y98w41z3L3fseY0yyMSY5Ozu7AnEBoHL4eLl0S7fGWvDry/W7a1srNf2oBr6yWBOnrtauQ7lOxwMAOOCSHyc3xrgkPS/p0bN8nCUpzlqbKOkRSVONMfUu5P7W2restUnW2qSwsLBLjQsAF8zfx0v39mumRb+9XPdf0Vw/bD6oXzy/QL/7fJ2yjp1yOh4AoBpVpDxnSoot9z6m7NgZQZLaSZpvjNktqbukWcaYJGttgbX2sCRZa1Mk7ZDUsuz6mHPcEwDcTj1/Hz16dSst+PXlGtO9saanZKjfs/P15OxNysktdDoeAKAaVKQ8r5LUwhjTxBjjK2mkpFlnPrTWHrPWhlpr46218ZKWSxpkrU02xoSVPXAoY0xTnX4wcKe1NkvScWNM97JVNm6TNLNyfxoAVI2wID/9eVBb/fBof93QIUr/WrxLfZ/5US99t00nC4qdjgcAqELnLc/W2mJJEyXNk5Qm6RNr7UZjzF+NMYPOc3lfSeuMMamSpksaZ63NKfvsPknvSNqu0yPScy/yNwCAI2JDAvXciI6a91Bf9WreUC98t1X9ykr0wePnfXYaAOCB2CQFACpJavpRvfDtVi3Ymi1vl9E17SI1pntjdWsSIpayBwDPwQ6DAFCNdh3K1YfL9+iT5HQdzy9Wi/C6GtOjsYYmRivI38fpeACA86A8A4ADThWW6Mt1+zRl2R6tzzymOr5eGtopWrd2b6zWkRe08BAAoBpRngHAYanpRzVl2R59uW6fCotL1TU+RLf2aKwBbSPl633Jq4YCACoR5RkA3MSR3EJ9mpKuD5bv1d6cPIXW9dXILnEa1S1O0cEBTscDAIjyDABup7TUauG2bH2wfI++33xQRtIvLovQmB6N1atZqFwuHjAEAKecqzx7V3cYAIDkchn1bxWu/q3ClZ6Tp6kr92raqnR9s+mAmoTW0S3d4nRd+0aKYjQaANwKI88A4CYKiks0d/1+TVm+Ryl7jkiSWoTXVf9WYerXMlxdmjSQn7eXwykBoOZj2gYAeJjtB0/ox83ZWrA1Wyt35aiwpFQBPl7q2ayh+rUKU/+W4YprGOh0TACokSjPAODBcguKtXznYS3Ymq35W7K1NydPktQktI76tQxTv1Zh6t6koQJ8GZUGgMpAeQaAGsJaq92H8zR/y0Et2JqtZTsOq6C4VH7eLnVr2lD9Woapf6swNQ2tw66GAHCRKM8AUEPlF5Voxa4cLdiSrQVbD2pHdq4kKaZBgPq1DNMVrcN1eatwVu8AgAtAeQaAWiI9J08Ltp6eK710+yHlFpaoe9MQPTuso2JDmCMNABVBeQaAWqiwuFSfr87Q32anyVqr/7u+jUZ1jWU6BwCcx7nKM3vCAkAN5evt0siucfr6oT7qGBusx2es19j3Vmn/sXynowGAx6I8A0ANF9MgUB/c1U1/HdxWq3bl6OoXFmjGmgx50r88AoC7oDwDQC3gchnd1iNecx/so5YRQXp42lqN+yBFh04WOB0NADwK5RkAapH40Dqadm8PPX5da/24JVtXv7BQc9ZnOR0LADwG5RkAahkvl9E9fZtp9v29FR0coPs+XK0HPlqjo3mFTkcDALdHeQaAWqpFRJA+v6+nHrmqpeasz9LVLyzUD5sPOB0LANwa5RkAajEfL5ceuLKFvpjQSw0CfXXn5GT9dvo6ncgvcjoaALglyjMAQO2i62vW/b10X/9m+jQlXQNeXKQl2w85HQsA3A7lGQAgSfLz9tJvBrTW9PE95eft0i3vrNCfZm5QXmGx09EAwG1QngEA/6FTXAPNfqCP7ugVr/eX7dF1Ly1S8u4cp2MBgFugPAMA/keAr5f+dENbfXR3dxWXWg2btEy3vLNcX2/IUnFJqdPxAPKIricAACAASURBVMAxxpN2mEpKSrLJyclOxwCAWuVkQbHeX7pbU1fsVebRU4qs56/R3eI0skuswuv5Ox0PACqdMSbFWpt01s8ozwCAiigptfph80FNWb5HC7dmy9tldE27SI3p3ljdmoTIGON0RACoFOcqz97VHQYA4Jm8XEZXtYnQVW0itPtQrj5csUefJGdo9rostQivqzE9GmtoYrSC/H2cjgoAVYaRZwDARcsvKtGstfs0Zdkerc88pjq+XhraKVq3dm+s1pH1nI4HABeFaRsAgCq3Nv2opizfo1lr96mwuFRd40N0a4/GGtA2Ur7ePJ8OwHNQngEA1eZIbqE+TUnXB8v3am9OnkLr+mpklziN6han6OAAp+MBwHlRngEA1a601Grhtmx9sHyPvt98UEbSNW0j9fuBbSjRANwa5RkA4Kj0nDxNXblX7y/dLSPpd9ddptFd4+RysUIHAPdzrvLMJDQAQJWLDQnUbwe01ryH+iohLli//2KDbnlnhfYeznM6GgBcEMozAKDaxIYE6oO7uumpG9trfeYxXfPiQk1eskulpZ7zr6AAajfKMwCgWhljNKprnL55uK+6NgnRn7/cpJFvLdeuQ7lORwOA86I8AwAcERUcoMl3dNGzwzpo8/7jGvDiQr29cKdKGIUG4MYozwAAxxhjNDwpVt8+0k99WoTqyTlpGjZpqbYfPOF0NAA4K8ozAMBxEfX89fZtSXppZIJ2HcrVdS8v1uvzt6u4pNTpaADwHyjPAAC3YIzR4IRofftwP13ZOlzPfL1FQ19fqs37jzsdDQB+QnkGALiVsCA/vXFrZ702upP2HT2lG15ZrJe+26YiRqEBuAHKMwDALV3foZG+faSfrm3XSC98t1WDXl2iDZnHnI4FoJajPAMA3FZIHV+9PCpRb43prEMnCzTktSV67pstKigucToagFqK8gwAcHtXt43Utw/31aCEKL3yw3bd8MpiRqEBOILyDADwCMGBvnp+RILevT1Jx04VachrS/Tid1uZCw2gWlGeAQAe5YrWEfrmoX66oWOUXvxum4a+vkRbD7AuNIDqQXkGAHic+oE+euHmBE26tZOyjuZr4MuLNWnBDnYnBFDlKM8AAI81oF0jzXu4r65oHa6n527W8ElLtetQrtOxANRglGcAgEcLreunN27tpBdvTtD2gyd17UsLNXnJLpUyCg2gClCeAQAezxijIYnR+vaRfuretKH+/OUm3fLOCmUcyXM6GoAahvIMAKgxIur5673bu+gfN7XXuoyjGvDiIk1btVfWMgoNoHJQngEANYoxRjd3idPXD/VV++j6+u1n63Xn5FU6cDzf6WgAagDKMwCgRooNCdSHv+ymvwxqq2U7D+vqFxZqZmomo9AALgnlGQBQY7lcRmN7xmvug33VLKyOHvw4Vfd9uFqHTxY4HQ2Ah6I8AwBqvCahdfTpuJ567NrW+j7toK5+YaG+3rDf6VgAPBDlGQBQK3i5jMb1a6Yv7++tRsH+GvdBih6elqpjp4qcjgbAg1CeAQC1SqvIIM24r5cevLKFZq3dp2tfXKhlOw47HQuAh6A8AwBqHR8vlx6+qqU+G99Tfj5eGv3Ocj05e5Pyi0qcjgbAzVGeAQC1VkJssGY/0Fu3dIvT24t2achrS5SWddzpWADcWIXKszFmgDFmizFmuzHmsXOcd5MxxhpjksreX2WMSTHGrC/784py584vu2dq2Sv80n8OAAAXJtDXW38b0l7v3dFFh3MLNfjVJXpzwQ6VsL03gLM4b3k2xnhJek3StZLaSBpljGlzlvOCJD0oaUW5w4ck3WCtbS9prKQp/3XZLdbahLLXwYv8DQAAXLLLW4Vr3kN9dXnrMD01d7NGv72c7b0B/I+KjDx3lbTdWrvTWlso6WNJg89y3hOS/iHppy2crLVrrLX7yt5ulBRgjPG7xMwAAFSJkDq+mnRrZz07rIM27juua19cpM9XZ7CxCoCfVKQ8R0tKL/c+o+zYT4wxnSTFWmtnn+M+N0laba0tvzL9e2VTNv5gjDFnu8gYc48xJtkYk5ydnV2BuAAAXDxjjIYnxWrug33UulGQHvlkrSZOXaMjuYVORwPgBi75gUFjjEvS85IePcc5bXV6VPrecodvKZvO0afsNeZs11pr37LWJllrk8LCwi41LgAAFRIbEqiP7+mh3wxopW827dc1Ly7Ugq0M4gC1XUXKc6ak2HLvY8qOnREkqZ2k+caY3ZK6S5pV7qHBGEkzJN1mrd1x5iJrbWbZnyckTdXp6SEAALgNL5fRff2ba8Z9vVQ/wEdj312pP83coFOFLGkH1FYVKc+rJLUwxjQxxvhKGilp1pkPrbXHrLWh1tp4a228pOWSBllrk40xwZJmS3rMWrvkzDXGGG9jTGjZ330kDZS0odJ+FQAAlahddH19eX9v3dmrid5ftkcDX1mk9RnHnI4FwAHnLc/W2mJJEyXNk5Qm6RNr7UZjzF+NMYPOc/lESc0l/fG/lqTzkzTPGLNOUqpOj2S/fSk/BACAquTv46U/3tBGH9zVTbkFJRr6+hK98v02FZeUOh0NQDUynvQEcVJSkk1OTnY6BgCgljuaV6jff7FBX63LUqe4YL1wc4IaN6zjdCwAlcQYk2KtTTrbZ+wwCADABQoO9NWrozvppZEJ2nbwpG54ZbEWbeNhQqA2oDwDAHCRBidEa84DfdSofoBuf2+VJi/ZxZrQQA1HeQYA4BLEhgTqs/t66vJW4frzl5v0+IwNKmIeNFBjUZ4BALhEdf289daYzhrfv5k+WrlXY/61gk1VgBqK8gwAQCVwuYx+O6C1Xri5o1bvParBry3R1gMnnI4FoJJRngEAqERDE2P08T3dlVdYohtfX6ofNh9wOhKASkR5BgCgknWKa6BZE3upccNA3fV+st5euJMHCYEagvIMAEAViAoO0KfjemhA20g9OSdNv56+TgXFbOsNeDrKMwAAVSTQ11uvje6kB69soekpGRr99godOlngdCwAl4DyDABAFXK5jB6+qqVeHZ2oDZnHNPjVJUrLOu50LAAXifIMAEA1GNghSp+O66Hi0lLd9MZSzdu43+lIAC4C5RkAgGrSISZYsyb2Vovwurp3Sope+3E7DxICHobyDABANYqo569p9/bQ4IQoPTtvix6alqr8Ih4kBDyFt9MBAACobfx9vPTizQlqGRGkZ+dt0e7DeXp7TGeF1/N3OhqA82DkGQAABxhjNOHy5pp0a2dtO3BCg15dojV7jzgdC8B5UJ4BAHDQgHaRmj6up7xcRje+sVT/N2O9juYVOh0LwM+gPAMA4LA2UfU058E+GtsjXh+t3KsrnlugT5LTVVrKw4SAu6E8AwDgBuoH+OjPg9rqq/v7qEloHf1m+joNm7RUG/cdczoagHIozwAAuJE2UfX06b099OywDtpzOE83vLJYf561UcdOFTkdDYAozwAAuB2Xy2h4Uqx+eLS/bunWWO8v260rn1ugz1dnsC404DDKMwAAbqp+oI+eGNJOsyb0VkyDAD3yyVqNeHOZNu9ne2/AKZRnAADcXPuY+vp8fE89fWN7bT94Ute/vFhPfLVJJ/KZygFUN8ozAAAewOUyGtk1Tj882l8jkmL17pJduuK5BZqZmslUDqAaUZ4BAPAgDer46qkb22vGfb0UWc9fD36cqlFvL9e2Ayecjlal0rKOq4Sl++AGKM8AAHighNhgfTGhl/42pJ3Ssk7o2pcW6ak5acotKHY6WqWbsSZD1760SA98vEaFxaVOx0EtR3kGAMBDebmMbu3eWD882k83dorWmwt36srnFuj7tANOR6s0uQXFenruZoXW9dPsdVm6Z0qy8otKnI6FWozyDACAh2tY10/PDOuoz8b3VHCgj+56P1l/n5OmohLPH6WdtGCHDhwv0JtjOumpG9trwdZsjX13JQ9LwjGUZwAAaojOjRvoiwm9dGv3OL21cKdGvrVc+46ecjrWRUvPydObC3dqcEKUOjcO0aiucXppZKJS9hzRLe+s0JHcQqcjohaiPAMAUIP4+3jpb0Pa6+VRidqcdVzXv7xIP2456HSsi/LU3DS5jPTbAa1/OjaoY5TeHNNZm/ef0Ig3l+nA8XwHE6I2ojwDAFADDeoYpVn391ZEPX/d8d4q/ePrzSr2oGkcy3ce1pz1+zW+X3NFBQf8x2dXXhahyXd00b6jpzR80jKl5+Q5lBK1EeUZAIAaqllYXX0xoZdGdY3VG/N3aPTbK7T/mPuP1JaUWv3ly02KDg7QPX2bnvWcns1C9eHd3XXsVJGGT1qm7Qdr9lJ9cB+UZwAAajB/Hy89dWMHvXBzR63PPKbrX16khVuznY51Tp8kpyst67geu7a1Any9fva8hNhgTbu3u4pLrUa8uVwbMo9VY0rUVpRnAABqgaGJMfry/l5qWNdXY99bqee+2eKWm44cO1Wkf87boi7xDTSwQ6Pznt86sp4+HddDAT5eGvXWcq3anVMNKVGbUZ4BAKglmocHaeaE3hrWKUav/LBdt7yzXAfd7IG7V77fppy8Qv3phrYyxlTomiahdfTpuB4KC/LTmH+t0AI3H1mHZ6M8AwBQiwT4eunZ4R317LAOSk0/quteXqwl2w85HUuStCP7pCYv3a0RnWPVLrr+BV0bFRygT8b1UNPQuvrl+6s0d31WFaVEbUd5BgCgFhqeFKuZE3qrfoC3bv3XCr343VbHp3E8OTtN/j5e+tU1rS7q+tC6fvronu5qH11fE6au1vSUjEpOCFCeAQCotVpFBmnWxN4akhCtF7/bprHvrlT2iQJHsszfclA/bD6o+69orrAgv4u+T/0AH025q5t6NgvVrz5dq/eX7q68kIAozwAA1Gp1/Lz1/IiOevrG9lq1O0fXvbxIy3certYMRSWleuKrTYpvGKjbe8Vf8v3q+HnrnbFJurpNhP40a6Ne/WGbrHW/hyPhmSjPAADUcsYYjewapy8m9FKQn7dGv71cr/6wTaXVNI1jyrI92pGdq99f30Z+3j+/NN2F8Pfx0uu3dNLQxGj985utenruZgo0KgXlGQAASJIua1RPs+7vres7ROmf32zV+A9TlF9UUqXfmZNbqBe/26o+LUJ15WXhlXpvby+XnhveUbd2j9ObC3fq/77Y4Pi8bng+yjMAAPhJXT9vvTwyQb+//jJ9s+mAbnlnhY7kFlbZ9z3/7RblFpboDwPbVHhpugvhchk9Mbidxvdvpqkr9uqRT1I9aptyuB/KMwAA+A/GGP2yT1O9OqqT1mcc002Tlio9J6/Svyct67imrtirMd0bq2VEUKXf/wxjjH47oLV+fU0rzUzdp2e/2VJl34Waj/IMAADO6voOjTTlrq46dKJAN76xtFK3v7bW6q9fblK9AB899IsWlXbfc5lwefPTUzgW7NTsdawDjYtDeQYAAD+rW9OGmj6+p3xcRje/uazSdu+bt/GAlu08rEeuaqngQN9KuWdF/HFgW3WKC9avp6/V1gMnqu17UXNQngEAwDm1jAjSjAm9FBsSqLsmr9KnyemXdL/8ohI9OWeTWkbU1eiucZWUsmJ8vV1649bOquPnrXunpOjYqaJq/X54PsozAAA4r4h6/vp0XA91axqiX09fp1e+v/i1k99dskvpOaf0x4Ft5e1V/VUkop6/Xr+lk9Jz8vTItNRqW5IPNQPlGQAAVEiQv4/eu72rhiZG67lvt+rxGRsueOWKA8fz9eoP23VVmwj1bhFaRUnPr0t8iP4wsI2+33xQr/yw3bEc8DzeTgcAAACew9fbpedHdFRkfX+9MX+HDh7P1yujExXoW7FK8czXW1RUUqr/u+6yKk56frf1aKy1GUf14vdb1T6mnq5oHeF0JHgARp4BAMAFObP02xOD2+rHLQc16u0VOnyy4LzXrU0/qs9WZ+jO3k0UH1qnGpKemzFGfx/aXm2j6unBj1O161Cu05HgASjPAADgoozpEa9Jt3bW5qzjuumNpdpz+OfLp7VWf/5yo0Lr+mni5c2rMeW5+ft4adKtneXtMrp3SrJyC4qdjgQ3R3kGAAAX7eq2kZp6d3cdO1WkG19fqtT0o2c9b2bqPq3Ze1S/uaaVgvx9qjnlucU0CNQrozpp+8GT+s1n6y76QUjUDpRnAABwSTo3bqDPxvdUoJ+XRr21XN+nHfiPz/MKi/X03M1qH11fwzrHOJTy3Hq3CNVvBrTW7HVZenvRTqfjwI1RngEAwCVrGlZXn4/vpebhdXX3v5P10cq9P302af4O7T+erz/d0EYul3Ew5bnd27eprmsfqafnbtaS7YecjgM3RXkGAACVIizITx/f0139Wobpd5+v1/PfbFF6Tp7eXLhTN3SMUlJ8iNMRz8kYo2eGdVSzsLqaOHW1Mo7kOR0JbojyDAAAKk0dP2+9fVuSbk6K1cs/bNfQ15fIGOmxa1s7Ha1C6vp5680xnVVcYjX+g9XKLypxOhLcDOUZAABUKm8vl56+qb0e+kULHTpZqPH9mis6OMDpWBXWNKyuXhyZoPWZx/T7LzbwACH+A5ukAACASmeM0UO/aKkbE2MUG+I5xfmMKy+L0INXttBL329Tx5j6GtMj3ulIcBOMPAMAgCoT1zBQxrjvQ4Ln8uCVLXRF63D95ctNStmT43QcuAnKMwAAwFm4XEYv3JygmAYBGvfBah08nu90JLiBCpVnY8wAY8wWY8x2Y8xj5zjvJmOMNcYklTv2u7LrthhjrrnQewIAADilfoCP3hyTpJP5xRr/4WoVFpc6HQkOO295NsZ4SXpN0rWS2kgaZYxpc5bzgiQ9KGlFuWNtJI2U1FbSAEmvG2O8KnpPAAAAp7WKDNKzwzsoZc8R/W32JqfjwGEVGXnuKmm7tXantbZQ0seSBp/lvCck/UNS+X/TGCzpY2ttgbV2l6TtZfer6D0BAAAcN7BDlO7p21T/XrZH01MynI4DB1WkPEdLSi/3PqPs2E+MMZ0kxVprZ1fw2vPes9y97zHGJBtjkrOzsysQFwAAoPL95ppW6tW8oR6fsV7rM445HQcOueQHBo0xLknPS3r00uP8L2vtW9baJGttUlhYWFV8BQAAwHl5e7n08shEhdX107gPUnQkt9DpSHBARcpzpqTYcu9jyo6dESSpnaT5xpjdkrpLmlX20ODPXXu+ewIAALidhnX99MatnZR9okC/+nQtG6jUQhUpz6sktTDGNDHG+Or0A4CzznxorT1mrQ211sZba+MlLZc0yFqbXHbeSGOMnzGmiaQWklae754AAADuqkNMsB6/rrW+33xQ7yza5XQcVLPzlmdrbbGkiZLmSUqT9Im1dqMx5q/GmEHnuXajpE8kbZL0taQJ1tqSn7vnpf0UAACA6jG2Z7wGtI3UP77erNV7jzgdB9XIeNI/NyQlJdnk5GSnYwAAAOjYqSINfGWRSkul2Q/0VnCgr9ORUEmMMSnW2qSzfcYOgwAAABehfoCPXh3VSQdP5OtXn65j/nMtQXkGAAC4SB1jg/XYtZfpu7QDenfJbqfjoBpQngEAAC7Bnb3idVWbCD09N02p6UedjoMqRnkGAAC4BMYY/XNYR4UH+Wvi1NU6dqrI6UioQpRnAACAS1Q/0Eevjk7U/mP5+s101n+uySjPAAAAlSAxroEeu7a15m08oMlLdzsdB1WE8gwAAFBJ7urdRL+4LFx/n5OmdRnMf66JKM8AAACVxBijfw7vqLC6fprA/OcaifIMAABQiYIDffXK6ETtO5qvxz5j/eeahvIMAABQyTo3DtFvrmmluRv2a8ryPU7HQSWiPAMAAFSBu/s01eWtwvS3r9K0IfOY03FQSSjPAAAAVcDlMnpuRIJC6vhqwtTVOpHP/OeagPIMAABQRULqnJ7/nHHklB77fD3zn2sAyjMAAEAV6hIfokevbqnZ67L04Yq9TsfBJaI8AwAAVLFxfZupX8sw/fWrTdq4j/nPnozyDAAAUMVcLqPnR3RUg0AfTZy6RicLip2OhItEeQYAAKgGDev66eWRidpzOFePM//ZY1GeAQAAqkm3pg31yFUtNWvtPn20Mt3pOLgIlGcAAIBqdF//5urTIlR/+XKj0rKOOx0HF4jyDAAAUI1Oz39OUL0AH034cLVymf/sUSjPAAAA1SwsyE8vjUzQrsO5+sPMDU7HwQWgPAMAADigZ7NQ3X9FC32+OlOfpWQ4HQcVRHkGAABwyANXNFfXJiH6w8wN2pF90uk4qADKMwAAgEO8vVx6aWSC/Lxdmjh1jfKLSpyOhPOgPAMAADioUf0A/XN4R6VlHddTc9KcjoPzoDwDAAA47MrLInRX7yZ6f9kefb1hv9NxcA6UZwAAADfwmwGt1D66vn4zfa0yjuQ5HQc/g/IMAADgBvy8vfTq6ESVWumBj9aoqKTU6Ug4C8ozAACAm2jcsI7+fmN7rd57VC98u9XpODgLyjMAAIAbGdQxSjcnxeqNBTu0aFu203HwXyjPAAAAbubPg9qqeVhdPTxtrbJPFDgdB+VQngEAANxMgK+XXh3dSSfyi/TIJ6kqLbVOR0IZyjMAAIAbahUZpD/d0FaLth3SpIU7nI6DMpRnAAAANzWqa6yu79BIz32zVSl7cpyOA1GeAQAA3JYxRk/d2F5Rwf564KNUHc0rdDpSrUd5BgAAcGP1/H30yqhOOnA8X7/9bJ2sZf6zkyjPAAAAbi4hNli/HdBa8zYe0JTle5yOU6tRngEAADzAXb2b6PJWYfrbV2nauO+Y03FqLcozAACAB3C5jP45vKMa1PHR/VPXKLeg2OlItRLlGQAAwEM0rOunF29O1O7DufrDzA1Ox6mVKM8AAAAepEezhrr/ihb6fHWmPkvJcDpOrUN5BgAA8DD3X9FcXZuE6A8zN2hH9kmn49QqlGcAAAAP4+3l0ssjE+Xn7dLEqWuUX1TidKRag/IMAADggSLr++u5ER2VlnVcT8/d7HScWoPyDAAA4KGuaB2h23o01r+X7daew7lOx6kVKM8AAAAebOLlzeXtcumthTudjlIrUJ4BAAA8WHg9f93UOVqfpmQo+0SB03FqPMozAACAh7u7T1MVlZRq8tJdTkep8SjPAAAAHq5pWF0NaBupKcv26CQ7D1YpyjMAAEANMK5fMx3PL9ZHK/Y6HaVGozwDAADUAB1jg9WjaUP9a/EuFRaXOh2nxqI8AwAA1BDj+jfT/uP5+iI10+koNRblGQAAoIbo2yJUbRrV06QFO1Raap2OUyNRngEAAGoIY4zG9W+mndm5+jbtgNNxaiTKMwAAQA1yXbtIxYYEaNKCHbKW0efKRnkGAACoQby9XLqnT1Ot2XtUK3flOB2nxqE8AwAA1DDDk2LVsI6vJi3Y4XSUGofyDAAAUMP4+3jp9p7x+nFLtjbvP+50nBqlQuXZGDPAGLPFGLPdGPPYWT4fZ4xZb4xJNf+vvXuPlrK+7z3+/m4QRUQuchG5gyDiDRU1JiokYFZSU5LY1mDTRFeNFRObRHvOSc5qzlrn5KyzVpuclaZZtaKJMSaNErVNQpukNokCSjSCivcbN7mIiqCo3C/f/rEf6ohcng17z7Nn7/drrVnMPPPMzGe+zho+/nhmJuL+iBhfbP90sW33aVdETCium1Pc5+7rBrTuU5MkSeq8PnPucI7s1oUb5y6tOkqHcsDyHBFdgOuBjwLjgUt3l+Mat2XmKZk5AfgG8C2AzPxxZk4otn8GWJaZi2pu9+nd12fmq63xhCRJkgS9j+zGpWcPY/ZjL7Fy/aaq43QYZVaezwYWZ+bSzNwGzAI+XrtDZtb+e0APYG8f7by0uK0kSZLq4HPnj6Qp4Ob7l1UdpcMoU54HAytrLq8qtr1LRHwhIpbQvPL8xb3cz6eA2/fYdktxyMb/iojY24NHxF9ExMKIWLh27doScSVJkgQwqFd3Pj5hMLMWrGD9xm1Vx+kQWu0Dg5l5fWaOBr4CfK32uog4B9iUmU/WbP50Zp4CnF+cPrOP+70pMydm5sT+/fu3VlxJkqROYcakUWzZvotbf7e86igdQpnyvBoYWnN5SLFtX2YBn9hj23T2WHXOzNXFn28Bt9F8eIgkSZJa0fEDejL1xIHc+sByNm3bUXWchlemPC8AxkTEyIjoRnMRnl27Q0SMqbl4EfBCzXVNwCXUHO8cEV0jol9x/jDgY0DtqrQkSZJaydWTR/HGpu38ZMHKA++s/Tpgec7MHcA1wN3AM8AdmflURHw9IqYVu10TEU9FxCLgOuCymru4AFiZmbXfk3I4cHdEPA4sonkl+7uH/nQkSZK0pzOH9+WsEX343n3L2L5zV9VxGlo00m+eT5w4MRcuXFh1DEmSpIZzz7Ov8Oc/WMi3LjmNi88YUnWcdi0iHs7MiXu7zl8YlCRJ6gQ+eMIAThjYkxvnLqWRFk/bG8uzJElSJxARXDVpFM+98hb3Pudv0x0sy7MkSVIn8YenHcfg3t2ZOcef7D5YlmdJkqRO4rAuTVxx3kgeWr6eh198veo4DcnyLEmS1IlMP3sovY88jJlzl1QdpSFZniVJkjqRI7t15bPnjuDXT7/C4lffqjpOw7E8S5IkdTKXv38ERxzWxMy5HvvcUpZnSZKkTqZvj25MP2sYP1+0mjUbNlcdp6FYniVJkjqhK84bya6Em+9bVnWUhmJ5liRJ6oSG9j2SPzx1ELc/tIINm7ZXHadhWJ4lSZI6qasmjWbjtp386MHlVUdpGJZnSZKkTurEQUcz+YT+3DJ/OVu276w6TkOwPEuSJHViMyaNZt3Gbdy5cGXVURqC5VmSJKkTO2dkX04f1pub7lvKjp27qo7T7lmeJUmSOrGIYMak0axcv5lfPvly1XHaPcuzJElSJ3fhiQMZ3b8HN8xZwq5dWXWcds3yLEmS1Mk1NQVf+ODxPLPmTf7jaVef98fyLEmSJKaddhyj+vfg2795wdXn/bA8S5Ikia5dmvjSlDE8+/Jb/Mpjn/fJ8ixJkiQAPnbqaZMIfAAAD+pJREFUcRw/4Cj+/rfPu/q8D5ZnSZIkAdClKfjSlDE8/8rb/OKJNVXHaZcsz5IkSfovF50yiLEDj+Lbv3mena4+v4flWZIkSf+lqSn40pSxLFm7kX97/KWq47Q7lmdJkiS9y0dPPpZxx/bk73/zgr86uAfLsyRJkt6lqSn48tQxLH1tI7Mfc/W5luVZkiRJ7/Hh8ccyftDRfOe3rj7XsjxLkiTpPXavPi9ft4mfLXL1eTfLsyRJkvbqwvEDOXlw8+rzdlefAcuzJEmS9iEiuHbqWFas38RPH1lddZx2wfIsSZKkffrQuAGcNqQX37nH1WewPEuSJGk/IoIvTx3Lqtc3c9fDq6qOUznLsyRJkvZr8gn9mTC0N/9wz2K27ejcq8+WZ0mSJO1XRHDthWNZ/cZm7nx4ZdVxKmV5liRJ0gFdMKYfZwxrXn3eumNn1XEqY3mWJEnSAUUE1114Ams2bOGOBZ139dnyLEmSpFI+cPwxnDWiD9ffu4Qt2zvn6rPlWZIkSaXs/t7nl9/cwqyHVlQdpxKWZ0mSJJV27uhjOGdkX/5xTudcfbY8S5IkqbTd37zx6ltbue33nW/12fIsSZKkFnnfqGN4/+hj+Mc5S9i8rXOtPlueJUmS1GLXXjiW197eyo9//2LVUerK8ixJkqQWO2tEX847vh8z5y5h07YdVcepG8uzJEmSDsq1F47htbe38aMHOs/qs+VZkiRJB+XM4X25YGx/bpy3lI1bO8fqs+VZkiRJB+3aqWNYv3EbP+wkq8+WZ0mSJB2004f14YMn9OfGeUt4uxOsPlueJUmSdEi+PHUsb2zazq2/W151lDZneZYkSdIhOW1ob6aMG8BN85by1pbtVcdpU5ZnSZIkHbJrLxzLhs3buWX+8qqjtCnLsyRJkg7ZyYN7ceH4gXzvvqVs2NxxV58tz5IkSWoVX546hje37OCW+cuqjtJmLM+SJElqFScd14sPjx/ILfOXd9jvfbY8S5IkqdVcPXk0GzZv5/aHVlQdpU1YniVJktRqTh/Wh3NG9uXm+5exbceuquO0OsuzJEmSWtWMyaNZs2ELsx97qeoorc7yLEmSpFY1eWx/xh3bkxvnLmHXrqw6TquyPEuSJKlVRQRXTx7NC6++zT3Pvlp1nFZleZYkSVKru+iUQQzp052Zc5dUHaVVWZ4lSZLU6rp2aeLK80ex8MXXWbB8fdVxWk2p8hwRH4mI5yJicUR8dS/Xz4iIJyJiUUTcHxHji+0jImJzsX1RRMysuc2ZxW0WR8R3IiJa72lJkiSpapdMHErfHt2YOafjrD4fsDxHRBfgeuCjwHjg0t3luMZtmXlKZk4AvgF8q+a6JZk5oTjNqNl+A3AlMKY4feQQnockSZLame7dunDZuSP47bOv8tzLb1Udp1WUWXk+G1icmUszcxswC/h47Q6Z+WbNxR7Afj9WGRGDgKMz88HMTOCHwCdalFySJEnt3mfPHU73w7pw47yOsfpcpjwPBlbWXF5VbHuXiPhCRCyheeX5izVXjYyIRyNibkScX3Ofqw50n8X9/kVELIyIhWvXri0RV5IkSe1Fnx7dmH72UGYveonVb2yuOs4ha7UPDGbm9Zk5GvgK8LVi8xpgWGaeDlwH3BYRR7fwfm/KzImZObF///6tFVeSJEl18rnzRwFw833LKk5y6MqU59XA0JrLQ4pt+zKL4hCMzNyameuK8w8DS4Cxxe2HtOA+JUmS1KAG9+7OtAnHMWvBCl7fuK3qOIekTHleAIyJiJER0Q2YDsyu3SEixtRcvAh4odjev/jAIRExiuYPBi7NzDXAmxHxvuJbNj4L/PyQn40kSZLapRmTRrNp205++MCLVUc5JAcsz5m5A7gGuBt4BrgjM5+KiK9HxLRit2si4qmIWETz4RmXFdsvAB4vtt8FzMjM3V/093nge8Bimlekf9VaT0qSJEnty9iBPZkybgC3PrCczdt2Vh3noEXzl100hokTJ+bChQurjiFJkqSDsGD5ev5k5gP8n2kncdn7R1QdZ58i4uHMnLi36/yFQUmSJNXFWSP6cubwPnz3vqXs2Lmr6jgHxfIsSZKkupkxaTSrXt/ML55YU3WUg2J5liRJUt1MGTeAMQOOYubcpTTS4cO7WZ4lSZJUN01NwVWTRvPMmjeZ+3zj/QCe5VmSJEl1Ne204xjU6whumNN4P9lteZYkSVJddevaxBXnjeT3y9bz6IrXq47TIpZnSZIk1d2lZw+jV/fDmDm3sVafLc+SJEmqux6Hd+Wz5w7nP55+hSVr3646TmmWZ0mSJFXisvePoFuXJm6au7TqKKVZniVJklSJfkcdziUTh/Ivj67i5Q1bqo5TiuVZkiRJlbny/FHs3JV8f/6yqqOUYnmWJElSZYYdcyQfO/U4bvv9CjZs3l51nAOyPEuSJKlSV00axdtbd/BPD75YdZQDsjxLkiSpUicd14sLxvbnlvnL2bJ9Z9Vx9svyLEmSpMrNmDSK197eyj8/sqrqKPtleZYkSVLlzh11DKcN6cV35y1l566sOs4+WZ4lSZJUuYhgxqTRLF+3iX9/8uWq4+yT5VmSJEntwodPOpaR/Xpww9zFZLbP1WfLsyRJktqFLk3BVReM4snVbzJ/8bqq4+yV5VmSJEntxifPGMyAnoczc+6SqqPsleVZkiRJ7cbhXbvw5+eN5P7Fr/HEqg1Vx3kPy7MkSZLalT89Zxg9D+/KjfPa3+pz16oDSJIkSbWOPuIwvj19AuMGHV11lPewPEuSJKndmXLiwKoj7JWHbUiSJEklWZ4lSZKkkizPkiRJUkmWZ0mSJKkky7MkSZJUkuVZkiRJKsnyLEmSJJVkeZYkSZJKsjxLkiRJJVmeJUmSpJIsz5IkSVJJlmdJkiSpJMuzJEmSVJLlWZIkSSrJ8ixJkiSVZHmWJEmSSrI8S5IkSSVZniVJkqSSLM+SJElSSZZnSZIkqaTIzKozlBYRa4EXK3jofsBrFTxuZ+Oc254zrg/n3PaccX0457bnjOujpXMenpn993ZFQ5XnqkTEwsycWHWOjs45tz1nXB/Oue054/pwzm3PGddHa87ZwzYkSZKkkizPkiRJUkmW53JuqjpAJ+Gc254zrg/n3PaccX0457bnjOuj1ebsMc+SJElSSa48S5IkSSVZnmtExEci4rmIWBwRX93L9ddFxNMR8XhE/DYihleRs5GVmPGMiHgiIhZFxP0RMb6KnI3uQHOu2e+PIiIjwk96t1CJ1/LlEbG2eC0viojPVZGz0ZV5LUfEJcV781MRcVu9Mza6Eq/lv6t5HT8fEW9UkbPRlZjzsIi4NyIeLXrGH1SRs5GVmPHwor89HhFzImLIQT1QZnpqPnSlC7AEGAV0Ax4Dxu+xzweBI4vzVwM/qTp3I51KzvjomvPTgH+vOnejncrMudivJzAPeBCYWHXuRjqVfC1fDvxD1Vkb+VRyzmOAR4E+xeUBVedupFPZ94ua/f8S+H7VuRvtVPK1fBNwdXF+PLC86tyNdCo54zuBy4rzHwJ+dDCP5crzO84GFmfm0szcBswCPl67Q2bem5mbiosPAgf3fyydV5kZv1lzsQfgQfktd8A5F/4v8LfAlnqG6yDKzliHpsycrwSuz8zXATLz1TpnbHQtfS1fCtxel2QdS5k5J3B0cb4X8FId83UEZWY8HrinOH/vXq4vxfL8jsHAyprLq4pt+3IF8Ks2TdTxlJpxRHwhIpYA3wC+WKdsHckB5xwRZwBDM/MX9QzWgZR9v/ij4p8H74qIofWJ1qGUmfNYYGxEzI+IByPiI3VL1zGU/ruvOFRxJO+UD5VXZs7/G/iziFgF/JLmVX6VV2bGjwEXF+c/CfSMiGNa+kCW54MQEX8GTAS+WXWWjigzr8/M0cBXgK9VnaejiYgm4FvAX1WdpYP7V2BEZp4K/Bq4teI8HVVXmg/dmEzzquh3I6J3pYk6runAXZm5s+ogHdSlwA8ycwjwB8CPivdrtZ7/BkyKiEeBScBqoMWvZ/+jvGM1ULsyNKTY9i4RMRX4a2BaZm6tU7aOotSMa8wCPtGmiTqmA825J3AyMCcilgPvA2b7ocEWOeBrOTPX1bxHfA84s07ZOpIy7xmrgNmZuT0zlwHP01ymVU5L3pen4yEbB6vMnK8A7gDIzAeAI4B+dUnXMZR5X34pMy/OzNNp7nJkZos/AGt5fscCYExEjIyIbjS/Scyu3SEiTgdupLk4e1xdy5WZce1fehcBL9QxX0ex3zln5obM7JeZIzJzBM3H70/LzIXVxG1IZV7Lg2ouTgOeqWO+juKAcwZ+RvOqMxHRj+bDOJbWM2SDKzNjImIc0Ad4oM75Oooyc14BTAGIiBNpLs9r65qysZV5X+5Xs5r/P4HvH8wDWZ4LmbkDuAa4m+a/5O7IzKci4usRMa3Y7ZvAUcCdxVf2vOcNRvtWcsbXFF83tQi4DrisorgNq+ScdQhKzviLxWv5MZqP3b+8mrSNq+Sc7wbWRcTTNH8A6L9n5rpqEjeeFrxfTAdmZfE1BWqZknP+K+DK4j3jduBy511eyRlPBp6LiOeBgcD/O5jH8hcGJUmSpJJceZYkSZJKsjxLkiRJJVmeJUmSpJIsz5IkSVJJlmdJkiSpJMuzJFUsInpHxOeL85Mj4t/a4DF+EBF/3IL9R0TEk/u4bo4/qiOps7I8S1L1egOfb8kNIqJLG2WRJO2H5VmSqvc3wOjix4G+CRwVEXdFxLMR8eOICICIWB4RfxsRjwB/EhEfjogHIuKRiLgzIo4q9vubiHg6Ih6PiP9f8zgXRMTvImLp7lXoaPbNiHgyIp6IiE/tGS4iukfErIh4JiJ+CnRv64FIUnvVteoAkiS+CpycmRMiYjLwc+Ak4CVgPvAB4P5i33WZeUbxU9T/AkzNzI0R8RXguoi4HvgkMC4zMyJ61zzOIOA8YBzNP1t7F3AxMAE4DegHLIiIeXvkuxrYlJknRsSpwCOt/PwlqWG48ixJ7c9DmbkqM3cBi4ARNdf9pPjzfcB4YH6xYn0ZMBzYAGwBbo6Ii4FNNbf9WWbuysynaf5pWmgu07dn5s7MfAWYC5y1R54LgH8CyMzHgcdb52lKUuNx5VmS2p+tNed38u736o3FnwH8OjMv3fPGEXE2MAX4Y+Aa4EN7ud9otbSS1Im48ixJ1XsL6NnC2zwIfCAijgeIiB4RMbY47rlXZv4SuJbmwzH25z7gUxHRJSL607zK/NAe+8wD/rR4nJOBU1uYVZI6DFeeJalimbkuIuYXXw23GXilxG3WRsTlwO0RcXix+Ws0F/GfR8QRNK8uX3eAu/opcC7wGJDA/8jMlyNiRM0+NwC3RMQzwDPAw2WfmyR1NJGZVWeQJEmSGoKHbUiSJEklWZ4lSZKkkizPkiRJUkmWZ0mSJKkky7MkSZJUkuVZkiRJKsnyLEmSJJVkeZYkSZJK+k8Vl8m/wOu3/AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVN8aJUvrQzx"
      },
      "source": [
        "# resnet\n",
        "\n",
        "# Model is parametrized in a way to enable easy change of decoder_block type,\n",
        "# as this is an argument that can be given a function, like decoder_block_simple.\n",
        "def unet_resnet(input_size, decoder_block,\n",
        "                weights='imagenet',\n",
        "                loss_func='binary_crossentropy',\n",
        "                metrics_list=[my_iou_metric],\n",
        "                use_lovash=False):\n",
        "\n",
        "    # Base model - encoder\n",
        "    base_model = ResNet50(\n",
        "        input_shape=input_size, \n",
        "        include_top=False,\n",
        "        weights=weights)\n",
        "    \n",
        "    # Layers for feature extraction in the encoder part\n",
        "    encoder1 = base_model.get_layer('conv1_relu').output\n",
        "    encoder2 = base_model.get_layer('conv2_block1_out').output\n",
        "    encoder3 = base_model.get_layer('conv3_block1_out').output\n",
        "    encoder4 = base_model.get_layer('conv4_block1_out').output\n",
        "    encoder5 = base_model.get_layer('conv5_block3_out').output\n",
        "\n",
        "    # Center block\n",
        "    center = decoder_block(\n",
        "        encoder5, 'center', num_filters=512)\n",
        "    concat5 = concatenate([center, encoder5], axis=-1)\n",
        "\n",
        "    # Decoder part.\n",
        "    # Every decoder block processed concatenated output from encoder and decoder part.\n",
        "    # This creates skip connections.\n",
        "    # Afterwards, decoder output is upsampled to dimensions equal to encoder output part.\n",
        "    decoder4 = decoder_block(\n",
        "        concat5, 'decoder4', num_filters=256)\n",
        "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1)\n",
        "\n",
        "    decoder3 = decoder_block(\n",
        "        concat4, 'decoder3', num_filters=128)\n",
        "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1)\n",
        "\n",
        "    decoder2 = decoder_block(\n",
        "        concat3, 'decoder2', num_filters=64)\n",
        "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
        "\n",
        "    decoder1 = decoder_block(\n",
        "        concat2, 'decoder1', num_filters=64)\n",
        "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1) \n",
        "\n",
        "    # Final upsampling and decoder block for segmentation.\n",
        "    output = UpSampling2D()(concat1)\n",
        "    output = decoder_block(\n",
        "        output, 'decoder_output', num_filters=32)\n",
        "    output = Conv2D(\n",
        "        1, (1, 1), activation=None, name='prediction')(output)\n",
        "    if not use_lovash:\n",
        "        output = Activation('sigmoid')(output)\n",
        "        \n",
        "    model = Model(base_model.input, output)\n",
        "    model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ligURxpramd",
        "outputId": "ccba7ffa-dee4-486a-a519-1af76620e717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_size = (224, 224, 3)\n",
        "K.clear_session()\n",
        "resnet_model = ResNet50(\n",
        "        input_shape=input_size, \n",
        "        include_top=False,\n",
        "        weights='imagenet')\n",
        "resnet_model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBmftRhKrdUI",
        "outputId": "4e105f53-a3af-4b2b-8555-e494621b37d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_size = (224, 224, 3)\n",
        "\n",
        "K.clear_session()\n",
        "model = unet_resnet(\n",
        "    input_size, decoder_block_simple, weights='imagenet')\n",
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/kernel:0' shape=(3, 3, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'center_bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/kernel:0' shape=(3, 3, 2560, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'decoder4_bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/kernel:0' shape=(3, 3, 1280, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'decoder3_bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/kernel:0' shape=(3, 3, 640, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder2_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/kernel:0' shape=(3, 3, 320, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder1_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/kernel:0' shape=(3, 3, 128, 32) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/bias:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_bn/gamma:0' shape=(32,) dtype=float32>\n",
            "  <tf.Variable 'decoder_output_bn/beta:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/kernel:0' shape=(1, 1, 32, 1) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/bias:0' shape=(1,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asmQnLxqrix9",
        "outputId": "377b229c-3dfc-48f0-a0a6-bb3c69c2ef63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "# Build model:\n",
        "# Here, you can experiment with various losses.\n",
        "# For dice and BCE (binary_crossentropy), my_iou_metric should be used,\n",
        "# whereas for lovash_loss my_iou_metric2 should be used, because range of values\n",
        "# for lovash loss is between -inf and +inf, not between 0 and 1, as for BCE and dice.\n",
        "# What is more, when lovash loss is used, last layer (sigmoid) should be deleted.\n",
        "# This is controlled by use_lovash parameter.\n",
        "model_depth = unet_resnet(\n",
        "    input_size, decoder_block_bottleneck, weights='imagenet',\n",
        "    loss_func=bce_dice_loss, metrics_list=[my_iou_metric],\n",
        "    use_lovash=False)\n",
        "print(model_depth.summary())\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'unet_resnet.h5' ,monitor='val_my_iou_metric', mode='max',\n",
        "    save_best_only=True, save_weights_only=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_my_iou_metric',\n",
        "    mode='max',\n",
        "    factor=0.5, \n",
        "    patience=5, \n",
        "    min_lr=0.0001, \n",
        "    verbose=1)\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 16\n",
        "\n",
        "history = model_depth.fit(X_tr, y_tr,\n",
        "                    validation_data=(X_val, y_val), \n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=[model_checkpoint,reduce_lr], \n",
        "                    verbose=1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-525c8ea97098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Build model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Here, you can experiment with various losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# For dice and BCE (binary_crossentropy), my_iou_metric should be used,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKpiEQ60tNXc",
        "outputId": "950199cc-1d4e-4ab9-c65b-eddedf781d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "val_preds = model_depth.predict(X_val, batch_size=16)\n",
        "\n",
        "y_val_pred = np.asarray(list(map(lambda x: cv2.resize(x, (101, 101)), val_preds)))\n",
        "y_val_true = np.asarray(list(map(lambda x: cv2.resize(x, (101, 101)), y_val)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9ef199f5b604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_depth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_val_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_val_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_depth' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0S7mn2PEDOM"
      },
      "source": [
        "# Threshold range, over which optimization is performed\n",
        "thresholds = np.arange(0.2, 0.9, 0.02)\n",
        "\n",
        "# For every threshold, set predictions to binary arrays, \n",
        "# where values above threshold are treated as 1 and the rest as 0.\n",
        "# Loop over thresholds and compute IoU for them based on IoU function above.\n",
        "ious = np.array(\n",
        "    [iou_metric_batch(y_val_true,\n",
        "                      np.int32(y_val_pred > threshold)) for threshold in tqdm(thresholds)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dENV3HdIE0Fu"
      },
      "source": [
        "df_iou = pd.DataFrame(thresholds, columns=['threshold'])\n",
        "df_iou['iou'] = ious\n",
        "\n",
        "# Get index of best IoU\n",
        "best_index = df_iou['iou'].idxmax()\n",
        "print('Best IoU: {:.4f} at threshold: {:.3f}'.format(\n",
        "    df_iou.iou[best_index], df_iou.threshold[best_index]))\n",
        "\n",
        "# Describe IoU DF\n",
        "df_iou.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqw5oQF9E7RB"
      },
      "source": [
        "# Plot IoU values over threshold range.\n",
        "df_iou.plot(x='threshold', y='iou')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}