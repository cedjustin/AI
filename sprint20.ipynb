{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlQpndT6JubGvUWYvUWwbk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedjustin/AI/blob/master/sprint20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eIibpiWBPOI"
      },
      "source": [
        "[Problem 1] Code review\n",
        "\n",
        "- This implementation uses res-net instead of u-net\n",
        "- It uses layers that has been learned by using image-net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5apbzUrACWcC"
      },
      "source": [
        "[Problem 2] Code rewriting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roL5VSm38PhB"
      },
      "source": [
        "import gc\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.callbacks import *\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import *\n",
        "from keras.models import Model, load_model, save_model\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 9)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvrWeV7NC6UR"
      },
      "source": [
        "def compute_coverage(df, masks):\n",
        "    \n",
        "    df = df.copy()\n",
        "    \n",
        "    def cov_to_class(val):\n",
        "        for i in range(0, 11):\n",
        "            if val * 10 <= i:\n",
        "                return i\n",
        "\n",
        "    # Output percentage of area covered by class\n",
        "    df['coverage'] = np.mean(masks, axis=(1, 2))\n",
        "    # Coverage must be split into bins, otherwise stratified split will not be possible,\n",
        "    # because each coverage will occur only once.\n",
        "    df['coverage_class'] = df.coverage.map(\n",
        "        cov_to_class)\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_depth_abs_channels(image_tensor):\n",
        "    image_tensor = image_tensor.astype(np.float32)\n",
        "    h, w, c = image_tensor.shape\n",
        "    for row, const in enumerate(np.linspace(0, 1, h)):\n",
        "        image_tensor[row, :, 1] = const\n",
        "    image_tensor[:, :, 2] = (\n",
        "        image_tensor[:, :, 0] * image_tensor[:, :, 1])\n",
        "\n",
        "    x_dx = np.diff(image_tensor[:, :, 0], axis=0)\n",
        "    x_dy = np.diff(image_tensor[:, :, 0], axis=1)\n",
        "    x_dx = cv2.copyMakeBorder(x_dx, 1, 0, 0, 0, cv2.BORDER_CONSTANT, 0)\n",
        "    x_dy = cv2.copyMakeBorder(x_dy, 0, 0, 1, 0, cv2.BORDER_CONSTANT, 0)\n",
        "    image_tensor[:, :, 1] = np.abs(x_dx + x_dy)\n",
        "\n",
        "    return image_tensor"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ-f-qTIEWwU"
      },
      "source": [
        "Data loading & depth merge:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE6xEGyiHLr5",
        "outputId": "b6fa1d27-be77-4b1c-9196-c70221ecda18"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX4agkDagEBH",
        "outputId": "9af612a4-cb5b-4c01-af25-015696fe013c"
      },
      "source": [
        "cd drive/MyDrive/tgs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/tgs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkWmwiGvDG_h",
        "outputId": "301c3bdf-9880-46f0-e40a-9b3410d7a0e7"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('sample_submission.csv')\n",
        "depth = pd.read_csv('depths.csv')\n",
        "\n",
        "train_src = '../train_data/'\n",
        "\n",
        "print('train:\\n{}'.format(train.head()))\n",
        "print('\\ntest:\\n{}'.format(test.head()))\n",
        "\n",
        "\n",
        "train = train.merge(depth, how='left', on='id')\n",
        "test = test.merge(depth, how='left', on='id')\n",
        "\n",
        "print('\\n{}'.format(train.head()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:\n",
            "           id                                           rle_mask\n",
            "0  575d24d81d                                                NaN\n",
            "1  a266a2a9df                                          5051 5151\n",
            "2  75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...\n",
            "3  34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...\n",
            "4  4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...\n",
            "\n",
            "test:\n",
            "           id rle_mask\n",
            "0  155410d6fa      1 1\n",
            "1  78b32781d1      1 1\n",
            "2  63db2a476a      1 1\n",
            "3  17bfcdb967      1 1\n",
            "4  7ea0fd3c88      1 1\n",
            "\n",
            "           id                                           rle_mask    z\n",
            "0  575d24d81d                                                NaN  843\n",
            "1  a266a2a9df                                          5051 5151  794\n",
            "2  75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...  468\n",
            "3  34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...  727\n",
            "4  4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...  797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNtNkQNAhjVV"
      },
      "source": [
        "Load images and masks, examine random sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BREwNdrhcd3",
        "outputId": "9103ad0a-3226-4c9b-915b-b17b4e7be2ac"
      },
      "source": [
        "X_train = np.asarray(\n",
        "    [cv2.imread('train/images/{}.png'.format(x), 0) for x in train.id.tolist()], \n",
        "    dtype=np.uint8) / 255.\n",
        "y_train = np.asarray(\n",
        "    [cv2.imread('train/masks/{}.png'.format(x), 0) for x in train.id.tolist()],\n",
        "    dtype=np.uint8) / 255.\n",
        "\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 101, 101) (4000, 101, 101)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "gqaRMdwEhluH",
        "outputId": "3e3ee3f9-dc20-4ba4-928b-bb823a4348cf"
      },
      "source": [
        "random_index = np.random.randint(0, X_train.shape[0])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "ax[0].imshow(X_train[random_index], cmap='gray')\n",
        "ax[1].imshow(y_train[random_index], cmap='gray')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe3031f9210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_mU9em5-0th"
      },
      "source": [
        "\n",
        "Compute salt coverage (this will serve as a basis for stratified split):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDFVEDBN-mKo"
      },
      "source": [
        "train = compute_coverage(train, y_train)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXPVHJn1-58v"
      },
      "source": [
        "\n",
        "Prepare data for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIWQbXTJ-3R_",
        "outputId": "e3a301ba-3c62-4781-df53-777d4a70678e"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=5, random_state=1337)\n",
        "\n",
        "# Add channel features\n",
        "X_train_ch = np.repeat(np.expand_dims(X_train, axis=-1), 3, -1)\n",
        "X_train_ch = np.asarray(list(map(lambda x: create_depth_abs_channels(x), X_train_ch)))\n",
        "\n",
        "# Resize to 224x224, default ResNet50 image size\n",
        "X_resized = np.asarray(list(map(lambda x: cv2.resize(x, (224, 224)), X_train_ch)))\n",
        "y_resized = np.asarray(list(map(lambda x: cv2.resize(x, (224, 224)), y_train)))\n",
        "\n",
        "for train_index, valid_index in kfold.split(train.id.values, train.coverage_class.values):\n",
        "    \n",
        "    X_tr, X_val = X_resized[train_index], X_resized[valid_index]\n",
        "    y_tr, y_val = y_resized[train_index], y_resized[valid_index]\n",
        "    \n",
        "    break\n",
        "    \n",
        "y_tr = np.expand_dims(y_tr, axis=-1)\n",
        "y_val = np.expand_dims(y_val, axis=-1)\n",
        "\n",
        "print(X_tr.shape, y_tr.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "\n",
        "del X_train_ch, y_resized\n",
        "del X_resized\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3200, 224, 224, 3) (3200, 224, 224, 1)\n",
            "(800, 224, 224, 3) (800, 224, 224, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoJU8vQf_ANd"
      },
      "source": [
        "from keras.losses import binary_crossentropy\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# Dice & combined\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred = K.cast(y_pred, 'float32')\n",
        "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
        "    return score\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return 1. - score\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "\n",
        "def bce_logdice_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n",
        "\n",
        "# Lovash loss: https://github.com/bermanmaxim/LovaszSoftmax\n",
        "def lovasz_grad(gt_sorted):\n",
        "    \"\"\"\n",
        "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
        "    See Alg. 1 in paper\n",
        "    \"\"\"\n",
        "    gts = tf.reduce_sum(gt_sorted)\n",
        "    intersection = gts - tf.cumsum(gt_sorted)\n",
        "    union = gts + tf.cumsum(1. - gt_sorted)\n",
        "    jaccard = 1. - intersection / union\n",
        "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
        "    return jaccard\n",
        "\n",
        "# --------------------------- BINARY LOSSES ---------------------------\n",
        "\n",
        "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
        "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
        "      per_image: compute the loss per image instead of per batch\n",
        "      ignore: void class id\n",
        "    \"\"\"\n",
        "    if per_image:\n",
        "        def treat_image(log_lab):\n",
        "            log, lab = log_lab\n",
        "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
        "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
        "            return lovasz_hinge_flat(log, lab)\n",
        "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
        "        loss = tf.reduce_mean(losses)\n",
        "    else:\n",
        "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
        "    return loss\n",
        "\n",
        "def lovasz_hinge_flat(logits, labels):\n",
        "    \"\"\"\n",
        "    Binary Lovasz hinge loss\n",
        "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
        "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
        "      ignore: label to ignore\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss():\n",
        "        labelsf = tf.cast(labels, logits.dtype)\n",
        "        signs = 2. * labelsf - 1.\n",
        "        errors = 1. - logits * tf.stop_gradient(signs)\n",
        "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
        "        gt_sorted = tf.gather(labelsf, perm)\n",
        "        grad = lovasz_grad(gt_sorted)\n",
        "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
        "        return loss\n",
        "\n",
        "    # deal with the void prediction case (only void pixels)\n",
        "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
        "                   lambda: tf.reduce_sum(logits) * 0.,\n",
        "                   compute_loss,\n",
        "                   strict=True,\n",
        "                   name=\"loss\"\n",
        "                   )\n",
        "    return loss\n",
        "\n",
        "def flatten_binary_scores(scores, labels, ignore=None):\n",
        "    \"\"\"\n",
        "    Flattens predictions in the batch (binary case)\n",
        "    Remove labels equal to 'ignore'\n",
        "    \"\"\"\n",
        "    scores = tf.reshape(scores, (-1,))\n",
        "    labels = tf.reshape(labels, (-1,))\n",
        "    if ignore is None:\n",
        "        return scores, labels\n",
        "    valid = tf.not_equal(labels, ignore)\n",
        "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
        "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
        "    return vscores, vlabels\n",
        "\n",
        "def lovasz_loss(y_true, y_pred):\n",
        "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
        "    #logits = K.log(y_pred / (1. - y_pred))\n",
        "    logits = y_pred #Jiaxin\n",
        "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
        "    return loss\n",
        "\n",
        "# IoU metric for observation during training\n",
        "# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\n",
        "def get_iou_vector(A, B):\n",
        "    # Numpy version    \n",
        "    batch_size = A.shape[0]\n",
        "    metric = 0.0\n",
        "    for batch in range(batch_size):\n",
        "        t, p = A[batch], B[batch]\n",
        "        true = np.sum(t)\n",
        "        pred = np.sum(p)\n",
        "        \n",
        "        # deal with empty mask first\n",
        "        if true == 0:\n",
        "            metric += (pred == 0)\n",
        "            continue\n",
        "        \n",
        "        # non empty mask case.  Union is never empty \n",
        "        # hence it is safe to divide by its number of pixels\n",
        "        intersection = np.sum(t * p)\n",
        "        union = true + pred - intersection\n",
        "        iou = intersection / union\n",
        "        \n",
        "        # iou metrric is a stepwise approximation of the real iou over 0.5\n",
        "        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n",
        "        \n",
        "        metric += iou\n",
        "        \n",
        "    # teake the average over all images in batch\n",
        "    metric /= batch_size\n",
        "    return metric\n",
        "\n",
        "def my_iou_metric(label, pred):\n",
        "    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n",
        "\n",
        "# For Lovash loss\n",
        "def my_iou_metric_2(label, pred):\n",
        "    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP95_rWv_WHz",
        "outputId": "1590429d-82b3-4433-ffc3-fa431adf3323"
      },
      "source": [
        "input_size = (224, 224, 3)\n",
        "\n",
        "base_model = ResNet50(input_shape=input_size, include_top=False)\n",
        "base_model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI5mzPGf_wKA"
      },
      "source": [
        "Decoder blocks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srktrq_F_ZRM"
      },
      "source": [
        "# Basic decoder block with Conv, BN and PReLU activation.\n",
        "def decoder_block_simple(\n",
        "        layer_name, block_name,\n",
        "        num_filters=32,\n",
        "        conv_dim=(3, 3)):\n",
        "\n",
        "    x_dec = Conv2D(\n",
        "        num_filters, conv_dim,\n",
        "        padding='same',\n",
        "        name='{}_conv'.format(block_name))(layer_name)\n",
        "    x_dec = BatchNormalization(\n",
        "        name='{}_bn'.format(block_name))(x_dec)\n",
        "    x_dec = PReLU(\n",
        "        name='{}_activation'.format(block_name))(x_dec)\n",
        "\n",
        "    return x_dec\n",
        "\n",
        "# Decoder block with bottleneck architecture, where middle conv layer\n",
        "# is half the size of first and last, in order to compress representation.\n",
        "# This type of architecture is supposed to retain most useful information.\n",
        "def decoder_block_bottleneck(\n",
        "        layer_name, block_name,\n",
        "        num_filters=32,\n",
        "        conv_dim=(3, 3),\n",
        "        dropout_frac=0.2):\n",
        "\n",
        "    x_dec = Conv2D(\n",
        "        num_filters, conv_dim,\n",
        "        padding='same',\n",
        "        name='{}_conv1'.format(block_name))(layer_name)\n",
        "    x_dec = BatchNormalization(\n",
        "        name='{}_bn1'.format(block_name))(x_dec)\n",
        "    x_dec = PReLU(\n",
        "        name='{}_activation1'.format(block_name))(x_dec)\n",
        "    x_dec = Dropout(dropout_frac)(x_dec)\n",
        "\n",
        "    x_dec2 = Conv2D(\n",
        "        num_filters // 2, conv_dim,\n",
        "        padding='same',\n",
        "        name='{}_conv2'.format(block_name))(x_dec)\n",
        "    x_dec2 = BatchNormalization(\n",
        "        name='{}_bn2'.format(block_name))(x_dec2)\n",
        "    x_dec2 = PReLU(\n",
        "        name='{}_activation2'.format(block_name))(x_dec2)\n",
        "    x_dec2 = Dropout(dropout_frac)(x_dec2)\n",
        "\n",
        "    x_dec2 = Conv2D(\n",
        "        num_filters, conv_dim,\n",
        "        padding='same',\n",
        "        name='{}_conv3'.format(block_name))(x_dec2)\n",
        "    x_dec2 = BatchNormalization(\n",
        "        name='{}_bn3'.format(block_name))(x_dec2)\n",
        "    x_dec2 = PReLU(\n",
        "        name='{}_activation3'.format(block_name))(x_dec2)\n",
        "    x_dec2 = Dropout(dropout_frac)(x_dec2)\n",
        "\n",
        "    x_dec2 = Add()([x_dec, x_dec2])\n",
        "\n",
        "    return x_dec2"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDCIhwib_7Dp"
      },
      "source": [
        "Model definition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egHgo_al_2ZD"
      },
      "source": [
        "# Model is parametrized in a way to enable easy change of decoder_block type,\n",
        "# as this is an argument that can be given a function, like decoder_block_simple.\n",
        "def unet_resnet(input_size, decoder_block,\n",
        "                weights='imagenet',\n",
        "                loss_func='binary_crossentropy',\n",
        "                metrics_list=[my_iou_metric],\n",
        "                use_lovash=False):\n",
        "\n",
        "    # Base model - encoder\n",
        "    base_model = ResNet50(\n",
        "        input_shape=input_size, \n",
        "        include_top=False,\n",
        "        weights=weights)\n",
        "    \n",
        "    # Layers for feature extraction in the encoder part\n",
        "    encoder1 = base_model.get_layer('conv1_conv').output # activation_1\n",
        "    encoder2 = base_model.get_layer('conv2_block3_3_conv').output # activation_10\n",
        "    encoder3 = base_model.get_layer('conv3_block4_3_conv').output # activation_22\n",
        "    encoder4 = base_model.get_layer('conv4_block5_3_conv').output # activation_40\n",
        "    encoder5 = base_model.get_layer('conv5_block2_3_conv').output\n",
        "\n",
        "    # Center block\n",
        "    center = decoder_block(\n",
        "        encoder5, 'center', num_filters=512)\n",
        "    concat5 = concatenate([center, encoder5], axis=-1)\n",
        "\n",
        "    # Decoder part.\n",
        "    # Every decoder block processed concatenated output from encoder and decoder part.\n",
        "    # This creates skip connections.\n",
        "    # Afterwards, decoder output is upsampled to dimensions equal to encoder output part.\n",
        "    decoder4 = decoder_block(\n",
        "        concat5, 'decoder4', num_filters=256)\n",
        "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1)\n",
        "\n",
        "    decoder3 = decoder_block(\n",
        "        concat4, 'decoder3', num_filters=128)\n",
        "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1)\n",
        "\n",
        "    decoder2 = decoder_block(\n",
        "        concat3, 'decoder2', num_filters=64)\n",
        "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
        "\n",
        "    decoder1 = decoder_block(\n",
        "        concat2, 'decoder1', num_filters=64)\n",
        "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1)\n",
        "\n",
        "    # Final upsampling and decoder block for segmentation.\n",
        "    output = UpSampling2D()(concat1)\n",
        "    output = decoder_block(\n",
        "        output, 'decoder_output', num_filters=32)\n",
        "    output = Conv2D(\n",
        "        1, (1, 1), activation=None, name='prediction')(output)\n",
        "    if not use_lovash:\n",
        "        output = Activation('sigmoid')(output)\n",
        "        \n",
        "    model = Model(base_model.input, output)\n",
        "    model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_yLpetbAFJH"
      },
      "source": [
        "Inspect created model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLyirRHBAAxO",
        "outputId": "4b21cfc3-bd43-40fb-ee10-6393747da0ea"
      },
      "source": [
        "input_size = (224, 224, 3)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = unet_resnet(\n",
        "    input_size, decoder_block_simple, weights='imagenet')\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/kernel:0' shape=(3, 3, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'center_bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/kernel:0' shape=(3, 3, 2560, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'decoder4_bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/kernel:0' shape=(3, 3, 1280, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'decoder3_bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/kernel:0' shape=(3, 3, 640, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder2_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/kernel:0' shape=(3, 3, 320, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder1_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/kernel:0' shape=(3, 3, 128, 32) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/bias:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_bn/gamma:0' shape=(32,) dtype=float32>\n",
            "  <tf.Variable 'decoder_output_bn/beta:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/kernel:0' shape=(1, 1, 32, 1) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/bias:0' shape=(1,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Total params: 19,107,712\n",
            "Trainable params: 19,064,832\n",
            "Non-trainable params: 42,880\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9QqKvIJIAI1m",
        "outputId": "2fa6e4bd-291d-4039-f7b7-1097c0673762"
      },
      "source": [
        "model_depth = unet_resnet(input_size, \n",
        "                          decoder_block_simple, # bottle\n",
        "                          weights='imagenet',\n",
        "                          #loss_func=bce_dice_loss, \n",
        "                          #metrics_list=[my_iou_metric],\n",
        "                          use_lovash=False)\n",
        "\n",
        "#print(model_depth.summary())\n",
        "\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('unet_resnet.h5',\n",
        "                                   monitor='val_my_iou_metric', \n",
        "                                   mode='max',\n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=True,\n",
        "                                   verbose=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric',\n",
        "                              mode='max',\n",
        "                              factor=0.5, \n",
        "                              patience=5, \n",
        "                              min_lr=0.0001, \n",
        "                              verbose=1)\n",
        "\n",
        "epochs = 1  # 25\n",
        "batch_size = 16\n",
        "\n",
        "X_tr = X_tr.astype('float32')\n",
        "y_tr = y_tr.astype('float32')\n",
        "\n",
        "history = model_depth.fit(X_tr[:320],\n",
        "                          y_tr[:320],\n",
        "                          validation_data=[X_val[:32], y_val[:32]], \n",
        "                          epochs=epochs,\n",
        "                          batch_size=batch_size,\n",
        "                          #callbacks=[model_checkpoint,reduce_lr], \n",
        "                          verbose=1\n",
        "                         )\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/kernel:0' shape=(3, 3, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_conv/bias:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'center_bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'center_bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/kernel:0' shape=(3, 3, 2560, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_conv/bias:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder4_bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'decoder4_bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/kernel:0' shape=(3, 3, 1280, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_conv/bias:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder3_bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'decoder3_bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/kernel:0' shape=(3, 3, 640, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder2_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder2_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/kernel:0' shape=(3, 3, 320, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_conv/bias:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder1_bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'decoder1_bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/kernel:0' shape=(3, 3, 128, 32) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_conv/bias:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'decoder_output_bn/gamma:0' shape=(32,) dtype=float32>\n",
            "  <tf.Variable 'decoder_output_bn/beta:0' shape=(32,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/kernel:0' shape=(1, 1, 32, 1) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.bias_add_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'prediction/bias:0' shape=(1,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "20/20 [==============================] - ETA: 0s - loss: 0.7296 - my_iou_metric: 0.0788 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-147d6d1e9b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                           \u001b[0;31m#callbacks=[model_checkpoint,reduce_lr],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                          )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1198\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1201\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1298 test_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1282 run_step  *\n        outputs = model.test_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1241 test_step  *\n        y_pred = self(x, training=False)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py:197 assert_input_compatibility  *\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_1 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(16, 224, 224, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(16, 224, 224, 1) dtype=float32>]\n"
          ]
        }
      ]
    }
  ]
}